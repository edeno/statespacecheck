{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"statespacecheck","text":"<p>Goodness-of-fit diagnostics for state space models in neuroscience</p> <p><code>statespacecheck</code> provides tools to assess how well Bayesian state space models fit neural data by examining the consistency between posterior distributions and their component likelihood distributions. These diagnostics help identify issues with prior specification and model assumptions, enabling iterative model refinement.</p>"},{"location":"#overview","title":"Overview","text":"<p>State space models are powerful tools for relating neural activity to latent dynamic brain states (e.g., memory, attention, spatial navigation). The core assumption is that complex, high-dimensional neural activity can be related to low-dimensional latent states through:</p> <ol> <li>State transition model: How latent states evolve over time</li> <li>Observation model: How neural activity relates to the current latent state</li> </ol> <p>The posterior distribution combines information from both models, weighing current data (normalized likelihood) against accumulated history (prediction distribution). When these distributions agree, the model's prior expectations and data-driven evidence are consistent. When they diverge, the mismatch reveals where and when the model fails to capture the structure of the data.</p>"},{"location":"#scientific-context","title":"Scientific Context","text":"<p>This package implements goodness-of-fit diagnostics for state space models used in neuroscience. The methods are based on the principle that a well-specified model should have consistent posterior and likelihood distributions. Large divergences or low overlap indicate:</p> <ol> <li>Prior issues: State transition model too rigid or misspecified</li> <li>Observation model issues: Tuning curves or noise assumptions incorrect</li> <li>Model capacity: Latent state dimensionality insufficient</li> </ol> <p>These diagnostics complement but are distinct from:</p> <ul> <li>Cross-validation: Measures predictive generalization to new data</li> <li>Permutation tests: Assess whether model captures structure vs. random patterns</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>KL Divergence: Measure information divergence between posterior and likelihood distributions at each time point</li> <li>HPD Overlap: Compute spatial overlap between Highest Posterior Density (HPD) regions</li> <li>Vectorized Operations: Efficient NumPy-based implementation with no Python loops</li> <li>Flexible Dimensionality: Supports both 1D <code>(n_time, n_position_bins)</code> and 2D <code>(n_time, n_x_bins, n_y_bins)</code> spatial arrays</li> <li>Robust Edge Case Handling: Proper treatment of NaN values, zero sums, and empty distributions</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Using uv (recommended)\nuv pip install statespacecheck\n\n# Using pip\npip install statespacecheck\n</code></pre>"},{"location":"#verify-installation","title":"Verify Installation","text":"<pre><code># Verify installation\nimport statespacecheck\nprint(f\"\u2713 statespacecheck v{statespacecheck.__version__} installed successfully\")\n\n# Quick functionality test\nimport numpy as np\nfrom statespacecheck import kl_divergence\n\n# Test with identical distributions (should give near-zero divergence)\ndist = np.array([[0.5, 0.5]])\ndivergence = kl_divergence(dist, dist)\nprint(f\"\u2713 KL divergence test: {divergence[0]:.6f} (expected: ~0.0)\")\n</code></pre>"},{"location":"#basic-example","title":"Basic Example","text":"<pre><code>import numpy as np\nfrom statespacecheck import (\n    kl_divergence,\n    hpd_overlap,\n    highest_density_region,\n)\n\n# Example: 1D spatial arrays (time x position)\nn_time, n_bins = 100, 50\nstate_dist = np.random.dirichlet(np.ones(n_bins), size=n_time)  # (1)!\nlikelihood = np.random.dirichlet(np.ones(n_bins), size=n_time)  # (2)!\n\n# Compute KL divergence at each time point\nkl_div = kl_divergence(state_dist, likelihood)  # (3)!\n# Returns: (n_time,) array of divergence values\n\n# Compute HPD region overlap\noverlap = hpd_overlap(\n    state_dist,\n    likelihood,\n    coverage=0.95  # (4)!\n)\n# Returns: (n_time,) array of overlap proportions (0 = no overlap, 1 = complete)\n\n# Get highest density region mask\nhd_mask = highest_density_region(state_dist, coverage=0.95)\n# Returns: (n_time, n_bins) boolean mask\n</code></pre> <ol> <li>Predictive distribution - your model's prediction before seeing current data</li> <li>Normalized likelihood - what the data alone says about the state</li> <li>Computes Kullback-Leibler divergence for each time point</li> <li>95% credible region coverage probability</li> </ol>"},{"location":"#whats-next","title":"What's Next?","text":"<p>New to state space model diagnostics? Start with Tutorial 1: Introduction for a gentle introduction with visualizations and real examples.</p> <p>Want to understand HPD regions? Jump to Tutorial 2: Highest Density Regions for deep dive into spatial diagnostics.</p> <p>Ready for time-series analysis? Explore Tutorial 3: Time-Resolved Diagnostics to identify when and where your model fails.</p> <p>Need complete API details? Browse the API Reference for detailed function documentation with parameter specifications.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Tutorials: Step-by-step guides with interactive Jupyter notebooks</li> <li>API Reference: Complete API documentation with detailed parameter descriptions</li> <li>Contributing: Guide for contributing to the project</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use this package in your research, please cite:</p> <pre><code>@software{statespacecheck2025,\n  title={statespacecheck: Goodness-of-fit diagnostics for state space models},\n  author={Denovellis, Eric and Zeng, Sirui and Eden, Uri T.},\n  year={2025},\n  url={https://github.com/edeno/statespacecheck}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"#references","title":"References","text":"<ul> <li>Auger-M\u00e9th\u00e9, M., et al. (2021). A guide to state-space modeling of ecological time series. Ecological Monographs, 91(4), e01470.</li> <li>Newman, K. B., &amp; Thomas, L. (2014). Goodness of fit for state-space models. In Statistical Inference from Stochastic Processes (pp. 153-191).</li> <li>Gelman, A., et al. (2020). Bayesian Data Analysis (3rd ed.). CRC Press.</li> </ul>"},{"location":"contributing/","title":"Contributing to statespacecheck","text":"<p>Thank you for your interest in contributing to statespacecheck! This document provides guidelines for development, testing, and releasing.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or later</li> <li>uv (recommended) or pip</li> <li>Git</li> </ul>"},{"location":"contributing/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Clone the repository <pre><code>git clone https://github.com/edeno/statespacecheck.git\ncd statespacecheck\n</code></pre></p> </li> <li> <p>Create a virtual environment and install dependencies <pre><code># Using uv (recommended - much faster)\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv pip install -e \".[dev]\"\n\n# Or using pip\npython -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev]\"\n</code></pre></p> </li> <li> <p>Verify installation <pre><code>python -c \"import statespacecheck; print(statespacecheck.__version__)\"\npytest --version\nruff --version\nmypy --version\n</code></pre></p> </li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#code-quality-standards","title":"Code Quality Standards","text":"<p>This project follows strict code quality standards:</p> <ul> <li>Formatting: ruff format (100 char line length)</li> <li>Linting: ruff check (comprehensive rules including NumPy-specific)</li> <li>Type checking: mypy in strict mode (no <code># type: ignore</code> allowed)</li> <li>Testing: pytest with 100% coverage requirement</li> <li>Docstrings: NumPy style</li> </ul>"},{"location":"contributing/#running-checks-locally","title":"Running Checks Locally","text":"<p>Before committing, run all quality checks:</p> <pre><code># Format code\nruff format .\n\n# Check formatting (CI runs this)\nruff format --check .\n\n# Lint code\nruff check .\n\n# Fix auto-fixable linting issues\nruff check --fix .\n\n# Type check\nmypy src/\n\n# Run tests with coverage\npytest\n\n# Run tests without coverage report\npytest --no-cov\n\n# Run specific test file\npytest tests/test_highest_density.py -v\n\n# Run specific test\npytest tests/test_highest_density.py::TestHighestDensityRegion::test_exact_hd_region_1d -xvs\n</code></pre>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Pre-commit hooks automatically run code quality checks before every commit.</p> <p>Setup (one-time): <pre><code># Install dependencies (includes pre-commit)\nuv pip install -e \".[dev]\"\n\n# Install the git hooks\nuv run pre-commit install\n</code></pre></p> <p>Usage: <pre><code># Hooks run automatically on `git commit`\n\n# Run manually on all files\nuv run pre-commit run --all-files\n\n# Run manually on staged files only\nuv run pre-commit run\n\n# Update hook versions\nuv run pre-commit autoupdate\n</code></pre></p> <p>What it checks: - Code formatting with ruff - Linting with ruff (auto-fixes when possible) - Type checking with mypy - All tests with pytest</p> <p>This matches exactly what CI runs, so commits that pass hooks will pass CI.</p>"},{"location":"contributing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"contributing/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p>The CI/CD pipeline runs automatically on: - Pull requests to <code>main</code> branch - Pushes to <code>main</code> branch - Git tags matching <code>v*</code> pattern</p>"},{"location":"contributing/#ci-jobs","title":"CI Jobs","text":"<ol> <li>Code Quality (<code>quality</code>)</li> <li>Runs on Python 3.12</li> <li>Checks: <code>ruff format --check</code>, <code>ruff check</code>, <code>mypy src/</code></li> <li> <p>Fast feedback (~1-2 minutes)</p> </li> <li> <p>Tests (<code>test</code>)</p> </li> <li>Matrix: Python 3.10, 3.11, 3.12, 3.13</li> <li>Runs: pytest with coverage</li> <li> <p>Uploads coverage to Codecov (Python 3.12 only)</p> </li> <li> <p>Build (<code>build</code>)</p> </li> <li>Requires: <code>quality</code> and <code>test</code> jobs to pass</li> <li>Builds: wheel and sdist</li> <li>Validates: <code>twine check dist/*</code></li> <li> <p>Uploads: distribution artifacts</p> </li> <li> <p>Install Tests (<code>test-install</code>)</p> </li> <li>Matrix: wheel/sdist \u00d7 Python 3.10/3.13</li> <li>Tests actual installation from built packages</li> <li> <p>Verifies: imports, version, public API</p> </li> <li> <p>Publish to TestPyPI (<code>publish-testpypi</code>)</p> </li> <li>Trigger: git tags matching <code>v*</code></li> <li> <p>Tests publishing to TestPyPI first (safety check)</p> </li> <li> <p>Publish to PyPI (<code>publish-pypi</code>)</p> </li> <li>Trigger: after TestPyPI succeeds</li> <li> <p>Publishes to production PyPI</p> </li> <li> <p>Create GitHub Release (<code>create-release</code>)</p> </li> <li>Trigger: after PyPI publish succeeds</li> <li>Creates GitHub release with notes</li> <li>Attaches distribution files</li> </ol>"},{"location":"contributing/#viewing-ci-results","title":"Viewing CI Results","text":"<ul> <li>In Pull Requests: Check the \"Checks\" tab</li> <li>In Commits: Look for \u2713 or \u2717 next to commit hash</li> <li>In Actions: Go to the \"Actions\" tab in GitHub</li> </ul>"},{"location":"contributing/#release-process","title":"Release Process","text":""},{"location":"contributing/#version-management","title":"Version Management","text":"<p>This project uses VCS-based versioning with <code>hatch-vcs</code>: - Version is derived from git tags - Development versions: <code>0.1.dev19+gf37b8a4e4.d20251105</code> - Release versions: <code>0.1.0</code> (from tag <code>v0.1.0</code>)</p> <p>Do not edit version numbers manually in code!</p>"},{"location":"contributing/#creating-a-release","title":"Creating a Release","text":"<ol> <li> <p>Ensure main branch is ready <pre><code>git checkout main\ngit pull origin main\n</code></pre></p> </li> <li> <p>Verify tests pass locally <pre><code>pytest\nruff format --check .\nruff check .\nmypy src/\n</code></pre></p> </li> <li> <p>Create and push a version tag <pre><code># For version 0.1.0\ngit tag v0.1.0\ngit push origin v0.1.0\n</code></pre></p> </li> <li> <p>Monitor the CI/CD pipeline</p> </li> <li>Go to GitHub Actions tab</li> <li> <p>Watch the workflow progress through:</p> <ul> <li>\u2713 Code quality and tests</li> <li>\u2713 Build distributions</li> <li>\u2713 Test installations</li> <li>\u2713 Publish to TestPyPI</li> <li>\u23f8\ufe0f  Requires approval \u2192 Publish to PyPI</li> <li>\u2713 Create GitHub release</li> </ul> </li> <li> <p>Approve PyPI deployment (if required)</p> </li> <li>Go to Actions tab \u2192 Click on the workflow run</li> <li>Click \"Review deployments\" button</li> <li> <p>Approve the <code>pypi</code> environment</p> </li> <li> <p>Verify the release</p> </li> <li>Check PyPI</li> <li>Check GitHub Releases</li> <li>Test installation:      <pre><code>pip install statespacecheck==0.1.0\npython -c \"import statespacecheck; print(statespacecheck.__version__)\"\n</code></pre></li> </ol>"},{"location":"contributing/#release-checklist","title":"Release Checklist","text":"<ul> <li> All tests pass on main branch</li> <li> CHANGELOG updated (if you maintain one)</li> <li> Documentation is up to date</li> <li> Version tag follows semantic versioning</li> <li> Tag pushed to GitHub</li> <li> CI pipeline completes successfully</li> <li> Package available on PyPI</li> <li> GitHub release created</li> </ul>"},{"location":"contributing/#pypi-trusted-publishing-setup","title":"PyPI Trusted Publishing Setup","text":""},{"location":"contributing/#first-time-setup","title":"First-time Setup","text":"<p>The CI/CD pipeline uses Trusted Publishing (no API tokens needed!). Set it up once:</p>"},{"location":"contributing/#1-pypi-configuration","title":"1. PyPI Configuration","text":"<ol> <li>Go to pypi.org/manage/account/publishing/</li> <li>Scroll to \"Add a new pending publisher\"</li> <li>Fill in:</li> <li>PyPI Project Name: <code>statespacecheck</code></li> <li>Owner: <code>edeno</code> (your GitHub username)</li> <li>Repository name: <code>statespacecheck</code></li> <li>Workflow name: <code>ci.yml</code></li> <li>Environment name: <code>pypi</code></li> <li>Click \"Add\"</li> </ol>"},{"location":"contributing/#2-testpypi-configuration-optional-but-recommended","title":"2. TestPyPI Configuration (Optional but Recommended)","text":"<ol> <li>Go to test.pypi.org/manage/account/publishing/</li> <li>Repeat the same steps with environment name: <code>testpypi</code></li> </ol>"},{"location":"contributing/#3-github-environments-optional","title":"3. GitHub Environments (Optional)","text":"<p>Add approval gates for extra safety:</p> <ol> <li>Go to your repo \u2192 Settings \u2192 Environments</li> <li>Create <code>pypi</code> environment:</li> <li>Click \"New environment\"</li> <li>Name: <code>pypi</code></li> <li>Add required reviewers (yourself or team members)</li> <li>Protection rules: Require approval before deployment</li> <li>Create <code>testpypi</code> environment (optional, can be auto-approved)</li> </ol>"},{"location":"contributing/#how-trusted-publishing-works","title":"How Trusted Publishing Works","text":"<ol> <li>GitHub Actions generates a short-lived OIDC token</li> <li>PyPI verifies the token matches the configured repository/workflow</li> <li>No long-lived API tokens needed!</li> <li>More secure than using PyPI API tokens</li> </ol>"},{"location":"contributing/#testing","title":"Testing","text":""},{"location":"contributing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py                      # Shared fixtures\n\u251c\u2500\u2500 test_highest_density.py          # HPD region tests\n\u251c\u2500\u2500 test_state_consistency.py        # KL divergence, HPD overlap tests\n\u251c\u2500\u2500 test_predictive_density.py       # Predictive checks tests\n\u251c\u2500\u2500 test_validation.py               # Input validation tests\n\u251c\u2500\u2500 test_edge_cases.py              # Edge case handling\n\u2514\u2500\u2500 test_properties.py              # Property-based tests (Hypothesis)\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># All tests with coverage\npytest\n\n# Verbose output\npytest -v\n\n# Stop on first failure\npytest -x\n\n# Show print statements\npytest -s\n\n# Run specific test class\npytest tests/test_highest_density.py::TestHighestDensityRegion -v\n\n# Run specific test method\npytest tests/test_highest_density.py::TestHighestDensityRegion::test_exact_hd_region_1d -xvs\n\n# Run tests matching pattern\npytest -k \"test_hpd\" -v\n\n# Run with coverage report\npytest --cov=statespacecheck --cov-report=html\n# Then open htmlcov/index.html\n</code></pre>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<p>Follow these guidelines:</p> <ol> <li>Use descriptive test names: <code>test_kl_divergence_with_identical_distributions</code></li> <li>Use pytest fixtures: Defined in <code>conftest.py</code></li> <li>Test edge cases: NaN, inf, zeros, empty arrays</li> <li>Use property-based testing: With Hypothesis for robustness</li> <li>Aim for 100% coverage: Every line should be tested</li> <li>Document test intent: Add docstrings to complex tests</li> </ol> <p>Example test:</p> <pre><code>def test_highest_density_region_with_peaked_distribution() -&gt; None:\n    \"\"\"Test HPD region correctly identifies peak in simple 1D distribution.\"\"\"\n    # Arrange\n    distribution = np.array([[0.1, 0.6, 0.3]])\n\n    # Act\n    region = highest_density_region(distribution, coverage=0.95)\n\n    # Assert\n    expected = np.array([[True, True, True]])\n    np.testing.assert_array_equal(region, expected)\n    assert region.shape == distribution.shape\n</code></pre>"},{"location":"contributing/#code-style-guidelines","title":"Code Style Guidelines","text":""},{"location":"contributing/#general-principles","title":"General Principles","text":"<ol> <li>Readability: Code is read more often than written</li> <li>Simplicity: Prefer simple solutions over clever ones</li> <li>Explicitness: Explicit is better than implicit</li> <li>Documentation: All public APIs must be documented</li> <li>Type safety: Use type hints everywhere</li> </ol>"},{"location":"contributing/#python-style","title":"Python Style","text":"<ul> <li>Line length: 100 characters (ruff enforces this)</li> <li>Imports: Sorted and grouped (ruff handles this)</li> <li>Quotes: Double quotes for strings (ruff enforces this)</li> <li>Naming:</li> <li>Functions/variables: <code>snake_case</code></li> <li>Classes: <code>PascalCase</code></li> <li>Constants: <code>UPPER_SNAKE_CASE</code></li> <li>Private: <code>_leading_underscore</code></li> </ul>"},{"location":"contributing/#numpy-style","title":"NumPy Style","text":"<ul> <li>Array operations: Prefer vectorized operations over loops</li> <li>Broadcasting: Use NumPy broadcasting for clarity</li> <li>Type hints: Use <code>np.ndarray</code> or <code>NDArray[np.floating]</code></li> <li>Docstrings: Include shape information in parameter descriptions</li> </ul> <p>Example:</p> <pre><code>def compute_something(\n    data: DistributionArray,\n    threshold: float = 0.5,\n) -&gt; DistributionArray:\n    \"\"\"Compute something useful from data.\n\n    Parameters\n    ----------\n    data : np.ndarray, shape (n_time, n_spatial)\n        Input data array.\n    threshold : float, optional\n        Threshold value, by default 0.5.\n\n    Returns\n    -------\n    result : np.ndarray, shape (n_time,)\n        Computed result.\n    \"\"\"\n    # Implementation\n    pass\n</code></pre>"},{"location":"contributing/#type-hints","title":"Type Hints","text":"<ul> <li>Use everywhere: All function signatures must have type hints</li> <li>Import from typing: Use <code>from collections.abc import Callable</code></li> <li>Union types: Use <code>X | None</code> (Python 3.10+ syntax)</li> <li>Generic types: Use <code>DistributionArray</code> type alias</li> <li>No <code># type: ignore</code>: Fix the issue instead</li> </ul>"},{"location":"contributing/#common-issues","title":"Common Issues","text":""},{"location":"contributing/#import-errors","title":"Import Errors","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'statespacecheck'</code></p> <p>Solution: Install in editable mode: <pre><code>pip install -e .\n</code></pre></p>"},{"location":"contributing/#version-shows-development-string","title":"Version Shows Development String","text":"<p>Problem: <code>__version__</code> is <code>0.1.dev19+g...</code> instead of <code>0.1.0</code></p> <p>Solution: This is expected in development! Version comes from git tags. To test release version: <pre><code>git tag v0.1.0\npip install -e .\n</code></pre></p>"},{"location":"contributing/#tests-failing-locally-but-pass-in-ci","title":"Tests Failing Locally But Pass in CI","text":"<p>Problem: Tests pass on your machine but fail in CI</p> <p>Possible causes: 1. Missing file: Not committed to git 2. Platform differences: Windows vs Linux 3. Python version: Test with multiple versions 4. Dependencies: Check <code>pyproject.toml</code> is up to date</p> <p>Debug: <pre><code># Run with same Python version as CI\npython3.12 -m pytest\n\n# Check which files are committed\ngit status\n\n# Check differences from main\ngit diff main\n</code></pre></p>"},{"location":"contributing/#mypy-errors","title":"Mypy Errors","text":"<p>Problem: <code>mypy</code> complains about types</p> <p>Solution: Never use <code># type: ignore</code>! Instead: 1. Add proper type hints to function signatures 2. Use type aliases like <code>DistributionArray</code> 3. Import types correctly: <code>from collections.abc import Callable</code> 4. Use explicit types: <code>result: DistributionArray = np.array(...)</code></p>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Email: eric.denovellis@ucsf.edu</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to statespacecheck, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the code reference pages and navigation.\"\"\"\n</pre> \"\"\"Generate the code reference pages and navigation.\"\"\" In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>nav = mkdocs_gen_files.Nav()\n</pre> nav = mkdocs_gen_files.Nav() In\u00a0[\u00a0]: Copied! <pre>src = Path(__file__).parent.parent / \"src\"\nmod_path = src / \"statespacecheck\"\n</pre> src = Path(__file__).parent.parent / \"src\" mod_path = src / \"statespacecheck\" In\u00a0[\u00a0]: Copied! <pre>for path in sorted(mod_path.rglob(\"*.py\")):\n    module_path = path.relative_to(src).with_suffix(\"\")\n    doc_path = path.relative_to(src).with_suffix(\".md\")\n    full_doc_path = Path(\"reference\", doc_path)\n\n    parts = tuple(module_path.parts)\n\n    # Skip private modules and version file\n    if any(part.startswith(\"_\") for part in parts):\n        continue\n\n    # Create navigation structure\n    nav[parts] = doc_path.as_posix()\n\n    # Generate markdown file with mkdocstrings directive\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        ident = \".\".join(parts)\n        fd.write(f\"# {ident}\\n\\n\")\n        fd.write(f\"::: {ident}\\n\")\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path.relative_to(Path.cwd()))\n</pre> for path in sorted(mod_path.rglob(\"*.py\")):     module_path = path.relative_to(src).with_suffix(\"\")     doc_path = path.relative_to(src).with_suffix(\".md\")     full_doc_path = Path(\"reference\", doc_path)      parts = tuple(module_path.parts)      # Skip private modules and version file     if any(part.startswith(\"_\") for part in parts):         continue      # Create navigation structure     nav[parts] = doc_path.as_posix()      # Generate markdown file with mkdocstrings directive     with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:         ident = \".\".join(parts)         fd.write(f\"# {ident}\\n\\n\")         fd.write(f\"::: {ident}\\n\")      mkdocs_gen_files.set_edit_path(full_doc_path, path.relative_to(Path.cwd())) In\u00a0[\u00a0]: Copied! <pre># Write the navigation file\nwith mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> # Write the navigation file with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>statespacecheck<ul> <li>highest_density</li> <li>periods</li> <li>predictive_checks</li> <li>state_consistency</li> <li>viz</li> </ul> </li> </ul>"},{"location":"reference/statespacecheck/highest_density/","title":"statespacecheck.highest_density","text":""},{"location":"reference/statespacecheck/highest_density/#statespacecheck.highest_density","title":"highest_density","text":"<p>Functions for computing highest density regions.</p>"},{"location":"reference/statespacecheck/highest_density/#statespacecheck.highest_density-functions","title":"Functions","text":""},{"location":"reference/statespacecheck/highest_density/#statespacecheck.highest_density.highest_density_region","title":"highest_density_region","text":"<pre><code>highest_density_region(distribution: DistributionArray, *, coverage: float = DEFAULT_COVERAGE) -&gt; NDArray[bool_]\n</code></pre> <p>Compute boolean mask indicating highest density region membership.</p> <p>Vectorized HPD mask for arrays shaped (n_time, *spatial). For each time t, includes all bins with value &gt;= threshold_t, where threshold_t is chosen so cumulative mass &gt;= coverage * total_t.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>(ndarray, shape(n_time, ...))</code> <p>Probability distributions over position at each time point where ... represents arbitrary spatial dimensions.</p> required <code>coverage</code> <code>float</code> <p>Desired coverage probability for the highest density region. Must be between 0 and 1. Default is 0.95 for 95% coverage.</p> <code>DEFAULT_COVERAGE</code> <p>Returns:</p> Name Type Description <code>isin_hd</code> <code>(ndarray, shape(n_time, ...))</code> <p>Boolean mask indicating which positions are in the highest density region at each time point, matching input shape.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If coverage is not in the range (0, 1).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck import highest_density_region\n&gt;&gt;&gt; # Simple 1D example with peaked distribution\n&gt;&gt;&gt; distribution = np.array([[0.1, 0.6, 0.3], [0.2, 0.5, 0.3]])\n&gt;&gt;&gt; region = highest_density_region(distribution, coverage=0.9)\n&gt;&gt;&gt; region.shape\n(2, 3)\n&gt;&gt;&gt; region.dtype\ndtype('bool')\n</code></pre> See Also <p>hpd_overlap : Compute overlap between HPD regions of two distributions kl_divergence : Measure information divergence between distributions</p> Notes <ul> <li>NaNs are ignored (treated as 0 mass).</li> <li>If total mass at time t &lt;= 0 or not finite, returns all-False for that t.</li> <li>Works in unnormalized space to avoid numerical issues.</li> <li>Fully vectorized with no Python loops for efficiency.</li> <li>Uses <code>&gt;=</code> threshold: all bins with value equal to cutoff are included.</li> <li>Due to ties, actual coverage may slightly exceed requested coverage.</li> <li>This ensures consistent behavior across equivalent distributions.</li> </ul> References <p>.. [1] https://stats.stackexchange.com/questions/240749/how-to-find-95-credible-interval</p> Source code in <code>src/statespacecheck/highest_density.py</code> <pre><code>def highest_density_region(\n    distribution: DistributionArray, *, coverage: float = DEFAULT_COVERAGE\n) -&gt; NDArray[np.bool_]:\n    \"\"\"Compute boolean mask indicating highest density region membership.\n\n    Vectorized HPD mask for arrays shaped (n_time, *spatial). For each time t,\n    includes all bins with value &gt;= threshold_t, where threshold_t is chosen so\n    cumulative mass &gt;= coverage * total_t.\n\n    Parameters\n    ----------\n    distribution : np.ndarray, shape (n_time, ...)\n        Probability distributions over position at each time point where\n        ... represents arbitrary spatial dimensions.\n    coverage : float, optional\n        Desired coverage probability for the highest density region. Must be between 0 and 1.\n        Default is 0.95 for 95% coverage.\n\n    Returns\n    -------\n    isin_hd : np.ndarray, shape (n_time, ...)\n        Boolean mask indicating which positions are in the highest density region at each\n        time point, matching input shape.\n\n    Raises\n    ------\n    ValueError\n        If coverage is not in the range (0, 1).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck import highest_density_region\n    &gt;&gt;&gt; # Simple 1D example with peaked distribution\n    &gt;&gt;&gt; distribution = np.array([[0.1, 0.6, 0.3], [0.2, 0.5, 0.3]])\n    &gt;&gt;&gt; region = highest_density_region(distribution, coverage=0.9)\n    &gt;&gt;&gt; region.shape\n    (2, 3)\n    &gt;&gt;&gt; region.dtype\n    dtype('bool')\n\n    See Also\n    --------\n    hpd_overlap : Compute overlap between HPD regions of two distributions\n    kl_divergence : Measure information divergence between distributions\n\n    Notes\n    -----\n    - NaNs are ignored (treated as 0 mass).\n    - If total mass at time t &lt;= 0 or not finite, returns all-False for that t.\n    - Works in unnormalized space to avoid numerical issues.\n    - Fully vectorized with no Python loops for efficiency.\n    - Uses `&gt;=` threshold: all bins with value equal to cutoff are included.\n    - Due to ties, actual coverage may slightly exceed requested coverage.\n    - This ensures consistent behavior across equivalent distributions.\n\n    References\n    ----------\n    .. [1] https://stats.stackexchange.com/questions/240749/how-to-find-95-credible-interval\n\n    \"\"\"\n    validate_coverage(coverage)\n\n    # Use centralized validation: handles NaN/inf \u2192 0, checks non-negativity, validates dimensions\n    clean = validate_distribution(\n        distribution,\n        name=\"distribution\",\n        min_ndim=2,  # Require at least (n_time, n_spatial)\n        allow_nan=True,\n    )\n\n    # Flatten to (n_time, n_spatial) for vectorized operations\n    flat = flatten_time_spatial(clean)\n\n    n_time = clean.shape[0]\n    n_spatial = flat.shape[1]\n\n    # Compute total mass and target mass for each time point\n    # Shape: (n_time,)\n    totals = flat.sum(axis=1)\n    target = coverage * totals\n\n    # Identify rows with no mass -&gt; empty HPD (all False)\n    empty = ~np.isfinite(totals) | (totals &lt;= 0)\n\n    # Sort each row descending (vectorized)\n    # Shape: (n_time, n_spatial)\n    flat_sorted = np.sort(flat, axis=1)[:, ::-1]\n\n    # Row-wise cumulative sums\n    # Shape: (n_time, n_spatial)\n    csum = np.cumsum(flat_sorted, axis=1)\n\n    # Find the first index where cumulative &gt;= target (per row)\n    # Shape: (n_time, n_spatial) boolean\n    ge = csum &gt;= target[:, None]\n\n    # Check if each row has at least one True value\n    # Shape: (n_time,)\n    has_true = ge.any(axis=1)\n\n    # argmax gives first True index; if none True, returns 0 (we fix below)\n    # Shape: (n_time,)\n    idx = ge.argmax(axis=1)\n\n    # If a row never reaches target but has positive mass (rare numeric case),\n    # choose the last index. If it's truly empty, handle later.\n    idx = np.where(has_true, idx, n_spatial - 1)\n\n    # Per-row cutoff (unnormalized)\n    # Shape: (n_time,)\n    cutoff = np.take_along_axis(flat_sorted, idx[:, None], axis=1).squeeze(1)\n\n    # Empty rows -&gt; set cutoff to +inf so mask is all False\n    cutoff = np.where(empty, np.inf, cutoff)\n\n    # Broadcast cutoff back to spatial shape and build mask\n    # Use the **clean** array for the comparison to keep behavior consistent\n    # Broadcasting: reshape cutoff from (n_time,) to (n_time, 1, 1, ...) to match spatial dims\n    # Using tuple unpacking for clarity\n    broadcast_shape = (n_time,) + (1,) * (clean.ndim - 1)\n    return clean &gt;= cutoff.reshape(broadcast_shape)\n</code></pre>"},{"location":"reference/statespacecheck/periods/","title":"statespacecheck.periods","text":""},{"location":"reference/statespacecheck/periods/#statespacecheck.periods","title":"periods","text":"<p>Period-level aggregation and detection utilities for time-series metrics.</p> <p>This module provides functions to: 1. Aggregate time-series goodness-of-fit metrics over specified time periods 2. Detect problematic periods based on threshold exceedances 3. Combine multiple diagnostic methods via majority voting</p>"},{"location":"reference/statespacecheck/periods/#statespacecheck.periods-functions","title":"Functions","text":""},{"location":"reference/statespacecheck/periods/#statespacecheck.periods.aggregate_over_period","title":"aggregate_over_period","text":"<pre><code>aggregate_over_period(metric_values: NDArray[floating], time_mask: NDArray[bool_], *, reduction: str = 'mean', weights: NDArray[floating] | None = None) -&gt; float\n</code></pre> <p>Aggregate metric values over specified time period.</p> <p>Aggregates time-series metrics (e.g., KL divergence, HPD overlap, or predictive checks) over specified time periods using an indicator function approach from the paper.</p> <p>Parameters:</p> Name Type Description Default <code>metric_values</code> <code>(ndarray, shape(n_time))</code> <p>Time-series metric array. Must be 1-dimensional.</p> required <code>time_mask</code> <code>(ndarray, shape(n_time))</code> <p>Boolean array indicating which time points to include. True values indicate time points to aggregate. Must have same length as metric_values.</p> required <code>reduction</code> <code>('mean', 'sum')</code> <p>Aggregation method. Default is 'mean'. - 'mean': Compute mean over selected time points (optionally weighted) - 'sum': Compute sum over selected time points</p> <code>'mean'</code> <code>weights</code> <code>(ndarray, shape(n_time))</code> <p>Optional weights for weighted mean (e.g., occupancy/time weighting). Must be non-negative and have same length as metric_values. Only used when reduction='mean'. Ignored for 'sum' with a warning.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>aggregated_value</code> <code>float</code> <p>Aggregated metric value (scalar float). Returns NaN if no time points are selected (all-false mask).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If metric_values is not 1-dimensional, if shapes don't match, if reduction is invalid, or if weights are negative.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If weights are provided when reduction='sum' (weights are ignored).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck import aggregate_over_period\n&gt;&gt;&gt; # Aggregate KL divergence over non-local events\n&gt;&gt;&gt; kl_values = np.array([0.5, 1.0, 0.3, 0.8, 0.6])\n&gt;&gt;&gt; is_non_local = np.array([True, False, True, True, False])\n&gt;&gt;&gt; result = aggregate_over_period(kl_values, is_non_local, reduction=\"mean\")\n&gt;&gt;&gt; result  # Mean of [0.5, 0.3, 0.8]\n0.5333333333333333\n</code></pre> <pre><code>&gt;&gt;&gt; # Aggregate log-likelihoods using sum\n&gt;&gt;&gt; log_likes = np.array([-1.0, -2.0, -1.5, -3.0])\n&gt;&gt;&gt; period_mask = np.array([True, True, True, True])\n&gt;&gt;&gt; total = aggregate_over_period(log_likes, period_mask, reduction=\"sum\")\n&gt;&gt;&gt; total  # Sum of all values\n-7.5\n</code></pre> <pre><code>&gt;&gt;&gt; # Weighted mean with occupancy weights\n&gt;&gt;&gt; metrics = np.array([1.0, 2.0, 3.0])\n&gt;&gt;&gt; mask = np.array([True, True, True])\n&gt;&gt;&gt; occupancy = np.array([10.0, 5.0, 10.0])  # Time spent in each state\n&gt;&gt;&gt; weighted = aggregate_over_period(metrics, mask, weights=occupancy)\n&gt;&gt;&gt; weighted  # (1*10 + 2*5 + 3*10) / (10 + 5 + 10)\n2.0\n</code></pre> See Also <p>kl_divergence : Compute KL divergence between distributions hpd_overlap : Compute spatial overlap between HPD regions predictive_density : Compute predictive density log_predictive_density : Compute log predictive density</p> Notes <p>This function implements the period-level aggregation approach from the paper, using indicator functions (time_mask) to select time points for aggregation.</p> <p>Use cases: - Period-level KL divergence: weighted mean over non-local events - Period-level log-likelihood: sum for predictive checks - Consistent with paper's weighted average equations</p> <p>When no time points are selected (all-false mask), returns NaN to indicate an undefined aggregation.</p> Source code in <code>src/statespacecheck/periods.py</code> <pre><code>def aggregate_over_period(\n    metric_values: NDArray[np.floating],\n    time_mask: NDArray[np.bool_],\n    *,\n    reduction: str = \"mean\",\n    weights: NDArray[np.floating] | None = None,\n) -&gt; float:\n    \"\"\"Aggregate metric values over specified time period.\n\n    Aggregates time-series metrics (e.g., KL divergence, HPD overlap, or\n    predictive checks) over specified time periods using an indicator\n    function approach from the paper.\n\n    Parameters\n    ----------\n    metric_values : np.ndarray, shape (n_time,)\n        Time-series metric array. Must be 1-dimensional.\n    time_mask : np.ndarray, shape (n_time,)\n        Boolean array indicating which time points to include.\n        True values indicate time points to aggregate.\n        Must have same length as metric_values.\n    reduction : {'mean', 'sum'}, optional\n        Aggregation method. Default is 'mean'.\n        - 'mean': Compute mean over selected time points (optionally weighted)\n        - 'sum': Compute sum over selected time points\n    weights : np.ndarray, shape (n_time,), optional\n        Optional weights for weighted mean (e.g., occupancy/time weighting).\n        Must be non-negative and have same length as metric_values.\n        Only used when reduction='mean'. Ignored for 'sum' with a warning.\n\n    Returns\n    -------\n    aggregated_value : float\n        Aggregated metric value (scalar float).\n        Returns NaN if no time points are selected (all-false mask).\n\n    Raises\n    ------\n    ValueError\n        If metric_values is not 1-dimensional, if shapes don't match,\n        if reduction is invalid, or if weights are negative.\n\n    Warns\n    -----\n    UserWarning\n        If weights are provided when reduction='sum' (weights are ignored).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck import aggregate_over_period\n    &gt;&gt;&gt; # Aggregate KL divergence over non-local events\n    &gt;&gt;&gt; kl_values = np.array([0.5, 1.0, 0.3, 0.8, 0.6])\n    &gt;&gt;&gt; is_non_local = np.array([True, False, True, True, False])\n    &gt;&gt;&gt; result = aggregate_over_period(kl_values, is_non_local, reduction=\"mean\")\n    &gt;&gt;&gt; result  # Mean of [0.5, 0.3, 0.8]\n    0.5333333333333333\n\n    &gt;&gt;&gt; # Aggregate log-likelihoods using sum\n    &gt;&gt;&gt; log_likes = np.array([-1.0, -2.0, -1.5, -3.0])\n    &gt;&gt;&gt; period_mask = np.array([True, True, True, True])\n    &gt;&gt;&gt; total = aggregate_over_period(log_likes, period_mask, reduction=\"sum\")\n    &gt;&gt;&gt; total  # Sum of all values\n    -7.5\n\n    &gt;&gt;&gt; # Weighted mean with occupancy weights\n    &gt;&gt;&gt; metrics = np.array([1.0, 2.0, 3.0])\n    &gt;&gt;&gt; mask = np.array([True, True, True])\n    &gt;&gt;&gt; occupancy = np.array([10.0, 5.0, 10.0])  # Time spent in each state\n    &gt;&gt;&gt; weighted = aggregate_over_period(metrics, mask, weights=occupancy)\n    &gt;&gt;&gt; weighted  # (1*10 + 2*5 + 3*10) / (10 + 5 + 10)\n    2.0\n\n    See Also\n    --------\n    kl_divergence : Compute KL divergence between distributions\n    hpd_overlap : Compute spatial overlap between HPD regions\n    predictive_density : Compute predictive density\n    log_predictive_density : Compute log predictive density\n\n    Notes\n    -----\n    This function implements the period-level aggregation approach from the paper,\n    using indicator functions (time_mask) to select time points for aggregation.\n\n    Use cases:\n    - Period-level KL divergence: weighted mean over non-local events\n    - Period-level log-likelihood: sum for predictive checks\n    - Consistent with paper's weighted average equations\n\n    When no time points are selected (all-false mask), returns NaN to indicate\n    an undefined aggregation.\n    \"\"\"\n    # Validate metric_values is 1D\n    metric_arr = np.asarray(metric_values, dtype=float)\n    if metric_arr.ndim != 1:\n        raise ValueError(\n            f\"metric_values must be 1-dimensional, \"\n            f\"got {metric_arr.ndim}D array with shape {metric_arr.shape}\"\n        )\n\n    # Validate time_mask\n    mask_arr = np.asarray(time_mask, dtype=bool)\n    if mask_arr.shape != metric_arr.shape:\n        raise ValueError(\n            f\"time_mask must have same length as metric_values, \"\n            f\"got {mask_arr.shape} vs {metric_arr.shape}\"\n        )\n\n    # Validate reduction parameter\n    if reduction not in (\"mean\", \"sum\"):\n        raise ValueError(f\"reduction must be 'mean' or 'sum', got '{reduction}'\")\n\n    # Validate weights if provided\n    if weights is not None:\n        weights_arr = np.asarray(weights, dtype=float)\n        if weights_arr.shape != metric_arr.shape:\n            raise ValueError(\n                f\"weights must have same length as metric_values, \"\n                f\"got {weights_arr.shape} vs {metric_arr.shape}\"\n            )\n        if not np.isfinite(weights_arr).all():\n            raise ValueError(\"weights must be finite (no NaN or inf values)\")\n        if np.any(weights_arr &lt; 0):\n            raise ValueError(\"weights must be non-negative\")\n\n        # Warn if weights provided with sum reduction\n        if reduction == \"sum\":\n            warnings.warn(\n                \"weights are ignored when reduction='sum'\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    # Select values based on time_mask\n    selected_values = metric_arr[mask_arr]\n\n    # Handle empty period (no time points selected)\n    if len(selected_values) == 0:\n        return np.nan\n\n    # Perform aggregation\n    if reduction == \"sum\":\n        return float(np.sum(selected_values))\n    else:  # reduction == \"mean\"\n        if weights is None:\n            return float(np.mean(selected_values))\n        else:\n            # Weighted mean\n            selected_weights = weights_arr[mask_arr]\n            weight_sum = np.sum(selected_weights)\n            if weight_sum == 0:\n                # All weights are zero -&gt; return NaN\n                return np.nan\n            return float(np.sum(selected_values * selected_weights) / weight_sum)\n</code></pre>"},{"location":"reference/statespacecheck/periods/#statespacecheck.periods.flag_low_overlap","title":"flag_low_overlap","text":"<pre><code>flag_low_overlap(overlap: NDArray[floating], threshold: float = 0.4, min_len: int = 5) -&gt; NDArray[bool_]\n</code></pre> <p>Flag times where HPD overlap is below threshold.</p> <p>This is the boolean array version of find_low_overlap_intervals(). Use this when combining multiple diagnostics with combine_flags(). Use find_low_overlap_intervals() when you need interval boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>overlap</code> <code>(ndarray, shape(n_time))</code> <p>HPD overlap values.</p> required <code>threshold</code> <code>float</code> <p>Threshold below which overlap is considered problematic. Default is 0.4. A value of 0.4 identifies periods where less than 40% of HPD regions overlap, indicating substantial spatial disagreement.</p> <code>0.4</code> <code>min_len</code> <code>int</code> <p>Minimum length for flagged runs. Default is 5. Filters out transient single-timepoint artifacts. Adjust based on temporal resolution and expected duration of model failures.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>flags</code> <code>(ndarray, shape(n_time))</code> <p>Boolean array indicating flagged time points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck.periods import flag_low_overlap, combine_flags\n&gt;&gt;&gt; overlap = np.array([0.8, 0.8, 0.3, 0.3, 0.3, 0.3, 0.3, 0.8])\n&gt;&gt;&gt; flags = flag_low_overlap(overlap, threshold=0.4, min_len=5)\n&gt;&gt;&gt; flags\narray([False, False,  True,  True,  True,  True,  True, False])\n&gt;&gt;&gt; # Combine with other diagnostics\n&gt;&gt;&gt; kl_flags = flag_extreme_kl(kl_values, z_thresh=3.0, min_len=5)\n&gt;&gt;&gt; combined = combine_flags(flags, kl_flags, min_votes=2, min_len=5)\n</code></pre> See Also <p>find_low_overlap_intervals : Returns interval boundaries instead of boolean mask combine_flags : Combine multiple diagnostic flag arrays</p> Source code in <code>src/statespacecheck/periods.py</code> <pre><code>def flag_low_overlap(\n    overlap: NDArray[np.floating],\n    threshold: float = 0.4,\n    min_len: int = 5,\n) -&gt; NDArray[np.bool_]:\n    \"\"\"Flag times where HPD overlap is below threshold.\n\n    This is the boolean array version of find_low_overlap_intervals().\n    Use this when combining multiple diagnostics with combine_flags().\n    Use find_low_overlap_intervals() when you need interval boundaries.\n\n    Parameters\n    ----------\n    overlap : np.ndarray, shape (n_time,)\n        HPD overlap values.\n    threshold : float, optional\n        Threshold below which overlap is considered problematic. Default is 0.4.\n        A value of 0.4 identifies periods where less than 40% of HPD regions\n        overlap, indicating substantial spatial disagreement.\n    min_len : int, optional\n        Minimum length for flagged runs. Default is 5.\n        Filters out transient single-timepoint artifacts. Adjust based on\n        temporal resolution and expected duration of model failures.\n\n    Returns\n    -------\n    flags : np.ndarray, shape (n_time,)\n        Boolean array indicating flagged time points.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck.periods import flag_low_overlap, combine_flags\n    &gt;&gt;&gt; overlap = np.array([0.8, 0.8, 0.3, 0.3, 0.3, 0.3, 0.3, 0.8])\n    &gt;&gt;&gt; flags = flag_low_overlap(overlap, threshold=0.4, min_len=5)\n    &gt;&gt;&gt; flags\n    array([False, False,  True,  True,  True,  True,  True, False])\n    &gt;&gt;&gt; # Combine with other diagnostics\n    &gt;&gt;&gt; kl_flags = flag_extreme_kl(kl_values, z_thresh=3.0, min_len=5)\n    &gt;&gt;&gt; combined = combine_flags(flags, kl_flags, min_votes=2, min_len=5)\n\n    See Also\n    --------\n    find_low_overlap_intervals : Returns interval boundaries instead of boolean mask\n    combine_flags : Combine multiple diagnostic flag arrays\n    \"\"\"\n    overlap_arr = np.asarray(overlap, dtype=float)\n    flags = (overlap_arr &lt; threshold) &amp; np.isfinite(overlap_arr)\n    return _enforce_min_len(flags, min_len)\n</code></pre>"},{"location":"reference/statespacecheck/periods/#statespacecheck.periods.find_low_overlap_intervals","title":"find_low_overlap_intervals","text":"<pre><code>find_low_overlap_intervals(overlap: NDArray[floating], threshold: float = 0.4, min_len: int = 5) -&gt; list[tuple[int, int]]\n</code></pre> <p>Identify contiguous intervals where HPD overlap &lt; threshold and length &gt;= min_len.</p> <p>Returns interval boundaries rather than boolean flags. Use flag_low_overlap() if you need a boolean array compatible with combine_flags().</p> <p>Parameters:</p> Name Type Description Default <code>overlap</code> <code>(ndarray, shape(n_time))</code> <p>HPD overlap values.</p> required <code>threshold</code> <code>float</code> <p>Threshold below which overlap is considered problematic. Default is 0.4.</p> <code>0.4</code> <code>min_len</code> <code>int</code> <p>Minimum length for intervals to be reported. Default is 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>intervals</code> <code>list[tuple[int, int]]</code> <p>List of (start, stop) index pairs for problematic periods. Uses Python slice notation: interval includes start but excludes stop, so to extract values use array[start:stop] not array[start:stop+1].</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck.periods import find_low_overlap_intervals\n&gt;&gt;&gt; overlap = np.array([0.8, 0.8, 0.3, 0.3, 0.3, 0.3, 0.3, 0.8])\n&gt;&gt;&gt; intervals = find_low_overlap_intervals(overlap, threshold=0.4, min_len=5)\n&gt;&gt;&gt; intervals\n[(2, 7)]\n&gt;&gt;&gt; # Extract first problematic interval\n&gt;&gt;&gt; if intervals:\n&gt;&gt;&gt;     start, stop = intervals[0]\n&gt;&gt;&gt;     problem_overlap = overlap[start:stop]  # Correct: excludes stop\n&gt;&gt;&gt;     print(f\"Problem period: timepoints {start}-{stop-1}\")\n</code></pre> See Also <p>flag_low_overlap : Returns boolean mask instead of interval boundaries</p> Source code in <code>src/statespacecheck/periods.py</code> <pre><code>def find_low_overlap_intervals(\n    overlap: NDArray[np.floating],\n    threshold: float = 0.4,\n    min_len: int = 5,\n) -&gt; list[tuple[int, int]]:\n    \"\"\"Identify contiguous intervals where HPD overlap &lt; threshold and length &gt;= min_len.\n\n    Returns interval boundaries rather than boolean flags. Use flag_low_overlap()\n    if you need a boolean array compatible with combine_flags().\n\n    Parameters\n    ----------\n    overlap : np.ndarray, shape (n_time,)\n        HPD overlap values.\n    threshold : float, optional\n        Threshold below which overlap is considered problematic. Default is 0.4.\n    min_len : int, optional\n        Minimum length for intervals to be reported. Default is 5.\n\n    Returns\n    -------\n    intervals : list[tuple[int, int]]\n        List of (start, stop) index pairs for problematic periods.\n        Uses Python slice notation: interval includes start but excludes stop,\n        so to extract values use array[start:stop] not array[start:stop+1].\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck.periods import find_low_overlap_intervals\n    &gt;&gt;&gt; overlap = np.array([0.8, 0.8, 0.3, 0.3, 0.3, 0.3, 0.3, 0.8])\n    &gt;&gt;&gt; intervals = find_low_overlap_intervals(overlap, threshold=0.4, min_len=5)\n    &gt;&gt;&gt; intervals\n    [(2, 7)]\n    &gt;&gt;&gt; # Extract first problematic interval\n    &gt;&gt;&gt; if intervals:\n    &gt;&gt;&gt;     start, stop = intervals[0]\n    &gt;&gt;&gt;     problem_overlap = overlap[start:stop]  # Correct: excludes stop\n    &gt;&gt;&gt;     print(f\"Problem period: timepoints {start}-{stop-1}\")\n\n    See Also\n    --------\n    flag_low_overlap : Returns boolean mask instead of interval boundaries\n    \"\"\"\n    overlap_arr = np.asarray(overlap, dtype=float)\n    bad = (overlap_arr &lt; threshold) &amp; np.isfinite(overlap_arr)\n    bad = _enforce_min_len(bad, min_len)\n    return _contiguous_runs(bad)\n</code></pre>"},{"location":"reference/statespacecheck/periods/#statespacecheck.periods.flag_extreme_kl","title":"flag_extreme_kl","text":"<pre><code>flag_extreme_kl(kl: NDArray[floating], z_thresh: float = 3.0, min_len: int = 5) -&gt; NDArray[bool_]\n</code></pre> <p>Flag times where KL divergence is extreme via robust z-score.</p> <p>Parameters:</p> Name Type Description Default <code>kl</code> <code>(ndarray, shape(n_time))</code> <p>KL divergence values.</p> required <code>z_thresh</code> <code>float</code> <p>Z-score threshold above which values are flagged. Default is 3.0. A value of 3.0 corresponds to p &lt; 0.003 for normal distributions, providing a conservative threshold to avoid false positives. Lower values (e.g., 2.0) are more sensitive but may flag more noise.</p> <code>3.0</code> <code>min_len</code> <code>int</code> <p>Minimum length for flagged runs. Default is 5. Filters out transient single-timepoint artifacts. Adjust based on temporal resolution and expected duration of model failures.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>flags</code> <code>(ndarray, shape(n_time))</code> <p>Boolean array indicating flagged time points.</p> Notes <p>Inf/NaN KL values are ignored (not flagged) by default.</p> <p>The min_len parameter filters short runs to reduce false positives from single-timepoint artifacts or noise. This is a practical filter, not a statistical requirement. Appropriate values depend on: - Temporal resolution of your data (higher sampling \u2192 larger min_len) - Expected duration of real model failures (persistent vs transient) - Tolerance for false alarms (strict \u2192 larger min_len)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck.periods import flag_extreme_kl\n&gt;&gt;&gt; kl = np.ones(20)\n&gt;&gt;&gt; kl[5:10] = 100.0  # Extreme spike\n&gt;&gt;&gt; flags = flag_extreme_kl(kl, z_thresh=3.0, min_len=5)\n&gt;&gt;&gt; np.sum(flags[5:10])\n5\n</code></pre> See Also <p>flag_low_overlap : Flag periods with low HPD overlap flag_extreme_pvalues : Flag extreme predictive p-values combine_flags : Combine multiple diagnostic methods</p> Source code in <code>src/statespacecheck/periods.py</code> <pre><code>def flag_extreme_kl(\n    kl: NDArray[np.floating],\n    z_thresh: float = 3.0,\n    min_len: int = 5,\n) -&gt; NDArray[np.bool_]:\n    \"\"\"Flag times where KL divergence is extreme via robust z-score.\n\n    Parameters\n    ----------\n    kl : np.ndarray, shape (n_time,)\n        KL divergence values.\n    z_thresh : float, optional\n        Z-score threshold above which values are flagged. Default is 3.0.\n        A value of 3.0 corresponds to p &lt; 0.003 for normal distributions,\n        providing a conservative threshold to avoid false positives.\n        Lower values (e.g., 2.0) are more sensitive but may flag more noise.\n    min_len : int, optional\n        Minimum length for flagged runs. Default is 5.\n        Filters out transient single-timepoint artifacts. Adjust based on\n        temporal resolution and expected duration of model failures.\n\n    Returns\n    -------\n    flags : np.ndarray, shape (n_time,)\n        Boolean array indicating flagged time points.\n\n    Notes\n    -----\n    Inf/NaN KL values are ignored (not flagged) by default.\n\n    The min_len parameter filters short runs to reduce false positives from\n    single-timepoint artifacts or noise. This is a practical filter, not a\n    statistical requirement. Appropriate values depend on:\n    - Temporal resolution of your data (higher sampling \u2192 larger min_len)\n    - Expected duration of real model failures (persistent vs transient)\n    - Tolerance for false alarms (strict \u2192 larger min_len)\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck.periods import flag_extreme_kl\n    &gt;&gt;&gt; kl = np.ones(20)\n    &gt;&gt;&gt; kl[5:10] = 100.0  # Extreme spike\n    &gt;&gt;&gt; flags = flag_extreme_kl(kl, z_thresh=3.0, min_len=5)\n    &gt;&gt;&gt; np.sum(flags[5:10])\n    5\n\n    See Also\n    --------\n    flag_low_overlap : Flag periods with low HPD overlap\n    flag_extreme_pvalues : Flag extreme predictive p-values\n    combine_flags : Combine multiple diagnostic methods\n    \"\"\"\n    kl_arr = np.asarray(kl, dtype=float)\n    zscores = _robust_zscore(kl_arr)\n    flags = np.isfinite(zscores) &amp; (zscores &gt; z_thresh)\n    return _enforce_min_len(flags, min_len)\n</code></pre>"},{"location":"reference/statespacecheck/periods/#statespacecheck.periods.flag_extreme_pvalues","title":"flag_extreme_pvalues","text":"<pre><code>flag_extreme_pvalues(pvalues: NDArray[floating], alpha: float = 0.05, min_len: int = 5) -&gt; NDArray[bool_]\n</code></pre> <p>Two-sided extremeness test for predictive p-values.</p> <p>Flags when pvalues &lt; alpha/2 or pvalues &gt; 1 - alpha/2.</p> <p>Parameters:</p> Name Type Description Default <code>pvalues</code> <code>(ndarray, shape(n_time))</code> <p>Predictive p-values.</p> required <code>alpha</code> <code>float</code> <p>Significance level for two-sided test. Default is 0.05. This flags the extreme 5% of p-values (2.5% in each tail), identifying observations that are unusually extreme under the model.</p> <code>0.05</code> <code>min_len</code> <code>int</code> <p>Minimum length for flagged runs. Default is 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>flags</code> <code>(ndarray, shape(n_time))</code> <p>Boolean array indicating flagged time points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck.periods import flag_extreme_pvalues\n&gt;&gt;&gt; pvalues = np.ones(20) * 0.5\n&gt;&gt;&gt; pvalues[5:10] = 0.01  # Very low p-values\n&gt;&gt;&gt; flags = flag_extreme_pvalues(pvalues, alpha=0.05, min_len=5)\n&gt;&gt;&gt; np.sum(flags[5:10])\n5\n&gt;&gt;&gt; # Combine with KL-based flags\n&gt;&gt;&gt; kl_flags = flag_extreme_kl(kl, z_thresh=3.0, min_len=5)\n&gt;&gt;&gt; combined = combine_flags(flags, kl_flags, min_votes=2, min_len=5)\n</code></pre> See Also <p>flag_extreme_kl : Flag extreme KL divergence times flag_low_overlap : Flag low HPD overlap periods combine_flags : Combine multiple diagnostic methods</p> Source code in <code>src/statespacecheck/periods.py</code> <pre><code>def flag_extreme_pvalues(\n    pvalues: NDArray[np.floating],\n    alpha: float = 0.05,\n    min_len: int = 5,\n) -&gt; NDArray[np.bool_]:\n    \"\"\"Two-sided extremeness test for predictive p-values.\n\n    Flags when pvalues &lt; alpha/2 or pvalues &gt; 1 - alpha/2.\n\n    Parameters\n    ----------\n    pvalues : np.ndarray, shape (n_time,)\n        Predictive p-values.\n    alpha : float, optional\n        Significance level for two-sided test. Default is 0.05.\n        This flags the extreme 5% of p-values (2.5% in each tail),\n        identifying observations that are unusually extreme under the model.\n    min_len : int, optional\n        Minimum length for flagged runs. Default is 5.\n\n    Returns\n    -------\n    flags : np.ndarray, shape (n_time,)\n        Boolean array indicating flagged time points.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck.periods import flag_extreme_pvalues\n    &gt;&gt;&gt; pvalues = np.ones(20) * 0.5\n    &gt;&gt;&gt; pvalues[5:10] = 0.01  # Very low p-values\n    &gt;&gt;&gt; flags = flag_extreme_pvalues(pvalues, alpha=0.05, min_len=5)\n    &gt;&gt;&gt; np.sum(flags[5:10])\n    5\n    &gt;&gt;&gt; # Combine with KL-based flags\n    &gt;&gt;&gt; kl_flags = flag_extreme_kl(kl, z_thresh=3.0, min_len=5)\n    &gt;&gt;&gt; combined = combine_flags(flags, kl_flags, min_votes=2, min_len=5)\n\n    See Also\n    --------\n    flag_extreme_kl : Flag extreme KL divergence times\n    flag_low_overlap : Flag low HPD overlap periods\n    combine_flags : Combine multiple diagnostic methods\n    \"\"\"\n    pvalues_arr = np.asarray(pvalues, dtype=float)\n    finite = np.isfinite(pvalues_arr)\n    too_low = pvalues_arr &lt; (alpha / 2.0)\n    too_high = pvalues_arr &gt; (1.0 - alpha / 2.0)\n    flags = finite &amp; (too_low | too_high)\n    return _enforce_min_len(flags, min_len)\n</code></pre>"},{"location":"reference/statespacecheck/periods/#statespacecheck.periods.combine_flags","title":"combine_flags","text":"<pre><code>combine_flags(*flags: NDArray[bool_], min_votes: int = 2, min_len: int = 5) -&gt; NDArray[bool_]\n</code></pre> <p>Majority-vote combination of multiple boolean flag arrays.</p> <p>Parameters:</p> Name Type Description Default <code>*flags</code> <code>bool arrays, each shape (n_time,)</code> <p>Variable number of boolean flag arrays to combine. Each should be 1D; all must have equal length.</p> <code>()</code> <code>min_votes</code> <code>int</code> <p>Number of agreeing methods required to flag a time point. Default is 2. For example, with 3 input flags and min_votes=2, a time point is flagged only if at least 2 of the 3 methods flag it.</p> <code>2</code> <code>min_len</code> <code>int</code> <p>Minimum length for flagged runs in final output. Default is 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>combined</code> <code>(ndarray, shape(n_time))</code> <p>Final boolean mask with short runs removed.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no flag arrays provided or if arrays have mismatched lengths.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck.periods import (\n&gt;&gt;&gt;     flag_extreme_kl, flag_extreme_pvalues, flag_low_overlap, combine_flags\n&gt;&gt;&gt; )\n&gt;&gt;&gt; # Combine two diagnostic methods (require both to agree)\n&gt;&gt;&gt; kl_flags = flag_extreme_kl(kl, z_thresh=3.0, min_len=5)\n&gt;&gt;&gt; overlap_flags = flag_low_overlap(overlap, tau=0.4, min_len=5)\n&gt;&gt;&gt; strict = combine_flags(kl_flags, overlap_flags, min_votes=2, min_len=5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Combine three methods (require any 2 to agree)\n&gt;&gt;&gt; pval_flags = flag_extreme_pvalues(pvals, alpha=0.05, min_len=5)\n&gt;&gt;&gt; moderate = combine_flags(\n&gt;&gt;&gt;     kl_flags, overlap_flags, pval_flags, min_votes=2, min_len=5\n&gt;&gt;&gt; )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Require all three methods to agree (strict consensus)\n&gt;&gt;&gt; consensus = combine_flags(\n&gt;&gt;&gt;     kl_flags, overlap_flags, pval_flags, min_votes=3, min_len=5\n&gt;&gt;&gt; )\n</code></pre> See Also <p>flag_extreme_kl : Flag extreme KL divergence times flag_extreme_pvalues : Flag extreme p-values flag_low_overlap : Flag low HPD overlap periods</p> Source code in <code>src/statespacecheck/periods.py</code> <pre><code>def combine_flags(\n    *flags: NDArray[np.bool_],\n    min_votes: int = 2,\n    min_len: int = 5,\n) -&gt; NDArray[np.bool_]:\n    \"\"\"Majority-vote combination of multiple boolean flag arrays.\n\n    Parameters\n    ----------\n    *flags : bool arrays, each shape (n_time,)\n        Variable number of boolean flag arrays to combine.\n        Each should be 1D; all must have equal length.\n    min_votes : int, optional\n        Number of agreeing methods required to flag a time point. Default is 2.\n        For example, with 3 input flags and min_votes=2, a time point is\n        flagged only if at least 2 of the 3 methods flag it.\n    min_len : int, optional\n        Minimum length for flagged runs in final output. Default is 5.\n\n    Returns\n    -------\n    combined : np.ndarray, shape (n_time,)\n        Final boolean mask with short runs removed.\n\n    Raises\n    ------\n    ValueError\n        If no flag arrays provided or if arrays have mismatched lengths.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck.periods import (\n    &gt;&gt;&gt;     flag_extreme_kl, flag_extreme_pvalues, flag_low_overlap, combine_flags\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; # Combine two diagnostic methods (require both to agree)\n    &gt;&gt;&gt; kl_flags = flag_extreme_kl(kl, z_thresh=3.0, min_len=5)\n    &gt;&gt;&gt; overlap_flags = flag_low_overlap(overlap, tau=0.4, min_len=5)\n    &gt;&gt;&gt; strict = combine_flags(kl_flags, overlap_flags, min_votes=2, min_len=5)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Combine three methods (require any 2 to agree)\n    &gt;&gt;&gt; pval_flags = flag_extreme_pvalues(pvals, alpha=0.05, min_len=5)\n    &gt;&gt;&gt; moderate = combine_flags(\n    &gt;&gt;&gt;     kl_flags, overlap_flags, pval_flags, min_votes=2, min_len=5\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Require all three methods to agree (strict consensus)\n    &gt;&gt;&gt; consensus = combine_flags(\n    &gt;&gt;&gt;     kl_flags, overlap_flags, pval_flags, min_votes=3, min_len=5\n    &gt;&gt;&gt; )\n\n    See Also\n    --------\n    flag_extreme_kl : Flag extreme KL divergence times\n    flag_extreme_pvalues : Flag extreme p-values\n    flag_low_overlap : Flag low HPD overlap periods\n    \"\"\"\n    if len(flags) == 0:\n        raise ValueError(\n            \"Error: No flag arrays provided.\\n\\n\"\n            \"What went wrong: combine_flags() requires at least one flag array.\\n\"\n            \"How to fix: Pass one or more boolean flag arrays as arguments, e.g.:\\n\"\n            \"    combined = combine_flags(kl_flags, overlap_flags, min_votes=2)\"\n        )\n    flag_arrays = [np.asarray(flag_arr, dtype=bool) for flag_arr in flags]\n    n_time = flag_arrays[0].shape[0]\n    if any(flag_arr.shape != (n_time,) for flag_arr in flag_arrays):\n        shapes = [flag_arr.shape for flag_arr in flag_arrays]\n        raise ValueError(\n            f\"Error: All flag arrays must be 1D with matching length.\\n\\n\"\n            f\"What went wrong: Flag arrays have mismatched shapes: {shapes}\\n\"\n            f\"Why: combine_flags() performs element-wise majority voting across time,\\n\"\n            f\"     requiring all flags to represent the same time points.\\n\\n\"\n            f\"How to fix:\\n\"\n            f\"  1. Check that all input arrays have shape (n_time,)\\n\"\n            f\"  2. Verify arrays come from same dataset with same time axis\\n\"\n            f\"  3. Ensure no accidental transposition or subsetting\"\n        )\n    votes = np.sum(np.stack(flag_arrays, axis=0), axis=0)\n    combined = votes &gt;= int(min_votes)\n    return _enforce_min_len(combined, min_len)\n</code></pre>"},{"location":"reference/statespacecheck/predictive_checks/","title":"statespacecheck.predictive_checks","text":""},{"location":"reference/statespacecheck/predictive_checks/#statespacecheck.predictive_checks","title":"predictive_checks","text":"<p>Predictive check functions for state space model goodness of fit.</p> <p>This module provides functions to compute predictive densities and perform predictive checks for Bayesian state space models.</p>"},{"location":"reference/statespacecheck/predictive_checks/#statespacecheck.predictive_checks-functions","title":"Functions","text":""},{"location":"reference/statespacecheck/predictive_checks/#statespacecheck.predictive_checks.predictive_density","title":"predictive_density","text":"<pre><code>predictive_density(state_dist: DistributionArray, likelihood: DistributionArray) -&gt; DistributionArray\n</code></pre> <p>Compute predictive density by integrating state dist with obs likelihood.</p> <p>CRITICAL: This function normalizes state_dist ONLY, NOT likelihood. The likelihood p(y|x) is a likelihood function, not a distribution over x. Normalizing it over x would change its value and mask real model misfit.</p> <p>Formula: f_predictive(y) = \u2211_x p(x) * p(y|x)</p> <p>Parameters:</p> Name Type Description Default <code>state_dist</code> <code>(ndarray, shape(n_time, ...))</code> <p>State probability distributions over position at each time point where ... represents arbitrary spatial dimensions. Non-negative values (NaN allowed to mark invalid bins). Will be normalized over spatial dimensions (everything except time).</p> required <code>likelihood</code> <code>(ndarray, shape(n_time, ...))</code> <p>Likelihood p(y|x) evaluated at observed data across all positions. Non-negative values (NaN allowed to mark invalid bins). DO NOT normalize - this is a likelihood function. Must have same shape as state_dist.</p> required <p>Returns:</p> Name Type Description <code>predictive_density</code> <code>(ndarray, shape(n_time))</code> <p>Predictive density at each time point.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If state_dist and likelihood have different shapes, or if distributions contain negative values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck import predictive_density\n&gt;&gt;&gt; # Simple 1D example with unnormalized state\n&gt;&gt;&gt; state = np.array([[3.0, 4.0, 3.0]])  # Unnormalized (sums to 10, not 1)\n&gt;&gt;&gt; like = np.array([[2.0, 3.0, 1.0]])  # Likelihood values (not normalized)\n&gt;&gt;&gt; pred = predictive_density(state, like)\n&gt;&gt;&gt; pred.shape\n(1,)\n</code></pre> See Also <p>log_predictive_density : Compute log predictive density for numerical stability kl_divergence : Measure information divergence between distributions hpd_overlap : Compute spatial overlap between HPD regions</p> Notes <p>The predictive density is computed via discrete Riemann sum:     f_predictive(y_k) = \u2211_x p(x_k) * p(y_k | x_k)</p> <p>Where: - p(x_k) is the state distribution (normalized to sum to 1) - p(y_k | x_k) is the observation likelihood (NOT normalized)</p> <p>Distributions are validated using validate_paired_distributions: - NaN/inf values in input are converted to 0.0 - Shape and non-negativity are checked - State distribution is normalized after validation - Likelihood is NOT normalized (critical for correct results)</p> <p>Integration is performed by flattening spatial dimensions and computing row-wise sums over all spatial bins.</p> Source code in <code>src/statespacecheck/predictive_checks.py</code> <pre><code>def predictive_density(\n    state_dist: DistributionArray,\n    likelihood: DistributionArray,\n) -&gt; DistributionArray:\n    \"\"\"Compute predictive density by integrating state dist with obs likelihood.\n\n    CRITICAL: This function normalizes state_dist ONLY, NOT likelihood.\n    The likelihood p(y|x) is a likelihood function, not a distribution over x.\n    Normalizing it over x would change its value and mask real model misfit.\n\n    Formula: f_predictive(y) = \u2211_x p(x) * p(y|x)\n\n    Parameters\n    ----------\n    state_dist : np.ndarray, shape (n_time, ...)\n        State probability distributions over position at each time point where\n        ... represents arbitrary spatial dimensions.\n        Non-negative values (NaN allowed to mark invalid bins).\n        Will be normalized over spatial dimensions (everything except time).\n    likelihood : np.ndarray, shape (n_time, ...)\n        Likelihood p(y|x) evaluated at observed data across all positions.\n        Non-negative values (NaN allowed to mark invalid bins).\n        DO NOT normalize - this is a likelihood function.\n        Must have same shape as state_dist.\n\n    Returns\n    -------\n    predictive_density : np.ndarray, shape (n_time,)\n        Predictive density at each time point.\n\n    Raises\n    ------\n    ValueError\n        If state_dist and likelihood have different shapes, or if distributions\n        contain negative values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck import predictive_density\n    &gt;&gt;&gt; # Simple 1D example with unnormalized state\n    &gt;&gt;&gt; state = np.array([[3.0, 4.0, 3.0]])  # Unnormalized (sums to 10, not 1)\n    &gt;&gt;&gt; like = np.array([[2.0, 3.0, 1.0]])  # Likelihood values (not normalized)\n    &gt;&gt;&gt; pred = predictive_density(state, like)\n    &gt;&gt;&gt; pred.shape\n    (1,)\n\n    See Also\n    --------\n    log_predictive_density : Compute log predictive density for numerical stability\n    kl_divergence : Measure information divergence between distributions\n    hpd_overlap : Compute spatial overlap between HPD regions\n\n    Notes\n    -----\n    The predictive density is computed via discrete Riemann sum:\n        f_predictive(y_k) = \u2211_x p(x_k) * p(y_k | x_k)\n\n    Where:\n    - p(x_k) is the state distribution (normalized to sum to 1)\n    - p(y_k | x_k) is the observation likelihood (NOT normalized)\n\n    Distributions are validated using validate_paired_distributions:\n    - NaN/inf values in input are converted to 0.0\n    - Shape and non-negativity are checked\n    - State distribution is normalized after validation\n    - Likelihood is NOT normalized (critical for correct results)\n\n    Integration is performed by flattening spatial dimensions and computing\n    row-wise sums over all spatial bins.\n    \"\"\"\n    # Validate both distributions (converts NaN/inf to 0, checks shapes)\n    state, like = validate_paired_distributions(\n        state_dist, likelihood, name1=\"state_dist\", name2=\"likelihood\", min_ndim=2\n    )\n\n    # Flatten for vectorized operations\n    state_flat = flatten_time_spatial(state)\n    like_flat = flatten_time_spatial(like)\n\n    # Normalize state distribution ONLY (not likelihood!)\n    # Shape: (n_time,)\n    state_sum = state_flat.sum(axis=1)\n\n    # Check for zero-sum state rows before normalization\n    zero_rows = state_sum == 0\n    if np.any(zero_rows):\n        warnings.warn(\n            \"state_dist has zero-sum rows; predictive set to NaN for those rows\",\n            UserWarning,\n            stacklevel=2,\n        )\n\n    # Normalize state, handling zero-sum rows\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        state_normalized = state_flat / state_sum[:, np.newaxis]\n\n    # Replace non-finite values (from zero-sum rows) with 0\n    state_normalized = np.nan_to_num(state_normalized, nan=0.0, posinf=0.0, neginf=0.0)\n\n    # Compute predictive density: sum over spatial dimensions\n    # f_predictive(y) = \u2211_x p(x) * p(y|x)\n    # Note: likelihood is NOT normalized (critical!)\n    predictive: DistributionArray = (state_normalized * like_flat).sum(axis=1)\n\n    # Set zero-sum rows to NaN (they have no valid state mass)\n    predictive[zero_rows] = np.nan\n\n    return predictive\n</code></pre>"},{"location":"reference/statespacecheck/predictive_checks/#statespacecheck.predictive_checks.log_predictive_density","title":"log_predictive_density","text":"<pre><code>log_predictive_density(state_dist: DistributionArray, likelihood: DistributionArray | None = None, log_likelihood: DistributionArray | None = None) -&gt; DistributionArray\n</code></pre> <p>Compute log predictive density directly in log-space using logsumexp.</p> <p>CRITICAL: This function normalizes state_dist ONLY, NOT likelihood. Computes log predictive density natively in log-space for numerical stability. DO NOT compute as np.log(predictive_density(...)) - this loses precision.</p> <p>Formula: log f_predictive(y) = log \u2211_x p(x) * p(y|x)          = logsumexp(log p(x) + log p(y|x))</p> <p>Parameters:</p> Name Type Description Default <code>state_dist</code> <code>(ndarray, shape(n_time, ...))</code> <p>State probability distributions over position at each time point where ... represents arbitrary spatial dimensions. Non-negative values (NaN allowed to mark invalid bins). Will be normalized over spatial dimensions (everything except time).</p> required <code>likelihood</code> <code>(ndarray, shape(n_time, ...))</code> <p>Likelihood p(y|x) evaluated at observed data across all positions. Non-negative values (NaN allowed to mark invalid bins). DO NOT normalize - this is a likelihood function. Must have same shape as state_dist. Exactly one of <code>likelihood</code> or <code>log_likelihood</code> must be provided.</p> <code>None</code> <code>log_likelihood</code> <code>(ndarray, shape(n_time, ...))</code> <p>Log-likelihood log p(y|x) evaluated at observed data. Allows users who already have log-likelihood to avoid exp/log round-trip. Must have same shape as state_dist. Exactly one of <code>likelihood</code> or <code>log_likelihood</code> must be provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>log_predictive_density</code> <code>(ndarray, shape(n_time))</code> <p>Log predictive density at each time point.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither or both of likelihood and log_likelihood are provided, if shapes don't match, or if distributions contain negative values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck import log_predictive_density\n&gt;&gt;&gt; # Using likelihood\n&gt;&gt;&gt; state = np.array([[1.0, 1.0, 1.0]])\n&gt;&gt;&gt; like = np.array([[2.0, 3.0, 4.0]])\n&gt;&gt;&gt; log_pred = log_predictive_density(state, likelihood=like)\n&gt;&gt;&gt; log_pred.shape\n(1,)\n</code></pre> <pre><code>&gt;&gt;&gt; # Using log_likelihood for numerical stability\n&gt;&gt;&gt; log_like = np.log(like)\n&gt;&gt;&gt; log_pred2 = log_predictive_density(state, log_likelihood=log_like)\n&gt;&gt;&gt; np.allclose(log_pred, log_pred2)\nTrue\n</code></pre> See Also <p>predictive_density : Compute predictive density in linear space kl_divergence : Measure information divergence between distributions hpd_overlap : Compute spatial overlap between HPD regions</p> Notes <p>This function computes log predictive density directly in log-space using scipy.special.logsumexp for numerical stability. This prevents underflow when working with very small probabilities or peaked distributions.</p> <p>The computation is:     log \u2211_x p(x) * p(y|x) = logsumexp(log p(x) + log p(y|x))</p> <p>Where: - p(x) is the state distribution (normalized to sum to 1) - p(y|x) is the observation likelihood (NOT normalized)</p> <p>For users who already have log-likelihood computed, passing it via <code>log_likelihood</code> parameter avoids the exp/log round-trip and is more efficient and numerically stable.</p> Source code in <code>src/statespacecheck/predictive_checks.py</code> <pre><code>def log_predictive_density(\n    state_dist: DistributionArray,\n    likelihood: DistributionArray | None = None,\n    log_likelihood: DistributionArray | None = None,\n) -&gt; DistributionArray:\n    \"\"\"Compute log predictive density directly in log-space using logsumexp.\n\n    CRITICAL: This function normalizes state_dist ONLY, NOT likelihood.\n    Computes log predictive density natively in log-space for numerical stability.\n    DO NOT compute as np.log(predictive_density(...)) - this loses precision.\n\n    Formula: log f_predictive(y) = log \u2211_x p(x) * p(y|x)\n             = logsumexp(log p(x) + log p(y|x))\n\n    Parameters\n    ----------\n    state_dist : np.ndarray, shape (n_time, ...)\n        State probability distributions over position at each time point where\n        ... represents arbitrary spatial dimensions.\n        Non-negative values (NaN allowed to mark invalid bins).\n        Will be normalized over spatial dimensions (everything except time).\n    likelihood : np.ndarray, shape (n_time, ...), optional\n        Likelihood p(y|x) evaluated at observed data across all positions.\n        Non-negative values (NaN allowed to mark invalid bins).\n        DO NOT normalize - this is a likelihood function.\n        Must have same shape as state_dist.\n        Exactly one of `likelihood` or `log_likelihood` must be provided.\n    log_likelihood : np.ndarray, shape (n_time, ...), optional\n        Log-likelihood log p(y|x) evaluated at observed data.\n        Allows users who already have log-likelihood to avoid exp/log round-trip.\n        Must have same shape as state_dist.\n        Exactly one of `likelihood` or `log_likelihood` must be provided.\n\n    Returns\n    -------\n    log_predictive_density : np.ndarray, shape (n_time,)\n        Log predictive density at each time point.\n\n    Raises\n    ------\n    ValueError\n        If neither or both of likelihood and log_likelihood are provided,\n        if shapes don't match, or if distributions contain negative values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck import log_predictive_density\n    &gt;&gt;&gt; # Using likelihood\n    &gt;&gt;&gt; state = np.array([[1.0, 1.0, 1.0]])\n    &gt;&gt;&gt; like = np.array([[2.0, 3.0, 4.0]])\n    &gt;&gt;&gt; log_pred = log_predictive_density(state, likelihood=like)\n    &gt;&gt;&gt; log_pred.shape\n    (1,)\n\n    &gt;&gt;&gt; # Using log_likelihood for numerical stability\n    &gt;&gt;&gt; log_like = np.log(like)\n    &gt;&gt;&gt; log_pred2 = log_predictive_density(state, log_likelihood=log_like)\n    &gt;&gt;&gt; np.allclose(log_pred, log_pred2)\n    True\n\n    See Also\n    --------\n    predictive_density : Compute predictive density in linear space\n    kl_divergence : Measure information divergence between distributions\n    hpd_overlap : Compute spatial overlap between HPD regions\n\n    Notes\n    -----\n    This function computes log predictive density directly in log-space using\n    scipy.special.logsumexp for numerical stability. This prevents underflow\n    when working with very small probabilities or peaked distributions.\n\n    The computation is:\n        log \u2211_x p(x) * p(y|x) = logsumexp(log p(x) + log p(y|x))\n\n    Where:\n    - p(x) is the state distribution (normalized to sum to 1)\n    - p(y|x) is the observation likelihood (NOT normalized)\n\n    For users who already have log-likelihood computed, passing it via\n    `log_likelihood` parameter avoids the exp/log round-trip and is more\n    efficient and numerically stable.\n    \"\"\"\n    # Validate that exactly one of likelihood or log_likelihood is provided\n    if (likelihood is None) == (log_likelihood is None):\n        raise ValueError(\"Exactly one of 'likelihood' or 'log_likelihood' must be provided\")\n\n    # Convert likelihood to log_likelihood if needed\n    if likelihood is not None:\n        # Validate both distributions (both are probabilities)\n        state, like = validate_paired_distributions(\n            state_dist, likelihood, name1=\"state_dist\", name2=\"likelihood\", min_ndim=2\n        )\n        # Convert to log-space (avoiding log(0) by using where)\n        like_flat = flatten_time_spatial(like)\n        with np.errstate(divide=\"ignore\"):\n            log_like_flat = np.where(like_flat &gt; 0, np.log(like_flat), -np.inf)\n    else:\n        # Validate state_dist (it's a probability)\n        state = validate_distribution(state_dist, name=\"state_dist\", min_ndim=2)\n\n        # Validate log_likelihood manually (it's in log-space, can be negative!)\n        log_like = np.asarray(log_likelihood, dtype=float)\n\n        if log_like.ndim &lt; 2:\n            raise ValueError(\n                f\"log_likelihood must be at least 2D with shape (n_time, ...), \"\n                f\"got shape {log_like.shape}\"\n            )\n\n        if log_like.shape != state.shape:\n            raise ValueError(\n                f\"state_dist and log_likelihood must have same shape, \"\n                f\"got {state.shape} vs {log_like.shape}\"\n            )\n\n        # Check for +inf in log_likelihood (indicates upstream bug or overflow)\n        if np.isposinf(log_like).any():\n            raise ValueError(\n                \"log_likelihood contains +inf; this indicates an upstream bug or overflow\"\n            )\n\n        # Handle non-finite values: NaN \u2192 -inf (makes sense in log-space)\n        # Note: We do NOT check for negative values (negative is expected in log-space!)\n        log_like = np.nan_to_num(log_like, nan=-np.inf, posinf=np.inf, neginf=-np.inf)\n\n        log_like_flat = flatten_time_spatial(log_like)\n\n    # Flatten state for vectorized operations\n    state_flat = flatten_time_spatial(state)\n\n    # Normalize state distribution ONLY (not likelihood!)\n    state_sum = state_flat.sum(axis=1)\n\n    # Check for zero-sum state rows before normalization\n    zero_rows = state_sum == 0\n    if np.any(zero_rows):\n        warnings.warn(\n            \"state_dist has zero-sum rows; predictive set to NaN for those rows\",\n            UserWarning,\n            stacklevel=2,\n        )\n\n    # Normalize state, handling zero-sum rows\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        state_normalized = state_flat / state_sum[:, np.newaxis]\n\n    # Replace non-finite values (from zero-sum rows) with 0\n    state_normalized = np.nan_to_num(state_normalized, nan=0.0, posinf=0.0, neginf=0.0)\n\n    # Convert normalized state to log-space\n    with np.errstate(divide=\"ignore\"):\n        log_state_normalized = np.where(state_normalized &gt; 0, np.log(state_normalized), -np.inf)\n\n    # Compute log predictive density using logsumexp\n    # log \u2211_x p(x) * p(y|x) = logsumexp(log p(x) + log p(y|x))\n    log_predictive: DistributionArray = logsumexp(log_state_normalized + log_like_flat, axis=1)\n\n    # Set zero-sum rows to NaN (they have no valid state mass)\n    log_predictive[zero_rows] = np.nan\n\n    return log_predictive\n</code></pre>"},{"location":"reference/statespacecheck/predictive_checks/#statespacecheck.predictive_checks.predictive_pvalue","title":"predictive_pvalue","text":"<pre><code>predictive_pvalue(observed_log_pred: DistributionArray, sample_log_pred: Callable[[int], DistributionArray], *, n_samples: int = 1000) -&gt; DistributionArray\n</code></pre> <p>Compute predictive p-value via Monte Carlo sampling.</p> <p>Computes p-values for predictive checks by comparing observed log predictive densities to a distribution of simulated log predictive densities. The p-value at each time point is the proportion of simulated values that are greater than or equal to the observed value.</p> <p>This provides a posterior predictive check: if the model is correct, p-values should be uniformly distributed. Systematic deviations indicate model misfit.</p> <p>Parameters:</p> Name Type Description Default <code>observed_log_pred</code> <code>(ndarray, shape(n_time))</code> <p>Observed log predictive densities for actual data. Must be 1-dimensional.</p> required <code>sample_log_pred</code> <code>callable</code> <p>Function that generates samples of log predictive densities under the model. Must accept a single integer argument <code>n_samples</code> and return an array of shape (n_samples, n_time) containing simulated log predictive densities. For reproducibility, use np.random.Generator with a fixed seed internally. Example: <code>lambda n: rng.normal(loc=model_mean, scale=model_std, size=(n, n_time))</code></p> required <code>n_samples</code> <code>int</code> <p>Number of Monte Carlo samples to draw for p-value computation. Higher values give more accurate p-value estimates but take longer. Default is 1000.</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>p_values</code> <code>(ndarray, shape(n_time))</code> <p>P-value at each time point, computed as the proportion of simulated log predictive densities &gt;= observed value. Values range from 0 to 1.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If observed_log_pred is not 1-dimensional, if n_samples &lt;= 0, or if sample_log_pred returns array with wrong shape.</p> <code>TypeError</code> <p>If sample_log_pred is not callable.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck import predictive_pvalue\n&gt;&gt;&gt; # Observed log predictive densities\n&gt;&gt;&gt; observed = np.array([-2.0, -1.5, -1.0])\n&gt;&gt;&gt; # Sampler with internal random state for reproducibility\n&gt;&gt;&gt; def sampler(n_samples):\n...     rng = np.random.default_rng(42)  # Fixed seed for reproducibility\n...     return rng.normal(loc=-1.5, scale=0.5, size=(n_samples, 3))\n&gt;&gt;&gt; p_vals = predictive_pvalue(observed, sampler, n_samples=100)\n&gt;&gt;&gt; p_vals.shape\n(3,)\n&gt;&gt;&gt; np.all((p_vals &gt;= 0) &amp; (p_vals &lt;= 1))\nTrue\n</code></pre> See Also <p>log_predictive_density : Compute log predictive density for observed data predictive_density : Compute predictive density in linear space aggregate_over_period : Aggregate metrics over time periods</p> Notes <p>The p-value at time t is computed as:     p_value[t] = (1 / n_samples) * sum(simulated[t] &gt;= observed[t])</p> <p>Interpretation: - p-value near 0.5: observed data consistent with model - p-value near 0 or 1: observed data extreme relative to model predictions - Systematic patterns across time suggest model misspecification</p> <p>The sampler function should: 1. Generate new data from the model 2. Compute log predictive density for each generated dataset 3. Return array of shape (n_samples, n_time) 4. Use np.random.Generator internally for reproducibility</p> <p>For reproducible results, create your sampler with a fixed seed:     rng = np.random.default_rng(42)     sampler = lambda n: rng.normal(size=(n, n_time))</p> Source code in <code>src/statespacecheck/predictive_checks.py</code> <pre><code>def predictive_pvalue(\n    observed_log_pred: DistributionArray,\n    sample_log_pred: Callable[[int], DistributionArray],\n    *,\n    n_samples: int = 1000,\n) -&gt; DistributionArray:\n    \"\"\"Compute predictive p-value via Monte Carlo sampling.\n\n    Computes p-values for predictive checks by comparing observed log predictive\n    densities to a distribution of simulated log predictive densities. The p-value\n    at each time point is the proportion of simulated values that are greater than\n    or equal to the observed value.\n\n    This provides a posterior predictive check: if the model is correct, p-values\n    should be uniformly distributed. Systematic deviations indicate model misfit.\n\n    Parameters\n    ----------\n    observed_log_pred : np.ndarray, shape (n_time,)\n        Observed log predictive densities for actual data.\n        Must be 1-dimensional.\n    sample_log_pred : callable\n        Function that generates samples of log predictive densities under the model.\n        Must accept a single integer argument `n_samples` and return an array of\n        shape (n_samples, n_time) containing simulated log predictive densities.\n        For reproducibility, use np.random.Generator with a fixed seed internally.\n        Example: `lambda n: rng.normal(loc=model_mean, scale=model_std, size=(n, n_time))`\n    n_samples : int, optional\n        Number of Monte Carlo samples to draw for p-value computation.\n        Higher values give more accurate p-value estimates but take longer.\n        Default is 1000.\n\n    Returns\n    -------\n    p_values : np.ndarray, shape (n_time,)\n        P-value at each time point, computed as the proportion of simulated\n        log predictive densities &gt;= observed value.\n        Values range from 0 to 1.\n\n    Raises\n    ------\n    ValueError\n        If observed_log_pred is not 1-dimensional, if n_samples &lt;= 0,\n        or if sample_log_pred returns array with wrong shape.\n    TypeError\n        If sample_log_pred is not callable.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck import predictive_pvalue\n    &gt;&gt;&gt; # Observed log predictive densities\n    &gt;&gt;&gt; observed = np.array([-2.0, -1.5, -1.0])\n    &gt;&gt;&gt; # Sampler with internal random state for reproducibility\n    &gt;&gt;&gt; def sampler(n_samples):\n    ...     rng = np.random.default_rng(42)  # Fixed seed for reproducibility\n    ...     return rng.normal(loc=-1.5, scale=0.5, size=(n_samples, 3))\n    &gt;&gt;&gt; p_vals = predictive_pvalue(observed, sampler, n_samples=100)\n    &gt;&gt;&gt; p_vals.shape\n    (3,)\n    &gt;&gt;&gt; np.all((p_vals &gt;= 0) &amp; (p_vals &lt;= 1))\n    True\n\n    See Also\n    --------\n    log_predictive_density : Compute log predictive density for observed data\n    predictive_density : Compute predictive density in linear space\n    aggregate_over_period : Aggregate metrics over time periods\n\n    Notes\n    -----\n    The p-value at time t is computed as:\n        p_value[t] = (1 / n_samples) * sum(simulated[t] &gt;= observed[t])\n\n    Interpretation:\n    - p-value near 0.5: observed data consistent with model\n    - p-value near 0 or 1: observed data extreme relative to model predictions\n    - Systematic patterns across time suggest model misspecification\n\n    The sampler function should:\n    1. Generate new data from the model\n    2. Compute log predictive density for each generated dataset\n    3. Return array of shape (n_samples, n_time)\n    4. Use np.random.Generator internally for reproducibility\n\n    For reproducible results, create your sampler with a fixed seed:\n        rng = np.random.default_rng(42)\n        sampler = lambda n: rng.normal(size=(n, n_time))\n    \"\"\"\n    # Validate observed_log_pred\n    observed_arr = np.asarray(observed_log_pred, dtype=float)\n    if observed_arr.ndim != 1:\n        msg = (\n            f\"observed_log_pred must be 1-dimensional, \"\n            f\"got {observed_arr.ndim}D array with shape {observed_arr.shape}\"\n        )\n        raise ValueError(msg)\n\n    n_time = observed_arr.shape[0]\n\n    # Validate n_samples\n    if n_samples &lt;= 0:\n        msg = f\"n_samples must be positive, got {n_samples}\"\n        raise ValueError(msg)\n\n    # Validate sample_log_pred is callable\n    if not callable(sample_log_pred):\n        msg = f\"sample_log_pred must be callable, got {type(sample_log_pred).__name__}\"\n        raise TypeError(msg)\n\n    # Generate samples\n    simulated = sample_log_pred(n_samples)\n\n    # Validate shape of simulated samples\n    simulated_arr = np.asarray(simulated, dtype=float)\n    if simulated_arr.shape != (n_samples, n_time):\n        msg = (\n            f\"sample_log_pred output must have shape (n_samples, n_time) = \"\n            f\"({n_samples}, {n_time}), got shape {simulated_arr.shape}\"\n        )\n        raise ValueError(msg)\n\n    # Compute p-values: proportion of samples &gt;= observed\n    # Broadcasting: observed_arr has shape (n_time,), simulated_arr has shape (n_samples, n_time)\n    # Comparison broadcasts to (n_samples, n_time), then mean over axis=0 gives (n_time,)\n    # Explicit type annotation for mypy strict mode\n    p_values: DistributionArray = np.mean(simulated_arr &gt;= observed_arr, axis=0)\n    return p_values\n</code></pre>"},{"location":"reference/statespacecheck/state_consistency/","title":"statespacecheck.state_consistency","text":""},{"location":"reference/statespacecheck/state_consistency/#statespacecheck.state_consistency","title":"state_consistency","text":"<p>State consistency tests for state space model goodness of fit.</p> <p>This module provides functions to assess the consistency between state distributions and their component likelihood distributions in Bayesian state space models. These tests help identify issues with prior specification and model assumptions.</p>"},{"location":"reference/statespacecheck/state_consistency/#statespacecheck.state_consistency-functions","title":"Functions","text":""},{"location":"reference/statespacecheck/state_consistency/#statespacecheck.state_consistency.kl_divergence","title":"kl_divergence","text":"<pre><code>kl_divergence(state_dist: DistributionArray, likelihood: DistributionArray) -&gt; DistributionArray\n</code></pre> <p>Compute Kullback-Leibler divergence between state distribution and likelihood.</p> <p>Measures the information divergence between the state distribution and likelihood distributions at each time point. Large divergences may indicate issues with the prior specification or model assumptions.</p> <p>Parameters:</p> Name Type Description Default <code>state_dist</code> <code>(ndarray, shape(n_time, ...))</code> <p>State probability distributions over position at each time point where ... represents arbitrary spatial dimensions. Can be either one-step predictive distribution or smoother output. Non-negative values (NaN allowed to mark invalid bins). Automatically normalized over valid (non-NaN) bins.</p> required <code>likelihood</code> <code>(ndarray, shape(n_time, ...))</code> <p>Likelihood distributions at each time point. This is the likelihood p(y_t | x_t) across spatial positions. Non-negative values (NaN allowed to mark invalid bins). Automatically normalized over valid (non-NaN) bins. Must have same shape as state_dist.</p> required <p>Returns:</p> Name Type Description <code>kl_divergence</code> <code>(ndarray, shape(n_time))</code> <p>Kullback-Leibler divergence D_KL(state_dist || likelihood) at each time point. Values are non-negative, with 0 indicating identical distributions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If state_dist and likelihood have different shapes, or if distributions contain negative values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck import kl_divergence\n&gt;&gt;&gt; # Identical distributions have zero divergence\n&gt;&gt;&gt; state = np.array([[0.3, 0.4, 0.3]])\n&gt;&gt;&gt; like = np.array([[0.3, 0.4, 0.3]])\n&gt;&gt;&gt; div = kl_divergence(state, like)\n&gt;&gt;&gt; div.shape\n(1,)\n&gt;&gt;&gt; bool(np.isclose(div[0], 0.0))\nTrue\n</code></pre> See Also <p>hpd_overlap : Compute spatial overlap between HPD regions highest_density_region : Compute highest density region mask</p> Notes <p>The KL divergence is computed using scipy.stats.entropy with the formula: D_KL(P || Q) = sum(P * log(P / Q)) where P is the state distribution and Q is the likelihood.</p> <p>Distributions are automatically normalized over valid (non-NaN) bins. NaN values mark invalid spatial bins (e.g., inaccessible locations) and are excluded from both normalization and KL computation.</p> <p>Time slices where distributions have no valid mass return inf for the divergence.</p> Source code in <code>src/statespacecheck/state_consistency.py</code> <pre><code>def kl_divergence(\n    state_dist: DistributionArray, likelihood: DistributionArray\n) -&gt; DistributionArray:\n    \"\"\"Compute Kullback-Leibler divergence between state distribution and likelihood.\n\n    Measures the information divergence between the state distribution and likelihood\n    distributions at each time point. Large divergences may indicate issues\n    with the prior specification or model assumptions.\n\n    Parameters\n    ----------\n    state_dist : np.ndarray, shape (n_time, ...)\n        State probability distributions over position at each time point where\n        ... represents arbitrary spatial dimensions.\n        Can be either one-step predictive distribution or smoother output.\n        Non-negative values (NaN allowed to mark invalid bins).\n        Automatically normalized over valid (non-NaN) bins.\n    likelihood : np.ndarray, shape (n_time, ...)\n        Likelihood distributions at each time point. This is the\n        likelihood p(y_t | x_t) across spatial positions.\n        Non-negative values (NaN allowed to mark invalid bins).\n        Automatically normalized over valid (non-NaN) bins.\n        Must have same shape as state_dist.\n\n    Returns\n    -------\n    kl_divergence : np.ndarray, shape (n_time,)\n        Kullback-Leibler divergence D_KL(state_dist || likelihood) at each\n        time point. Values are non-negative, with 0 indicating identical\n        distributions.\n\n    Raises\n    ------\n    ValueError\n        If state_dist and likelihood have different shapes, or if distributions\n        contain negative values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck import kl_divergence\n    &gt;&gt;&gt; # Identical distributions have zero divergence\n    &gt;&gt;&gt; state = np.array([[0.3, 0.4, 0.3]])\n    &gt;&gt;&gt; like = np.array([[0.3, 0.4, 0.3]])\n    &gt;&gt;&gt; div = kl_divergence(state, like)\n    &gt;&gt;&gt; div.shape\n    (1,)\n    &gt;&gt;&gt; bool(np.isclose(div[0], 0.0))\n    True\n\n    See Also\n    --------\n    hpd_overlap : Compute spatial overlap between HPD regions\n    highest_density_region : Compute highest density region mask\n\n    Notes\n    -----\n    The KL divergence is computed using scipy.stats.entropy with the formula:\n    D_KL(P || Q) = sum(P * log(P / Q))\n    where P is the state distribution and Q is the likelihood.\n\n    Distributions are automatically normalized over valid (non-NaN) bins.\n    NaN values mark invalid spatial bins (e.g., inaccessible locations)\n    and are excluded from both normalization and KL computation.\n\n    Time slices where distributions have no valid mass return inf for the divergence.\n\n    \"\"\"\n    # Validate and normalize distributions (handles NaN correctly)\n    state_norm, like_norm = _validate_and_normalize_distributions(state_dist, likelihood)\n\n    n_time = state_norm.shape[0]\n\n    # Flatten all spatial dimensions\n    state_flat = state_norm.reshape(n_time, -1)\n    like_flat = like_norm.reshape(n_time, -1)\n\n    # Check for empty rows (sum == 0)\n    # After normalization, arrays have no NaNs: valid rows sum to 1.0, empty rows sum to 0.0\n    state_sum = state_flat.sum(axis=1)\n    like_sum = like_flat.sum(axis=1)\n\n    # Initialize output with inf for invalid time slices\n    kl_div: DistributionArray = np.full(n_time, np.inf, dtype=float)\n\n    # Find valid time slices (both distributions have positive mass over valid bins)\n    valid = (state_sum &gt; 0) &amp; (like_sum &gt; 0)\n\n    # Compute entropy for valid time slices\n    # NaN already converted to 0 by validation\n    if np.any(valid):\n        kl_div[valid] = entropy(state_flat[valid], like_flat[valid], axis=1)\n\n    # Clip to non-negative values to handle floating point precision errors\n    # scipy.stats.entropy can return tiny negative values (~1e-113) with subnormal numbers\n    # KL divergence is mathematically always non-negative, so clip spurious negatives to 0\n    kl_div = np.maximum(kl_div, 0.0)\n\n    return kl_div\n</code></pre>"},{"location":"reference/statespacecheck/state_consistency/#statespacecheck.state_consistency.hpd_overlap","title":"hpd_overlap","text":"<pre><code>hpd_overlap(state_dist: DistributionArray, likelihood: DistributionArray, *, coverage: float = DEFAULT_COVERAGE) -&gt; DistributionArray\n</code></pre> <p>Compute overlap between HPD regions of state distribution and likelihood.</p> <p>Measures the spatial overlap between the highest posterior density regions of the state distribution and likelihood distributions. High overlap suggests consistency between the likelihood and prior contributions to the state estimate.</p> <p>Parameters:</p> Name Type Description Default <code>state_dist</code> <code>(ndarray, shape(n_time, ...))</code> <p>State probability distributions over position at each time point where ... represents arbitrary spatial dimensions. Can be either one-step predictive distribution or smoother output. Non-negative values (NaN allowed to mark invalid bins). Automatically normalized over valid (non-NaN) bins.</p> required <code>likelihood</code> <code>(ndarray, shape(n_time, ...))</code> <p>Likelihood distributions at each time point. This is the likelihood p(y_t | x_t) across spatial positions. Non-negative values (NaN allowed to mark invalid bins). Automatically normalized over valid (non-NaN) bins. Must have same shape as state_dist.</p> required <code>coverage</code> <code>float</code> <p>Coverage probability for the HPD regions. Must be between 0 and 1. Default is 0.95 for 95% HPD regions.</p> <code>DEFAULT_COVERAGE</code> <p>Returns:</p> Name Type Description <code>hpd_overlap</code> <code>(ndarray, shape(n_time))</code> <p>Proportion of overlap between the HPD regions of state_dist and likelihood at each time point. Values range from 0 (no overlap) to 1 (complete overlap).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If state_dist and likelihood have different shapes, if coverage is not in (0, 1), or if distributions contain negative values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statespacecheck import hpd_overlap\n&gt;&gt;&gt; # Identical distributions have perfect overlap\n&gt;&gt;&gt; state = np.array([[0.3, 0.4, 0.3]])\n&gt;&gt;&gt; like = np.array([[0.3, 0.4, 0.3]])\n&gt;&gt;&gt; overlap = hpd_overlap(state, like, coverage=0.9)\n&gt;&gt;&gt; overlap.shape\n(1,)\n&gt;&gt;&gt; bool(overlap[0] &gt;= 0.0 and overlap[0] &lt;= 1.0)\nTrue\n</code></pre> See Also <p>kl_divergence : Measure information divergence between distributions highest_density_region : Compute highest density region mask</p> Notes <p>The overlap is computed as:     overlap = intersection(HPD_state, HPD_like) / min(size(HPD_state), size(HPD_like))</p> <p>This normalization ensures that: - overlap = 1.0 when one region completely contains the other - overlap = 0.0 when regions don't overlap at all - Values are comparable even when HPD regions have different sizes</p> <p>When both HPD regions are empty (both sizes are 0), overlap is defined as 0.</p> <p>Distributions are automatically normalized over valid (non-NaN) bins. NaN values mark invalid spatial bins (e.g., inaccessible locations) and are excluded from both normalization and HPD region computation.</p> Source code in <code>src/statespacecheck/state_consistency.py</code> <pre><code>def hpd_overlap(\n    state_dist: DistributionArray,\n    likelihood: DistributionArray,\n    *,\n    coverage: float = DEFAULT_COVERAGE,\n) -&gt; DistributionArray:\n    \"\"\"Compute overlap between HPD regions of state distribution and likelihood.\n\n    Measures the spatial overlap between the highest posterior density regions\n    of the state distribution and likelihood distributions. High overlap suggests\n    consistency between the likelihood and prior contributions to the state estimate.\n\n    Parameters\n    ----------\n    state_dist : np.ndarray, shape (n_time, ...)\n        State probability distributions over position at each time point where\n        ... represents arbitrary spatial dimensions.\n        Can be either one-step predictive distribution or smoother output.\n        Non-negative values (NaN allowed to mark invalid bins).\n        Automatically normalized over valid (non-NaN) bins.\n    likelihood : np.ndarray, shape (n_time, ...)\n        Likelihood distributions at each time point. This is the\n        likelihood p(y_t | x_t) across spatial positions.\n        Non-negative values (NaN allowed to mark invalid bins).\n        Automatically normalized over valid (non-NaN) bins.\n        Must have same shape as state_dist.\n    coverage : float, optional\n        Coverage probability for the HPD regions. Must be between 0 and 1.\n        Default is 0.95 for 95% HPD regions.\n\n    Returns\n    -------\n    hpd_overlap : np.ndarray, shape (n_time,)\n        Proportion of overlap between the HPD regions of state_dist and\n        likelihood at each time point. Values range from 0 (no overlap)\n        to 1 (complete overlap).\n\n    Raises\n    ------\n    ValueError\n        If state_dist and likelihood have different shapes, if coverage\n        is not in (0, 1), or if distributions contain negative values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from statespacecheck import hpd_overlap\n    &gt;&gt;&gt; # Identical distributions have perfect overlap\n    &gt;&gt;&gt; state = np.array([[0.3, 0.4, 0.3]])\n    &gt;&gt;&gt; like = np.array([[0.3, 0.4, 0.3]])\n    &gt;&gt;&gt; overlap = hpd_overlap(state, like, coverage=0.9)\n    &gt;&gt;&gt; overlap.shape\n    (1,)\n    &gt;&gt;&gt; bool(overlap[0] &gt;= 0.0 and overlap[0] &lt;= 1.0)\n    True\n\n    See Also\n    --------\n    kl_divergence : Measure information divergence between distributions\n    highest_density_region : Compute highest density region mask\n\n    Notes\n    -----\n    The overlap is computed as:\n        overlap = intersection(HPD_state, HPD_like) / min(size(HPD_state), size(HPD_like))\n\n    This normalization ensures that:\n    - overlap = 1.0 when one region completely contains the other\n    - overlap = 0.0 when regions don't overlap at all\n    - Values are comparable even when HPD regions have different sizes\n\n    When both HPD regions are empty (both sizes are 0), overlap is defined as 0.\n\n    Distributions are automatically normalized over valid (non-NaN) bins.\n    NaN values mark invalid spatial bins (e.g., inaccessible locations)\n    and are excluded from both normalization and HPD region computation.\n\n    \"\"\"\n    validate_coverage(coverage)\n\n    # Validate but don't normalize - HPD works on relative magnitudes (unnormalized weights)\n    # This saves 2 full array normalizations for large datasets\n    state, like = validate_paired_distributions(\n        state_dist, likelihood, name1=\"state_dist\", name2=\"likelihood\", min_ndim=2\n    )\n\n    # Get HPD regions (highest_density_region works on unnormalized weights)\n    mask_state = highest_density_region(state, coverage=coverage)\n    mask_like = highest_density_region(like, coverage=coverage)\n\n    # Sum over all spatial dimensions (everything except time)\n    spatial_axes = get_spatial_axes(state)\n    size_state = mask_state.sum(axis=spatial_axes)\n    size_like = mask_like.sum(axis=spatial_axes)\n    intersection = (mask_state &amp; mask_like).sum(axis=spatial_axes)\n\n    # Compute denominator (minimum of the two sizes)\n    denom = np.minimum(size_state, size_like)\n\n    # Handle division by zero: when denom is 0, overlap is 0\n    # This matches the normalization pattern used elsewhere in the codebase\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        overlap: DistributionArray = intersection / denom\n    overlap = np.nan_to_num(overlap, nan=0.0, posinf=0.0, neginf=0.0)\n\n    return overlap\n</code></pre>"},{"location":"reference/statespacecheck/viz/","title":"statespacecheck.viz","text":""},{"location":"reference/statespacecheck/viz/#statespacecheck.viz","title":"viz","text":"<p>Visualization utilities for diagnostic plots.</p> <p>This module provides functions to create diagnostic plots for state space model goodness-of-fit assessment, including HPD overlap, KL divergence, and predictive p-values with optional flagged period highlighting.</p>"},{"location":"reference/statespacecheck/viz/#statespacecheck.viz-functions","title":"Functions","text":""},{"location":"reference/statespacecheck/viz/#statespacecheck.viz.plot_diagnostics","title":"plot_diagnostics","text":"<pre><code>plot_diagnostics(time: NDArray[floating], overlap: NDArray[floating], kl: NDArray[floating], pvals: NDArray[floating], flags: NDArray[bool_] | None = None, tau: float = 0.4, z_thresh: float = 3.0, alpha: float = 0.05) -&gt; Figure\n</code></pre> <p>Plot HPD overlap, KL (with robust z-band), and predictive p-values.</p> <p>Creates a three-panel diagnostic plot showing: 1. HPD overlap with threshold line 2. KL divergence with robust z-score on secondary axis 3. Predictive p-values with two-sided significance bounds</p> <p>Optionally shades flagged periods across all panels.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>(ndarray, shape(n_time))</code> <p>Time values for x-axis.</p> required <code>overlap</code> <code>(ndarray, shape(n_time))</code> <p>HPD overlap values.</p> required <code>kl</code> <code>(ndarray, shape(n_time))</code> <p>KL divergence values.</p> required <code>pvals</code> <code>(ndarray, shape(n_time))</code> <p>Predictive p-values.</p> required <code>flags</code> <code>(ndarray, shape(n_time))</code> <p>Boolean array indicating problematic time points to highlight with shading. True values mark periods where model-data agreement is poor (e.g., from combine_flags(), flag_extreme_kl(), or flag_low_overlap()). These regions will be shaded across all diagnostic panels to facilitate visual identification of problematic periods. Default is None (no shading).</p> <code>None</code> <code>tau</code> <code>float</code> <p>Threshold for HPD overlap visualization (dashed line). Default is 0.4.</p> <code>0.4</code> <code>z_thresh</code> <code>float</code> <p>Z-score threshold for KL visualization (dashed line). Default is 3.0.</p> <code>3.0</code> <code>alpha</code> <code>float</code> <p>Significance level for p-value bounds (dashed lines). Default is 0.05.</p> <code>0.05</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Figure object containing the diagnostic plots.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If flags array has different shape than metrics.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from statespacecheck.viz import plot_diagnostics\n&gt;&gt;&gt; time = np.arange(100)\n&gt;&gt;&gt; overlap = np.random.uniform(0.3, 0.9, 100)\n&gt;&gt;&gt; kl = np.random.uniform(0.1, 2.0, 100)\n&gt;&gt;&gt; pvals = np.random.uniform(0.1, 0.9, 100)\n&gt;&gt;&gt; fig = plot_diagnostics(time, overlap, kl, pvals)\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>src/statespacecheck/viz.py</code> <pre><code>def plot_diagnostics(\n    time: NDArray[np.floating],\n    overlap: NDArray[np.floating],\n    kl: NDArray[np.floating],\n    pvals: NDArray[np.floating],\n    flags: NDArray[np.bool_] | None = None,\n    tau: float = 0.4,\n    z_thresh: float = 3.0,\n    alpha: float = 0.05,\n) -&gt; Figure:\n    \"\"\"Plot HPD overlap, KL (with robust z-band), and predictive p-values.\n\n    Creates a three-panel diagnostic plot showing:\n    1. HPD overlap with threshold line\n    2. KL divergence with robust z-score on secondary axis\n    3. Predictive p-values with two-sided significance bounds\n\n    Optionally shades flagged periods across all panels.\n\n    Parameters\n    ----------\n    time : np.ndarray, shape (n_time,)\n        Time values for x-axis.\n    overlap : np.ndarray, shape (n_time,)\n        HPD overlap values.\n    kl : np.ndarray, shape (n_time,)\n        KL divergence values.\n    pvals : np.ndarray, shape (n_time,)\n        Predictive p-values.\n    flags : np.ndarray, shape (n_time,), optional\n        Boolean array indicating problematic time points to highlight with shading.\n        True values mark periods where model-data agreement is poor (e.g., from\n        combine_flags(), flag_extreme_kl(), or flag_low_overlap()). These regions\n        will be shaded across all diagnostic panels to facilitate visual identification\n        of problematic periods. Default is None (no shading).\n    tau : float, optional\n        Threshold for HPD overlap visualization (dashed line). Default is 0.4.\n    z_thresh : float, optional\n        Z-score threshold for KL visualization (dashed line). Default is 3.0.\n    alpha : float, optional\n        Significance level for p-value bounds (dashed lines). Default is 0.05.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Figure object containing the diagnostic plots.\n\n    Raises\n    ------\n    ValueError\n        If flags array has different shape than metrics.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from statespacecheck.viz import plot_diagnostics\n    &gt;&gt;&gt; time = np.arange(100)\n    &gt;&gt;&gt; overlap = np.random.uniform(0.3, 0.9, 100)\n    &gt;&gt;&gt; kl = np.random.uniform(0.1, 2.0, 100)\n    &gt;&gt;&gt; pvals = np.random.uniform(0.1, 0.9, 100)\n    &gt;&gt;&gt; fig = plot_diagnostics(time, overlap, kl, pvals)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    time_arr = np.asarray(time)\n    overlap_arr = np.asarray(overlap, dtype=float)\n    kl_arr = np.asarray(kl, dtype=float)\n    pvals_arr = np.asarray(pvals, dtype=float)\n\n    if flags is not None:\n        flags_arr = np.asarray(flags, dtype=bool)\n        if flags_arr.shape != overlap_arr.shape:\n            raise ValueError(\"flags must have same shape as metrics\")\n    else:\n        flags_arr = None\n\n    fig, axes = plt.subplots(3, 1, sharex=True, figsize=(10, 7))\n\n    # 1) Overlap\n    ax = axes[0]\n    ax.plot(time_arr, overlap_arr, linewidth=1)\n    ax.axhline(tau, linestyle=\"--\", linewidth=1)\n    ax.set_ylabel(\"HPD overlap\")\n    ax.set_ylim(-0.05, 1.05)\n    ax.grid(True, alpha=0.3)\n\n    # 2) KL (show raw KL and robust z as twin y if desired)\n    ax = axes[1]\n    ax.plot(time_arr, kl_arr, linewidth=1)\n    ax.set_ylabel(\"KL divergence\")\n    ax.grid(True, alpha=0.3)\n\n    # Optionally draw a z-threshold band using right y-axis (robust z)\n    z = _robust_zscore(kl_arr)\n    ax2 = ax.twinx()\n    ax2.plot(time_arr, z, linewidth=0.8, alpha=0.6)\n    ax2.axhline(z_thresh, linestyle=\"--\", linewidth=1)\n    ax2.set_ylabel(\"robust z(KL)\")\n\n    # 3) Predictive p-values\n    ax = axes[2]\n    ax.plot(time_arr, pvals_arr, linewidth=1)\n    ax.axhline(alpha / 2.0, linestyle=\"--\", linewidth=1)\n    ax.axhline(1.0 - alpha / 2.0, linestyle=\"--\", linewidth=1)\n    ax.set_ylabel(\"Predictive p\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylim(-0.05, 1.05)\n    ax.grid(True, alpha=0.3)\n\n    # Shade flagged intervals across all panels\n    if flags_arr is not None and np.any(flags_arr):\n        for a, b in _contiguous_runs(flags_arr):\n            # Handle edge case where b might be at the end\n            time_end = time_arr[b - 1] if b - 1 &lt; len(time_arr) else time_arr[-1]\n            for axi in axes:\n                axi.axvspan(time_arr[a], time_end, alpha=0.15)\n\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Welcome to the <code>statespacecheck</code> tutorials! These interactive Jupyter notebooks guide you through the core concepts and practical applications of goodness-of-fit diagnostics for state space models.</p>"},{"location":"tutorials/#tutorial-overview","title":"Tutorial Overview","text":""},{"location":"tutorials/#1-introduction","title":"1. Introduction","text":"<p>Get started with the fundamentals of model checking for state space models. This tutorial covers:</p> <ul> <li>What are state space models and why do we need goodness-of-fit diagnostics?</li> <li>Understanding posterior-likelihood consistency</li> <li>Basic usage of KL divergence and HPD overlap metrics</li> <li>Interpreting diagnostic results</li> </ul> <p>Duration: 20-25 minutes Prerequisites: Basic understanding of probability distributions and state space models</p>"},{"location":"tutorials/#2-highest-density-regions","title":"2. Highest Density Regions","text":"<p>Deep dive into highest posterior density (HPD) regions and their role in model diagnostics. Topics include:</p> <ul> <li>Computing HPD regions for univariate and multivariate distributions</li> <li>Handling multimodal distributions</li> <li>Visualizing HPD regions in 1D and 2D</li> <li>Understanding coverage probabilities</li> </ul> <p>Duration: 25-30 minutes Prerequisites: - Tutorial 1 (understanding of posterior-likelihood consistency) - Familiarity with probability density functions</p>"},{"location":"tutorials/#3-time-resolved-diagnostics","title":"3. Time-Resolved Diagnostics","text":"<p>Learn how to identify when and where your model fails using time-resolved metrics. This tutorial demonstrates:</p> <ul> <li>Computing diagnostics across time series</li> <li>Detecting periods of model-data mismatch</li> <li>Visualizing temporal patterns in model fit</li> <li>Identifying systematic biases vs. random errors</li> </ul> <p>Duration: 25-30 minutes Prerequisites: - Tutorial 1 (KL divergence and HPD overlap basics) - Tutorial 2 (HPD region computation) - Understanding of time-series data</p>"},{"location":"tutorials/#4-predictive-checks","title":"4. Predictive Checks","text":"<p>Advanced techniques for comprehensive model evaluation. Covers:</p> <ul> <li>Posterior predictive checks for state space models</li> <li>Comparing observed vs. predicted data patterns</li> <li>Detecting model misspecification</li> <li>Iterative model refinement workflows</li> </ul> <p>Duration: 30-35 minutes Prerequisites: - Tutorials 1-3 (core diagnostic methods) - Experience with Bayesian inference - Familiarity with posterior predictive distributions</p>"},{"location":"tutorials/#running-the-tutorials","title":"Running the Tutorials","text":"<p>All tutorials are provided as Jupyter notebooks with pre-computed outputs for quick reference. To run them interactively:</p>"},{"location":"tutorials/#which-option-should-i-choose","title":"Which option should I choose?","text":"<ul> <li>Local Installation: Best for exploring code, modifying examples, and integrating with your own data. Requires Python setup but gives full control.</li> <li>Google Colab: Quick start in the cloud with no installation needed. Requires a Google account. Good for trying out examples.</li> <li>Binder: Fully reproducible environment in the browser. No account needed, but slower to launch (~2-3 minutes). Best for workshops or teaching.</li> </ul>"},{"location":"tutorials/#option-1-local-installation","title":"Option 1: Local Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/edeno/statespacecheck.git\ncd statespacecheck\n\n# Install with notebook dependencies\nuv pip install -e \".[dev,docs]\"\n\n# Launch Jupyter\njupyter lab examples/\n</code></pre>"},{"location":"tutorials/#option-2-google-colab","title":"Option 2: Google Colab","text":"<p>Open any tutorial notebook on GitHub and click the \"Open in Colab\" badge at the top (if available), or manually upload the notebook to Google Colab.</p>"},{"location":"tutorials/#option-3-binder","title":"Option 3: Binder","text":"<p>Launch an interactive environment with all dependencies pre-installed:</p> <p></p>"},{"location":"tutorials/#learning-path","title":"Learning Path","text":"<p>We recommend following the tutorials in order, as each builds on concepts from the previous ones. However, if you're already familiar with state space models and want to jump to specific topics:</p> <ul> <li>Quick start: Tutorial 1</li> <li>Understanding HPD metrics: Tutorial 2</li> <li>Time series analysis: Tutorial 3</li> <li>Advanced diagnostics: Tutorial 4</li> </ul>"},{"location":"tutorials/#feedback-and-questions","title":"Feedback and Questions","text":"<p>Found an issue or have a question? Please open an issue on GitHub.</p> <p>Happy learning!</p>"},{"location":"tutorials/01_introduction/","title":"Introduction to Goodness-of-Fit for State Space Models","text":"In\u00a0[1]: Copied! <pre>import sys\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Import statespacecheck - if this fails, see prerequisites above\ntry:\n    from statespacecheck import hpd_overlap, kl_divergence\n\n    print(\"\u2713 statespacecheck imported successfully\")\nexcept ImportError as e:\n    raise ImportError(\n        \"statespacecheck not found. Please install it first:\\n\"\n        \"  cd /path/to/statespacecheck\\n\"\n        \"  pip install -e .\\n\"\n        \"Then restart the kernel.\"\n    ) from e\n\nfrom utils import configure_notebook_plotting, generate_1d_gaussian_distribution\n\n# Configure plotting\nconfigure_notebook_plotting()\n\n# Verify environment\nprint(f\"\u2713 Python {sys.version.split()[0]}\")\nprint(f\"\u2713 NumPy {np.__version__}\")\nprint(f\"\u2713 Matplotlib {plt.matplotlib.__version__}\")\nprint(\"\u2713 Environment configured correctly!\")\n</pre> import sys  import matplotlib.pyplot as plt import numpy as np  # Import statespacecheck - if this fails, see prerequisites above try:     from statespacecheck import hpd_overlap, kl_divergence      print(\"\u2713 statespacecheck imported successfully\") except ImportError as e:     raise ImportError(         \"statespacecheck not found. Please install it first:\\n\"         \"  cd /path/to/statespacecheck\\n\"         \"  pip install -e .\\n\"         \"Then restart the kernel.\"     ) from e  from utils import configure_notebook_plotting, generate_1d_gaussian_distribution  # Configure plotting configure_notebook_plotting()  # Verify environment print(f\"\u2713 Python {sys.version.split()[0]}\") print(f\"\u2713 NumPy {np.__version__}\") print(f\"\u2713 Matplotlib {plt.matplotlib.__version__}\") print(\"\u2713 Environment configured correctly!\") <pre>\u2713 statespacecheck imported successfully\n\u2713 Python 3.13.5\n\u2713 NumPy 2.3.4\n\u2713 Matplotlib 3.10.7\n\u2713 Environment configured correctly!\n</pre> In\u00a0[2]: Copied! <pre># Create position bins (e.g., 100 cm track, 50 bins)\nposition_bins = np.linspace(0, 100, 50)\n\n# Generate distributions centered at the same location with similar uncertainty\ntrue_position = 50.0\nstate_dist = generate_1d_gaussian_distribution(position_bins, mean=true_position, std=2.0)\nlikelihood = generate_1d_gaussian_distribution(position_bins, mean=true_position, std=2.5)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(position_bins, state_dist, label=\"State distribution\", linewidth=2.5, color=\"#1f77b4\")\nax.plot(\n    position_bins, likelihood, label=\"Likelihood\", linewidth=2.5, color=\"#ff7f0e\", linestyle=\"--\"\n)\nax.fill_between(position_bins, state_dist, alpha=0.3, color=\"#1f77b4\")\nax.fill_between(position_bins, likelihood, alpha=0.3, color=\"#ff7f0e\")\nax.axvline(\n    true_position, color=\"black\", linestyle=\":\", linewidth=2, label=\"True position\", alpha=0.5\n)\nax.set_xlabel(\"Position (cm)\")\nax.set_ylabel(\"Probability Density\")\nax.set_title(\"Perfect Model Fit: State and Likelihood Agree\")\nax.legend(frameon=True, loc=\"upper right\")\nplt.tight_layout()\nplt.show()\n</pre> # Create position bins (e.g., 100 cm track, 50 bins) position_bins = np.linspace(0, 100, 50)  # Generate distributions centered at the same location with similar uncertainty true_position = 50.0 state_dist = generate_1d_gaussian_distribution(position_bins, mean=true_position, std=2.0) likelihood = generate_1d_gaussian_distribution(position_bins, mean=true_position, std=2.5)  # Visualize fig, ax = plt.subplots(figsize=(10, 5)) ax.plot(position_bins, state_dist, label=\"State distribution\", linewidth=2.5, color=\"#1f77b4\") ax.plot(     position_bins, likelihood, label=\"Likelihood\", linewidth=2.5, color=\"#ff7f0e\", linestyle=\"--\" ) ax.fill_between(position_bins, state_dist, alpha=0.3, color=\"#1f77b4\") ax.fill_between(position_bins, likelihood, alpha=0.3, color=\"#ff7f0e\") ax.axvline(     true_position, color=\"black\", linestyle=\":\", linewidth=2, label=\"True position\", alpha=0.5 ) ax.set_xlabel(\"Position (cm)\") ax.set_ylabel(\"Probability Density\") ax.set_title(\"Perfect Model Fit: State and Likelihood Agree\") ax.legend(frameon=True, loc=\"upper right\") plt.tight_layout() plt.show() In\u00a0[3]: Copied! <pre># Generate distributions centered at different locations\nstate_mean = 30.0\nlikelihood_mean = 70.0  # Large spatial offset!\n\nstate_dist_poor = generate_1d_gaussian_distribution(position_bins, mean=state_mean, std=2.0)\nlikelihood_poor = generate_1d_gaussian_distribution(position_bins, mean=likelihood_mean, std=3.0)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(position_bins, state_dist_poor, label=\"State distribution\", linewidth=2.5, color=\"#1f77b4\")\nax.plot(\n    position_bins,\n    likelihood_poor,\n    label=\"Likelihood\",\n    linewidth=2.5,\n    color=\"#ff7f0e\",\n    linestyle=\"--\",\n)\nax.fill_between(position_bins, state_dist_poor, alpha=0.3, color=\"#1f77b4\")\nax.fill_between(position_bins, likelihood_poor, alpha=0.3, color=\"#ff7f0e\")\nax.axvline(state_mean, color=\"#1f77b4\", linestyle=\":\", alpha=0.5, linewidth=2)\nax.axvline(likelihood_mean, color=\"#ff7f0e\", linestyle=\":\", alpha=0.5, linewidth=2)\nax.set_xlabel(\"Position (cm)\")\nax.set_ylabel(\"Probability Density\")\nax.set_title(\"Poor Model Fit: State and Likelihood Disagree\")\nax.legend(frameon=True, loc=\"upper right\")\nplt.tight_layout()\nplt.show()\n</pre> # Generate distributions centered at different locations state_mean = 30.0 likelihood_mean = 70.0  # Large spatial offset!  state_dist_poor = generate_1d_gaussian_distribution(position_bins, mean=state_mean, std=2.0) likelihood_poor = generate_1d_gaussian_distribution(position_bins, mean=likelihood_mean, std=3.0)  # Visualize fig, ax = plt.subplots(figsize=(10, 5)) ax.plot(position_bins, state_dist_poor, label=\"State distribution\", linewidth=2.5, color=\"#1f77b4\") ax.plot(     position_bins,     likelihood_poor,     label=\"Likelihood\",     linewidth=2.5,     color=\"#ff7f0e\",     linestyle=\"--\", ) ax.fill_between(position_bins, state_dist_poor, alpha=0.3, color=\"#1f77b4\") ax.fill_between(position_bins, likelihood_poor, alpha=0.3, color=\"#ff7f0e\") ax.axvline(state_mean, color=\"#1f77b4\", linestyle=\":\", alpha=0.5, linewidth=2) ax.axvline(likelihood_mean, color=\"#ff7f0e\", linestyle=\":\", alpha=0.5, linewidth=2) ax.set_xlabel(\"Position (cm)\") ax.set_ylabel(\"Probability Density\") ax.set_title(\"Poor Model Fit: State and Likelihood Disagree\") ax.legend(frameon=True, loc=\"upper right\") plt.tight_layout() plt.show() In\u00a0[4]: Copied! <pre># For single time point, need to add time dimension\nstate_dist_2d = state_dist[np.newaxis, :]  # Shape: (1, n_bins) - time dimension added\nlikelihood_2d = likelihood[np.newaxis, :]\n\nstate_dist_poor_2d = state_dist_poor[np.newaxis, :]\nlikelihood_poor_2d = likelihood_poor[np.newaxis, :]\n\n# Compute KL divergence\nkl_good = kl_divergence(state_dist_2d, likelihood_2d)[0]\nkl_poor = kl_divergence(state_dist_poor_2d, likelihood_poor_2d)[0]\n\nprint(\"KL Divergence Results:\")\nprint(f\"  Perfect fit:  {kl_good:.4f} (low - distributions agree)\")\nprint(f\"  Poor fit:     {kl_poor:.4f} (high - distributions disagree)\")\n</pre> # For single time point, need to add time dimension state_dist_2d = state_dist[np.newaxis, :]  # Shape: (1, n_bins) - time dimension added likelihood_2d = likelihood[np.newaxis, :]  state_dist_poor_2d = state_dist_poor[np.newaxis, :] likelihood_poor_2d = likelihood_poor[np.newaxis, :]  # Compute KL divergence kl_good = kl_divergence(state_dist_2d, likelihood_2d)[0] kl_poor = kl_divergence(state_dist_poor_2d, likelihood_poor_2d)[0]  print(\"KL Divergence Results:\") print(f\"  Perfect fit:  {kl_good:.4f} (low - distributions agree)\") print(f\"  Poor fit:     {kl_poor:.4f} (high - distributions disagree)\") <pre>KL Divergence Results:\n  Perfect fit:  0.0431 (low - distributions agree)\n  Poor fit:     89.0166 (high - distributions disagree)\n</pre> <p>The KL divergence quantifies what we saw visually:</p> <ul> <li>Perfect fit has very low divergence</li> <li>Poor fit has much higher divergence (over 100x larger!)</li> </ul> <p>This metric gives us an objective measure of model-data agreement.</p> In\u00a0[5]: Copied! <pre># Compute HPD overlap\noverlap_good = hpd_overlap(state_dist_2d, likelihood_2d, coverage=0.95)[0]\noverlap_poor = hpd_overlap(state_dist_poor_2d, likelihood_poor_2d, coverage=0.95)[0]\n\nprint(\"HPD Overlap Results (95% coverage):\")\nprint(f\"  Perfect fit:  {overlap_good:.4f} (high - good spatial agreement)\")\nprint(f\"  Poor fit:     {overlap_poor:.4f} (low - poor spatial agreement)\")\n</pre> # Compute HPD overlap overlap_good = hpd_overlap(state_dist_2d, likelihood_2d, coverage=0.95)[0] overlap_poor = hpd_overlap(state_dist_poor_2d, likelihood_poor_2d, coverage=0.95)[0]  print(\"HPD Overlap Results (95% coverage):\") print(f\"  Perfect fit:  {overlap_good:.4f} (high - good spatial agreement)\") print(f\"  Poor fit:     {overlap_poor:.4f} (low - poor spatial agreement)\") <pre>HPD Overlap Results (95% coverage):\n  Perfect fit:  1.0000 (high - good spatial agreement)\n  Poor fit:     0.0000 (low - poor spatial agreement)\n</pre> <p>Again, the metric confirms our visual intuition:</p> <ul> <li>Perfect fit shows high overlap (distributions concentrate in similar regions)</li> <li>Poor fit shows low overlap (distributions are in different regions)</li> </ul> <p>HPD overlap provides a complementary view to KL divergence, focusing on spatial consistency.</p> In\u00a0[6]: Copied! <pre># Generate a range of offsets\noffsets = np.linspace(0, 50, 20)  # 0 to 50 cm offset\nkl_values = []\noverlap_values = []\n\nfor offset in offsets:\n    # Generate distributions with increasing offset\n    state_test = generate_1d_gaussian_distribution(position_bins, mean=40.0, std=2.0)\n    likelihood_test = generate_1d_gaussian_distribution(position_bins, mean=40.0 + offset, std=2.5)\n\n    # Add time dimension\n    state_test_2d = state_test[np.newaxis, :]\n    likelihood_test_2d = likelihood_test[np.newaxis, :]\n\n    # Compute metrics\n    kl_values.append(kl_divergence(state_test_2d, likelihood_test_2d)[0])\n    overlap_values.append(hpd_overlap(state_test_2d, likelihood_test_2d, coverage=0.95)[0])\n\n# Visualize\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# KL divergence\nax1.plot(offsets, kl_values, linewidth=2.5, marker=\"o\", markersize=6, color=\"#1f77b4\")\nax1.axhline(0.1, color=\"green\", linestyle=\"--\", alpha=0.5, label=\"Low threshold\")\nax1.axhline(1.0, color=\"orange\", linestyle=\"--\", alpha=0.5, label=\"High threshold\")\nax1.fill_between(offsets, 0, 0.1, alpha=0.2, color=\"green\", label=\"Good fit\")\nax1.fill_between(offsets, 1.0, ax1.get_ylim()[1], alpha=0.2, color=\"red\", label=\"Poor fit\")\nax1.set_xlabel(\"Spatial Offset (cm)\")\nax1.set_ylabel(\"KL Divergence\")\nax1.set_title(\"KL Divergence vs Spatial Offset\")\nax1.legend(frameon=True, loc=\"upper left\")\n\n# HPD overlap\nax2.plot(offsets, overlap_values, linewidth=2.5, marker=\"o\", markersize=6, color=\"#ff7f0e\")\nax2.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, label=\"High threshold\")\nax2.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, label=\"Low threshold\")\nax2.fill_between(offsets, 0.7, 1.0, alpha=0.2, color=\"green\", label=\"Good fit\")\nax2.fill_between(offsets, 0, 0.3, alpha=0.2, color=\"red\", label=\"Poor fit\")\nax2.set_xlabel(\"Spatial Offset (cm)\")\nax2.set_ylabel(\"HPD Overlap\")\nax2.set_title(\"HPD Overlap vs Spatial Offset\")\nax2.legend(frameon=True, loc=\"upper right\")\n\nplt.tight_layout()\nplt.show()\n</pre> # Generate a range of offsets offsets = np.linspace(0, 50, 20)  # 0 to 50 cm offset kl_values = [] overlap_values = []  for offset in offsets:     # Generate distributions with increasing offset     state_test = generate_1d_gaussian_distribution(position_bins, mean=40.0, std=2.0)     likelihood_test = generate_1d_gaussian_distribution(position_bins, mean=40.0 + offset, std=2.5)      # Add time dimension     state_test_2d = state_test[np.newaxis, :]     likelihood_test_2d = likelihood_test[np.newaxis, :]      # Compute metrics     kl_values.append(kl_divergence(state_test_2d, likelihood_test_2d)[0])     overlap_values.append(hpd_overlap(state_test_2d, likelihood_test_2d, coverage=0.95)[0])  # Visualize fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # KL divergence ax1.plot(offsets, kl_values, linewidth=2.5, marker=\"o\", markersize=6, color=\"#1f77b4\") ax1.axhline(0.1, color=\"green\", linestyle=\"--\", alpha=0.5, label=\"Low threshold\") ax1.axhline(1.0, color=\"orange\", linestyle=\"--\", alpha=0.5, label=\"High threshold\") ax1.fill_between(offsets, 0, 0.1, alpha=0.2, color=\"green\", label=\"Good fit\") ax1.fill_between(offsets, 1.0, ax1.get_ylim()[1], alpha=0.2, color=\"red\", label=\"Poor fit\") ax1.set_xlabel(\"Spatial Offset (cm)\") ax1.set_ylabel(\"KL Divergence\") ax1.set_title(\"KL Divergence vs Spatial Offset\") ax1.legend(frameon=True, loc=\"upper left\")  # HPD overlap ax2.plot(offsets, overlap_values, linewidth=2.5, marker=\"o\", markersize=6, color=\"#ff7f0e\") ax2.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, label=\"High threshold\") ax2.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, label=\"Low threshold\") ax2.fill_between(offsets, 0.7, 1.0, alpha=0.2, color=\"green\", label=\"Good fit\") ax2.fill_between(offsets, 0, 0.3, alpha=0.2, color=\"red\", label=\"Poor fit\") ax2.set_xlabel(\"Spatial Offset (cm)\") ax2.set_ylabel(\"HPD Overlap\") ax2.set_title(\"HPD Overlap vs Spatial Offset\") ax2.legend(frameon=True, loc=\"upper right\")  plt.tight_layout() plt.show() In\u00a0[7]: Copied! <pre># Generate distributions with varying uncertainty\nuncertainties = np.linspace(1.0, 10.0, 20)\nkl_values_unc = []\noverlap_values_unc = []\n\n# Fixed offset\nfixed_offset = 10.0\n\nfor unc in uncertainties:\n    # State distribution with fixed std, likelihood with varying std\n    state_test = generate_1d_gaussian_distribution(position_bins, mean=40.0, std=2.0)\n    likelihood_test = generate_1d_gaussian_distribution(\n        position_bins, mean=40.0 + fixed_offset, std=unc\n    )\n\n    state_test_2d = state_test[np.newaxis, :]\n    likelihood_test_2d = likelihood_test[np.newaxis, :]\n\n    kl_values_unc.append(kl_divergence(state_test_2d, likelihood_test_2d)[0])\n    overlap_values_unc.append(hpd_overlap(state_test_2d, likelihood_test_2d, coverage=0.95)[0])\n\n# Visualize\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.plot(uncertainties, kl_values_unc, linewidth=2.5, marker=\"o\", markersize=6, color=\"#1f77b4\")\nax1.set_xlabel(\"Likelihood Uncertainty (std, cm)\")\nax1.set_ylabel(\"KL Divergence\")\nax1.set_title(\"Effect of Uncertainty on KL Divergence\")\n\nax2.plot(\n    uncertainties, overlap_values_unc, linewidth=2.5, marker=\"o\", markersize=6, color=\"#ff7f0e\"\n)\nax2.set_xlabel(\"Likelihood Uncertainty (std, cm)\")\nax2.set_ylabel(\"HPD Overlap\")\nax2.set_title(\"Effect of Uncertainty on HPD Overlap\")\n\nplt.tight_layout()\nplt.show()\n</pre> # Generate distributions with varying uncertainty uncertainties = np.linspace(1.0, 10.0, 20) kl_values_unc = [] overlap_values_unc = []  # Fixed offset fixed_offset = 10.0  for unc in uncertainties:     # State distribution with fixed std, likelihood with varying std     state_test = generate_1d_gaussian_distribution(position_bins, mean=40.0, std=2.0)     likelihood_test = generate_1d_gaussian_distribution(         position_bins, mean=40.0 + fixed_offset, std=unc     )      state_test_2d = state_test[np.newaxis, :]     likelihood_test_2d = likelihood_test[np.newaxis, :]      kl_values_unc.append(kl_divergence(state_test_2d, likelihood_test_2d)[0])     overlap_values_unc.append(hpd_overlap(state_test_2d, likelihood_test_2d, coverage=0.95)[0])  # Visualize fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  ax1.plot(uncertainties, kl_values_unc, linewidth=2.5, marker=\"o\", markersize=6, color=\"#1f77b4\") ax1.set_xlabel(\"Likelihood Uncertainty (std, cm)\") ax1.set_ylabel(\"KL Divergence\") ax1.set_title(\"Effect of Uncertainty on KL Divergence\")  ax2.plot(     uncertainties, overlap_values_unc, linewidth=2.5, marker=\"o\", markersize=6, color=\"#ff7f0e\" ) ax2.set_xlabel(\"Likelihood Uncertainty (std, cm)\") ax2.set_ylabel(\"HPD Overlap\") ax2.set_title(\"Effect of Uncertainty on HPD Overlap\")  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/01_introduction/#introduction-to-goodness-of-fit-for-state-space-models","title":"Introduction to Goodness-of-Fit for State Space Models\u00b6","text":"<p>This notebook introduces the fundamental concepts of goodness-of-fit diagnostics for state space models in neuroscience. We'll build intuition through visual examples before diving into numerical metrics.</p> <p>Learning objectives:</p> <ul> <li>Understand what goodness-of-fit means for latent variable models</li> <li>Visualize the relationship between posterior and likelihood distributions</li> <li>Learn to use <code>kl_divergence()</code> and <code>hpd_overlap()</code> functions</li> <li>Interpret diagnostic metrics</li> </ul> <p>\u23f1 Estimated time: 20-25 minutes</p> <p>Previous: README.md | Next: 02_highest_density_regions.ipynb</p>"},{"location":"tutorials/01_introduction/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before starting this notebook, you should have:</p> <p>Background knowledge:</p> <ul> <li>Basic probability theory (probability distributions, expectation)</li> <li>Familiarity with Bayesian inference concepts (prior, likelihood, posterior)</li> <li>Basic Python and numpy</li> </ul> <p>Software requirements:</p> <ul> <li>Python 3.10+</li> <li>statespacecheck package installed: <code>pip install -e /path/to/statespacecheck</code></li> <li>Dependencies: numpy, scipy, matplotlib</li> </ul> <p>Note: If you're new to Bayesian state space models, see the README for background reading.</p>"},{"location":"tutorials/01_introduction/#setup","title":"Setup\u00b6","text":"<p>First, let's import the packages we'll need and verify the environment is configured correctly.</p>"},{"location":"tutorials/01_introduction/#the-problem-evaluating-latent-variable-models","title":"The Problem: Evaluating Latent Variable Models\u00b6","text":"<p>State space models relate neural activity to latent brain states we cannot directly observe. This creates a challenge: how do we know if our model is good if we can't see ground truth?</p> <p>Traditional approaches like cross-validation measure prediction accuracy but don't tell us:</p> <ul> <li>When does the model fail? (which time periods?)</li> <li>Why does it fail? (state model vs observation model?)</li> <li>Where in state space does it fail? (which regions?)</li> </ul> <p>The solution: Compare distributions that should agree if the model is well-specified.</p>"},{"location":"tutorials/01_introduction/#key-concept-state-distribution-vs-likelihood","title":"Key Concept: State Distribution vs Likelihood\u00b6","text":"<p>In Bayesian state space models, the posterior distribution combines two sources of information:</p> <ol> <li><p>State distribution (prior/predictive): What the model expects based on dynamics</p> <ul> <li>One-step-ahead prediction: $p(x_t | y_{1:t-1})$</li> <li>Smoothed distribution: $p(x_t | y_{1:T})$</li> </ul> </li> <li><p>Likelihood (normalized): What the current data says</p> <ul> <li>Normalized likelihood: $p(y_t | x_t) / \\sum_x p(y_t | x_t)$</li> <li>Equivalent to posterior with uniform prior: $p(x_t | y_t)$</li> </ul> </li> </ol> <p>Key insight: If the model is well-specified, these distributions should be consistent with each other. Large disagreements indicate model problems.</p>"},{"location":"tutorials/01_introduction/#example-1-perfect-model-fit","title":"Example 1: Perfect Model Fit\u00b6","text":"<p>Let's start with an ideal case where the model perfectly captures the data. The state distribution and likelihood should agree.</p>"},{"location":"tutorials/01_introduction/#interpretation","title":"Interpretation\u00b6","text":"<p>Notice how the two distributions:</p> <ul> <li>Peak at the same location</li> <li>Have similar spread</li> <li>Overlap substantially</li> </ul> <p>This indicates the model's predictions (state distribution) align well with what the data says (likelihood). This is what we expect from a well-specified model.</p>"},{"location":"tutorials/01_introduction/#example-2-poor-model-fit","title":"Example 2: Poor Model Fit\u00b6","text":"<p>Now let's see what happens when the model fails to capture the data. The distributions should disagree.</p>"},{"location":"tutorials/01_introduction/#interpretation","title":"Interpretation\u00b6","text":"<p>The distributions:</p> <ul> <li>Peak at very different locations (40 cm apart!)</li> <li>Have minimal overlap</li> <li>Indicate fundamental disagreement between model and data</li> </ul> <p>What does this tell us?</p> <ul> <li>The model's predictions don't match observations</li> <li>Could indicate: wrong dynamics model, wrong observation model, or insufficient model capacity</li> <li>We need to investigate and revise the model</li> </ul>"},{"location":"tutorials/01_introduction/#data-format-understanding-array-shapes","title":"Data Format: Understanding Array Shapes\u00b6","text":"<p>Important: All <code>statespacecheck</code> functions expect distributions with time as the first dimension:</p> <ul> <li>1D spatial: shape <code>(n_time, n_bins)</code></li> <li>2D spatial: shape <code>(n_time, n_x_bins, n_y_bins)</code></li> </ul> <p>Why time-first?</p> <ul> <li>Enables vectorized operations over time</li> <li>Consistent with time-series analysis conventions</li> <li>Allows functions to return time-resolved diagnostics</li> </ul> <p>For single time points (like our examples so far), we add a time dimension using <code>[np.newaxis, :]</code>.</p>"},{"location":"tutorials/01_introduction/#quantifying-agreement-kl-divergence","title":"Quantifying Agreement: KL Divergence\u00b6","text":"<p>Visual inspection is helpful, but we need quantitative metrics. Kullback-Leibler (KL) divergence measures information divergence between distributions.</p> <p>$$D_{KL}(P || Q) = \\sum_x P(x) \\log\\frac{P(x)}{Q(x)}$$</p> <p>Interpretation thresholds:</p> <ul> <li>Low divergence (&lt; 0.1): Distributions agree well</li> <li>Moderate divergence (0.1 - 1.0): Some disagreement, worth investigating</li> <li>High divergence (&gt; 1.0): Substantial mismatch, model problems likely</li> </ul> <p>About these thresholds: These are empirical guidelines based on experience with neuroscience data. In practice:</p> <ul> <li>KL &lt; 0.1 indicates distributions differ by less than 10% in information content</li> <li>KL &gt; 1.0 represents substantial information loss if using one distribution instead of the other</li> <li>Your data may differ: Validate these thresholds on known-good and known-bad models in your domain</li> </ul> <p>Let's compute KL divergence for our examples:</p>"},{"location":"tutorials/01_introduction/#quantifying-agreement-hpd-overlap","title":"Quantifying Agreement: HPD Overlap\u00b6","text":"<p>Another way to assess agreement is highest posterior density (HPD) region overlap. This measures spatial overlap between high-probability regions.</p> <p>Key idea:</p> <ul> <li>Identify the 95% HPD region for each distribution</li> <li>Compute how much they overlap</li> <li>Normalize by the smaller region size</li> </ul> <p>Interpretation:</p> <ul> <li>High overlap (&gt; 0.7): Distributions concentrate mass in similar regions</li> <li>Moderate overlap (0.3 - 0.7): Partial agreement</li> <li>Low overlap (&lt; 0.3): Distributions are spatially inconsistent</li> </ul>"},{"location":"tutorials/01_introduction/#progressive-disagreement","title":"Progressive Disagreement\u00b6","text":"<p>Let's explore the full spectrum from perfect agreement to complete disagreement by varying the spatial offset between distributions.</p>"},{"location":"tutorials/01_introduction/#key-observations","title":"Key Observations\u00b6","text":"<p>KL Divergence (left panel):</p> <ul> <li>Increases rapidly with spatial offset</li> <li>Stays low (&lt; 0.1) when offset is small</li> <li>Becomes very large when distributions are far apart</li> </ul> <p>HPD Overlap (right panel):</p> <ul> <li>Decreases as distributions separate</li> <li>Stays high (&gt; 0.7) when distributions are close</li> <li>Drops to zero when distributions don't overlap at all</li> </ul> <p>Notice: The two metrics provide complementary information. KL divergence is sensitive to the entire distribution shape, while HPD overlap focuses on where the high-probability mass is located.</p>"},{"location":"tutorials/01_introduction/#effect-of-uncertainty","title":"Effect of Uncertainty\u00b6","text":"<p>Let's examine how the width/uncertainty of distributions affects our metrics.</p>"},{"location":"tutorials/01_introduction/#key-observations","title":"Key Observations\u00b6","text":"<p>As likelihood uncertainty increases:</p> <ul> <li>KL divergence decreases: The distributions become more similar as the likelihood spreads out</li> <li>HPD overlap increases then plateaus: More spread means more spatial overlap, but eventually coverage limits are reached</li> </ul> <p>Important insight: Model uncertainty affects diagnostic metrics. A very uncertain model might appear to \"fit well\" simply because it doesn't make strong predictions. This is why we need multiple complementary diagnostics!</p>"},{"location":"tutorials/01_introduction/#summary-what-we-learned","title":"Summary: What We Learned\u00b6","text":"<p>Core concepts:</p> <ol> <li>State space models have latent variables we can't directly observe</li> <li>We assess fit by comparing state distribution (model prediction) vs likelihood (data-driven)</li> <li>Agreement indicates good fit; disagreement indicates problems</li> </ol> <p>Diagnostic metrics:</p> <ol> <li><p>KL Divergence: Measures information divergence</p> <ul> <li>Low (&lt; 0.1): Good agreement</li> <li>High (&gt; 1.0): Poor agreement</li> </ul> </li> <li><p>HPD Overlap: Measures spatial overlap of high-probability regions</p> <ul> <li>High (&gt; 0.7): Good spatial agreement</li> <li>Low (&lt; 0.3): Poor spatial agreement</li> </ul> </li> </ol> <p>Key insights:</p> <ul> <li>Visual inspection builds intuition</li> <li>Quantitative metrics provide objective assessment</li> <li>Multiple metrics give complementary views</li> <li>Uncertainty affects interpretation</li> </ul> <p>Next steps:</p> <ul> <li>Next notebook: 02_highest_density_regions.ipynb - Deep dive into HPD regions</li> <li>Jump ahead: 03_time_resolved_diagnostics.ipynb - Apply to real time series data</li> </ul>"},{"location":"tutorials/01_introduction/#exercises-optional","title":"Exercises (Optional)\u00b6","text":"<p>Try modifying the code above to explore these questions:</p> <ol> <li><p>\u2b50 Easy: What happens with very peaked distributions (std = 0.5)? How do KL and overlap change?</p> <ul> <li>Hint: Modify the <code>std</code> parameter in the distribution generation</li> </ul> </li> <li><p>\u2b50\u2b50 Moderate: How do metrics change if both distributions have high uncertainty (std &gt; 10)?</p> <ul> <li>Hint: Does high mutual uncertainty make disagreement harder to detect?</li> </ul> </li> <li><p>\u2b50\u2b50\u2b50 Advanced: Can you create a case where KL divergence is low but overlap is also low?</p> <ul> <li>Hint: Think about multimodal distributions (we'll explore these in notebook 02)</li> </ul> </li> </ol>"},{"location":"tutorials/02_highest_density_regions/","title":"Highest Density Regions (HDR)","text":"In\u00a0[1]: Copied! <pre>import sys\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nsys.path.insert(0, str(Path.cwd().parent))\n\nfrom utils import (\n    configure_notebook_plotting,\n    generate_1d_gaussian_distribution,\n    generate_multimodal_distribution,\n)\n\nfrom statespacecheck import highest_density_region, hpd_overlap\n\nconfigure_notebook_plotting()\n</pre> import sys from pathlib import Path  import matplotlib.pyplot as plt import numpy as np  sys.path.insert(0, str(Path.cwd().parent))  from utils import (     configure_notebook_plotting,     generate_1d_gaussian_distribution,     generate_multimodal_distribution, )  from statespacecheck import highest_density_region, hpd_overlap  configure_notebook_plotting() In\u00a0[2]: Copied! <pre># Create position bins\nposition_bins = np.linspace(0, 100, 100)\n\n# Generate unimodal distribution\ndist_unimodal = generate_1d_gaussian_distribution(position_bins, mean=50.0, std=5.0)\n\n# Compute 95% HDR\ndist_2d = dist_unimodal[np.newaxis, :]  # Add time dimension\nhdr_mask = highest_density_region(dist_2d, coverage=0.95)[0]  # Get first (only) time point\n\n# Visualize\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(position_bins, dist_unimodal, linewidth=3, color=\"#1f77b4\", label=\"Distribution\")\nax.fill_between(\n    position_bins, 0, dist_unimodal, where=hdr_mask, alpha=0.4, color=\"green\", label=\"95% HDR\"\n)\nax.fill_between(\n    position_bins, 0, dist_unimodal, where=~hdr_mask, alpha=0.2, color=\"gray\", label=\"Outside HDR\"\n)\nax.set_xlabel(\"Position (cm)\")\nax.set_ylabel(\"Probability Density\")\nax.set_title(\"Highest Density Region for Unimodal Distribution (95% coverage)\")\nax.legend(frameon=True, loc=\"upper right\")\nplt.tight_layout()\nplt.show()\n\n# Print statistics\nprint(f\"HDR coverage: {hdr_mask.sum() / len(hdr_mask) * 100:.1f}% of bins\")\nprint(f\"Probability mass in HDR: {dist_unimodal[hdr_mask].sum():.3f}\")\nprint(f\"Total probability mass: {dist_unimodal.sum():.3f}\")\n</pre> # Create position bins position_bins = np.linspace(0, 100, 100)  # Generate unimodal distribution dist_unimodal = generate_1d_gaussian_distribution(position_bins, mean=50.0, std=5.0)  # Compute 95% HDR dist_2d = dist_unimodal[np.newaxis, :]  # Add time dimension hdr_mask = highest_density_region(dist_2d, coverage=0.95)[0]  # Get first (only) time point  # Visualize fig, ax = plt.subplots(figsize=(12, 5)) ax.plot(position_bins, dist_unimodal, linewidth=3, color=\"#1f77b4\", label=\"Distribution\") ax.fill_between(     position_bins, 0, dist_unimodal, where=hdr_mask, alpha=0.4, color=\"green\", label=\"95% HDR\" ) ax.fill_between(     position_bins, 0, dist_unimodal, where=~hdr_mask, alpha=0.2, color=\"gray\", label=\"Outside HDR\" ) ax.set_xlabel(\"Position (cm)\") ax.set_ylabel(\"Probability Density\") ax.set_title(\"Highest Density Region for Unimodal Distribution (95% coverage)\") ax.legend(frameon=True, loc=\"upper right\") plt.tight_layout() plt.show()  # Print statistics print(f\"HDR coverage: {hdr_mask.sum() / len(hdr_mask) * 100:.1f}% of bins\") print(f\"Probability mass in HDR: {dist_unimodal[hdr_mask].sum():.3f}\") print(f\"Total probability mass: {dist_unimodal.sum():.3f}\") <pre>HDR coverage: 20.0% of bins\nProbability mass in HDR: 0.947\nTotal probability mass: 0.990\n</pre> In\u00a0[3]: Copied! <pre># Test different coverage values\ncoverages = [0.50, 0.75, 0.90, 0.95, 0.99]\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\naxes = axes.flatten()\n\nfor idx, cov in enumerate(coverages):\n    hdr = highest_density_region(dist_2d, coverage=cov)[0]\n\n    axes[idx].plot(position_bins, dist_unimodal, linewidth=2.5, color=\"#1f77b4\")\n    axes[idx].fill_between(\n        position_bins, 0, dist_unimodal, where=hdr, alpha=0.4, color=\"green\", label=f\"{cov:.0%} HDR\"\n    )\n    axes[idx].fill_between(position_bins, 0, dist_unimodal, where=~hdr, alpha=0.2, color=\"gray\")\n    axes[idx].set_xlabel(\"Position (cm)\")\n    axes[idx].set_ylabel(\"Probability Density\")\n    axes[idx].set_title(f\"Coverage = {cov:.0%}\")\n    axes[idx].legend(frameon=True, loc=\"upper right\")\n\n# Remove empty subplot\naxes[-1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</pre> # Test different coverage values coverages = [0.50, 0.75, 0.90, 0.95, 0.99]  fig, axes = plt.subplots(2, 3, figsize=(15, 8)) axes = axes.flatten()  for idx, cov in enumerate(coverages):     hdr = highest_density_region(dist_2d, coverage=cov)[0]      axes[idx].plot(position_bins, dist_unimodal, linewidth=2.5, color=\"#1f77b4\")     axes[idx].fill_between(         position_bins, 0, dist_unimodal, where=hdr, alpha=0.4, color=\"green\", label=f\"{cov:.0%} HDR\"     )     axes[idx].fill_between(position_bins, 0, dist_unimodal, where=~hdr, alpha=0.2, color=\"gray\")     axes[idx].set_xlabel(\"Position (cm)\")     axes[idx].set_ylabel(\"Probability Density\")     axes[idx].set_title(f\"Coverage = {cov:.0%}\")     axes[idx].legend(frameon=True, loc=\"upper right\")  # Remove empty subplot axes[-1].axis(\"off\")  plt.tight_layout() plt.show() In\u00a0[4]: Copied! <pre># Generate bimodal distribution (mixture of two Gaussians)\ndist_bimodal = generate_multimodal_distribution(\n    position_bins, means=[30.0, 70.0], stds=[4.0, 4.0], weights=[0.5, 0.5]\n)\n\n# Compute 95% HDR\ndist_bimodal_2d = dist_bimodal[np.newaxis, :]\nhdr_bimodal = highest_density_region(dist_bimodal_2d, coverage=0.95)[0]\n\n# Visualize\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(position_bins, dist_bimodal, linewidth=3, color=\"#1f77b4\", label=\"Distribution\")\nax.fill_between(\n    position_bins, 0, dist_bimodal, where=hdr_bimodal, alpha=0.4, color=\"green\", label=\"95% HDR\"\n)\nax.fill_between(\n    position_bins,\n    0,\n    dist_bimodal,\n    where=~hdr_bimodal,\n    alpha=0.2,\n    color=\"gray\",\n    label=\"Outside HDR\",\n)\nax.set_xlabel(\"Position (cm)\")\nax.set_ylabel(\"Probability Density\")\nax.set_title(\"Highest Density Region for Bimodal Distribution (95% coverage)\")\nax.legend(frameon=True, loc=\"upper right\")\nplt.tight_layout()\nplt.show()\n\nprint(\"Number of HDR regions: 2 (non-contiguous)\")\nprint(f\"Probability mass in HDR: {dist_bimodal[hdr_bimodal].sum():.3f}\")\n</pre> # Generate bimodal distribution (mixture of two Gaussians) dist_bimodal = generate_multimodal_distribution(     position_bins, means=[30.0, 70.0], stds=[4.0, 4.0], weights=[0.5, 0.5] )  # Compute 95% HDR dist_bimodal_2d = dist_bimodal[np.newaxis, :] hdr_bimodal = highest_density_region(dist_bimodal_2d, coverage=0.95)[0]  # Visualize fig, ax = plt.subplots(figsize=(12, 5)) ax.plot(position_bins, dist_bimodal, linewidth=3, color=\"#1f77b4\", label=\"Distribution\") ax.fill_between(     position_bins, 0, dist_bimodal, where=hdr_bimodal, alpha=0.4, color=\"green\", label=\"95% HDR\" ) ax.fill_between(     position_bins,     0,     dist_bimodal,     where=~hdr_bimodal,     alpha=0.2,     color=\"gray\",     label=\"Outside HDR\", ) ax.set_xlabel(\"Position (cm)\") ax.set_ylabel(\"Probability Density\") ax.set_title(\"Highest Density Region for Bimodal Distribution (95% coverage)\") ax.legend(frameon=True, loc=\"upper right\") plt.tight_layout() plt.show()  print(\"Number of HDR regions: 2 (non-contiguous)\") print(f\"Probability mass in HDR: {dist_bimodal[hdr_bimodal].sum():.3f}\") <pre>Number of HDR regions: 2 (non-contiguous)\nProbability mass in HDR: 0.947\n</pre> In\u00a0[5]: Copied! <pre># Create three different bimodal distributions\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Case 1: Equal weights, well-separated\ndist1 = generate_multimodal_distribution(\n    position_bins, means=[30, 70], stds=[4, 4], weights=[0.5, 0.5]\n)\ndist1_2d = dist1[np.newaxis, :]\nhdr1 = highest_density_region(dist1_2d, coverage=0.90)[0]\n\naxes[0].plot(position_bins, dist1, linewidth=2.5, color=\"#1f77b4\")\naxes[0].fill_between(position_bins, 0, dist1, where=hdr1, alpha=0.4, color=\"green\")\naxes[0].fill_between(position_bins, 0, dist1, where=~hdr1, alpha=0.2, color=\"gray\")\naxes[0].set_title(\"Equal Weights\\nWell-Separated\")\naxes[0].set_xlabel(\"Position (cm)\")\naxes[0].set_ylabel(\"Density\")\n\n# Case 2: Unequal weights (70/30 split)\ndist2 = generate_multimodal_distribution(\n    position_bins, means=[30, 70], stds=[4, 4], weights=[0.7, 0.3]\n)\ndist2_2d = dist2[np.newaxis, :]\nhdr2 = highest_density_region(dist2_2d, coverage=0.90)[0]\n\naxes[1].plot(position_bins, dist2, linewidth=2.5, color=\"#1f77b4\")\naxes[1].fill_between(position_bins, 0, dist2, where=hdr2, alpha=0.4, color=\"green\")\naxes[1].fill_between(position_bins, 0, dist2, where=~hdr2, alpha=0.2, color=\"gray\")\naxes[1].set_title(\"Unequal Weights (70/30)\\nWell-Separated\")\naxes[1].set_xlabel(\"Position (cm)\")\naxes[1].set_ylabel(\"Density\")\n\n# Case 3: Close peaks (potentially overlapping HDR)\ndist3 = generate_multimodal_distribution(\n    position_bins, means=[45, 55], stds=[4, 4], weights=[0.5, 0.5]\n)\ndist3_2d = dist3[np.newaxis, :]\nhdr3 = highest_density_region(dist3_2d, coverage=0.90)[0]\n\naxes[2].plot(position_bins, dist3, linewidth=2.5, color=\"#1f77b4\")\naxes[2].fill_between(position_bins, 0, dist3, where=hdr3, alpha=0.4, color=\"green\")\naxes[2].fill_between(position_bins, 0, dist3, where=~hdr3, alpha=0.2, color=\"gray\")\naxes[2].set_title(\"Equal Weights\\nClose Peaks\")\naxes[2].set_xlabel(\"Position (cm)\")\naxes[2].set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n</pre> # Create three different bimodal distributions fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Case 1: Equal weights, well-separated dist1 = generate_multimodal_distribution(     position_bins, means=[30, 70], stds=[4, 4], weights=[0.5, 0.5] ) dist1_2d = dist1[np.newaxis, :] hdr1 = highest_density_region(dist1_2d, coverage=0.90)[0]  axes[0].plot(position_bins, dist1, linewidth=2.5, color=\"#1f77b4\") axes[0].fill_between(position_bins, 0, dist1, where=hdr1, alpha=0.4, color=\"green\") axes[0].fill_between(position_bins, 0, dist1, where=~hdr1, alpha=0.2, color=\"gray\") axes[0].set_title(\"Equal Weights\\nWell-Separated\") axes[0].set_xlabel(\"Position (cm)\") axes[0].set_ylabel(\"Density\")  # Case 2: Unequal weights (70/30 split) dist2 = generate_multimodal_distribution(     position_bins, means=[30, 70], stds=[4, 4], weights=[0.7, 0.3] ) dist2_2d = dist2[np.newaxis, :] hdr2 = highest_density_region(dist2_2d, coverage=0.90)[0]  axes[1].plot(position_bins, dist2, linewidth=2.5, color=\"#1f77b4\") axes[1].fill_between(position_bins, 0, dist2, where=hdr2, alpha=0.4, color=\"green\") axes[1].fill_between(position_bins, 0, dist2, where=~hdr2, alpha=0.2, color=\"gray\") axes[1].set_title(\"Unequal Weights (70/30)\\nWell-Separated\") axes[1].set_xlabel(\"Position (cm)\") axes[1].set_ylabel(\"Density\")  # Case 3: Close peaks (potentially overlapping HDR) dist3 = generate_multimodal_distribution(     position_bins, means=[45, 55], stds=[4, 4], weights=[0.5, 0.5] ) dist3_2d = dist3[np.newaxis, :] hdr3 = highest_density_region(dist3_2d, coverage=0.90)[0]  axes[2].plot(position_bins, dist3, linewidth=2.5, color=\"#1f77b4\") axes[2].fill_between(position_bins, 0, dist3, where=hdr3, alpha=0.4, color=\"green\") axes[2].fill_between(position_bins, 0, dist3, where=~hdr3, alpha=0.2, color=\"gray\") axes[2].set_title(\"Equal Weights\\nClose Peaks\") axes[2].set_xlabel(\"Position (cm)\") axes[2].set_ylabel(\"Density\")  plt.tight_layout() plt.show() In\u00a0[6]: Copied! <pre># Create two partially overlapping distributions\ndist_a = generate_1d_gaussian_distribution(position_bins, mean=40.0, std=5.0)\ndist_b = generate_1d_gaussian_distribution(position_bins, mean=55.0, std=5.0)\n\n# Add time dimension\ndist_a_2d = dist_a[np.newaxis, :]\ndist_b_2d = dist_b[np.newaxis, :]\n\n# Compute HDRs\nhdr_a = highest_density_region(dist_a_2d, coverage=0.95)[0]\nhdr_b = highest_density_region(dist_b_2d, coverage=0.95)[0]\n\n# Compute overlap\noverlap = hpd_overlap(dist_a_2d, dist_b_2d, coverage=0.95)[0]\n\n# Visualize\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n# Top: Both distributions\nax1.plot(position_bins, dist_a, linewidth=2.5, color=\"#1f77b4\", label=\"Distribution A\")\nax1.plot(position_bins, dist_b, linewidth=2.5, color=\"#ff7f0e\", label=\"Distribution B\")\nax1.fill_between(position_bins, dist_a, alpha=0.3, color=\"#1f77b4\")\nax1.fill_between(position_bins, dist_b, alpha=0.3, color=\"#ff7f0e\")\nax1.set_xlabel(\"Position (cm)\")\nax1.set_ylabel(\"Probability Density\")\nax1.set_title(\"Two Partially Overlapping Distributions\")\nax1.legend(frameon=True, loc=\"upper right\")\n\n# Bottom: HDR visualization\nax2.fill_between(\n    position_bins,\n    0,\n    1,\n    where=hdr_a &amp; hdr_b,\n    alpha=0.6,\n    color=\"green\",\n    label=f\"Both HDRs (overlap = {overlap:.2f})\",\n)\nax2.fill_between(\n    position_bins, 0, 1, where=hdr_a &amp; ~hdr_b, alpha=0.4, color=\"#1f77b4\", label=\"Only HDR A\"\n)\nax2.fill_between(\n    position_bins, 0, 1, where=~hdr_a &amp; hdr_b, alpha=0.4, color=\"#ff7f0e\", label=\"Only HDR B\"\n)\nax2.set_xlabel(\"Position (cm)\")\nax2.set_ylabel(\"HDR Membership\")\nax2.set_title(\"HDR Overlap Visualization\")\nax2.set_ylim([0, 1.2])\nax2.legend(frameon=True, loc=\"upper right\")\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"HDR overlap: {overlap:.3f}\")\nprint(f\"Interpretation: {overlap * 100:.1f}% of the smaller HDR overlaps with the larger HDR\")\n</pre> # Create two partially overlapping distributions dist_a = generate_1d_gaussian_distribution(position_bins, mean=40.0, std=5.0) dist_b = generate_1d_gaussian_distribution(position_bins, mean=55.0, std=5.0)  # Add time dimension dist_a_2d = dist_a[np.newaxis, :] dist_b_2d = dist_b[np.newaxis, :]  # Compute HDRs hdr_a = highest_density_region(dist_a_2d, coverage=0.95)[0] hdr_b = highest_density_region(dist_b_2d, coverage=0.95)[0]  # Compute overlap overlap = hpd_overlap(dist_a_2d, dist_b_2d, coverage=0.95)[0]  # Visualize fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))  # Top: Both distributions ax1.plot(position_bins, dist_a, linewidth=2.5, color=\"#1f77b4\", label=\"Distribution A\") ax1.plot(position_bins, dist_b, linewidth=2.5, color=\"#ff7f0e\", label=\"Distribution B\") ax1.fill_between(position_bins, dist_a, alpha=0.3, color=\"#1f77b4\") ax1.fill_between(position_bins, dist_b, alpha=0.3, color=\"#ff7f0e\") ax1.set_xlabel(\"Position (cm)\") ax1.set_ylabel(\"Probability Density\") ax1.set_title(\"Two Partially Overlapping Distributions\") ax1.legend(frameon=True, loc=\"upper right\")  # Bottom: HDR visualization ax2.fill_between(     position_bins,     0,     1,     where=hdr_a &amp; hdr_b,     alpha=0.6,     color=\"green\",     label=f\"Both HDRs (overlap = {overlap:.2f})\", ) ax2.fill_between(     position_bins, 0, 1, where=hdr_a &amp; ~hdr_b, alpha=0.4, color=\"#1f77b4\", label=\"Only HDR A\" ) ax2.fill_between(     position_bins, 0, 1, where=~hdr_a &amp; hdr_b, alpha=0.4, color=\"#ff7f0e\", label=\"Only HDR B\" ) ax2.set_xlabel(\"Position (cm)\") ax2.set_ylabel(\"HDR Membership\") ax2.set_title(\"HDR Overlap Visualization\") ax2.set_ylim([0, 1.2]) ax2.legend(frameon=True, loc=\"upper right\")  plt.tight_layout() plt.show()  print(f\"HDR overlap: {overlap:.3f}\") print(f\"Interpretation: {overlap * 100:.1f}% of the smaller HDR overlaps with the larger HDR\") <pre>HDR overlap: 0.250\nInterpretation: 25.0% of the smaller HDR overlaps with the larger HDR\n</pre> In\u00a0[7]: Copied! <pre># Generate distributions with varying separation\nseparations = np.linspace(0, 40, 15)\noverlaps = []\n\nfor sep in separations:\n    dist1 = generate_1d_gaussian_distribution(position_bins, mean=40.0, std=5.0)\n    dist2 = generate_1d_gaussian_distribution(position_bins, mean=40.0 + sep, std=5.0)\n\n    dist1_2d = dist1[np.newaxis, :]\n    dist2_2d = dist2[np.newaxis, :]\n\n    overlap_val = hpd_overlap(dist1_2d, dist2_2d, coverage=0.95)[0]\n    overlaps.append(overlap_val)\n\n# Visualize\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Overlap vs separation\nax1.plot(separations, overlaps, linewidth=3, marker=\"o\", markersize=8, color=\"#1f77b4\")\nax1.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\")\nax1.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\")\nax1.fill_between(separations, 0.7, 1.0, alpha=0.2, color=\"green\", label=\"High agreement\")\nax1.fill_between(separations, 0, 0.3, alpha=0.2, color=\"red\", label=\"Low agreement\")\nax1.set_xlabel(\"Separation Distance (cm)\")\nax1.set_ylabel(\"HDR Overlap\")\nax1.set_title(\"HDR Overlap vs Distribution Separation\")\nax1.legend(frameon=True, loc=\"upper right\")\nax1.grid(True, alpha=0.3)\n\n# Right: Example distributions at different separations\nexample_seps = [0, 10, 20, 30]\nfor sep in example_seps:\n    dist1 = generate_1d_gaussian_distribution(position_bins, mean=40.0, std=5.0)\n    dist2 = generate_1d_gaussian_distribution(position_bins, mean=40.0 + sep, std=5.0)\n    dist1_2d = dist1[np.newaxis, :]\n    dist2_2d = dist2[np.newaxis, :]\n    overlap_val = hpd_overlap(dist1_2d, dist2_2d, coverage=0.95)[0]\n\n    ax2.plot(\n        position_bins,\n        dist1,\n        alpha=0.6,\n        linewidth=2,\n        label=f\"Sep={sep}cm, overlap={overlap_val:.2f}\",\n    )\n\nax2.set_xlabel(\"Position (cm)\")\nax2.set_ylabel(\"Probability Density\")\nax2.set_title(\"Example Distributions at Different Separations\")\nax2.legend(frameon=True, loc=\"upper right\")\n\nplt.tight_layout()\nplt.show()\n</pre> # Generate distributions with varying separation separations = np.linspace(0, 40, 15) overlaps = []  for sep in separations:     dist1 = generate_1d_gaussian_distribution(position_bins, mean=40.0, std=5.0)     dist2 = generate_1d_gaussian_distribution(position_bins, mean=40.0 + sep, std=5.0)      dist1_2d = dist1[np.newaxis, :]     dist2_2d = dist2[np.newaxis, :]      overlap_val = hpd_overlap(dist1_2d, dist2_2d, coverage=0.95)[0]     overlaps.append(overlap_val)  # Visualize fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))  # Left: Overlap vs separation ax1.plot(separations, overlaps, linewidth=3, marker=\"o\", markersize=8, color=\"#1f77b4\") ax1.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\") ax1.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\") ax1.fill_between(separations, 0.7, 1.0, alpha=0.2, color=\"green\", label=\"High agreement\") ax1.fill_between(separations, 0, 0.3, alpha=0.2, color=\"red\", label=\"Low agreement\") ax1.set_xlabel(\"Separation Distance (cm)\") ax1.set_ylabel(\"HDR Overlap\") ax1.set_title(\"HDR Overlap vs Distribution Separation\") ax1.legend(frameon=True, loc=\"upper right\") ax1.grid(True, alpha=0.3)  # Right: Example distributions at different separations example_seps = [0, 10, 20, 30] for sep in example_seps:     dist1 = generate_1d_gaussian_distribution(position_bins, mean=40.0, std=5.0)     dist2 = generate_1d_gaussian_distribution(position_bins, mean=40.0 + sep, std=5.0)     dist1_2d = dist1[np.newaxis, :]     dist2_2d = dist2[np.newaxis, :]     overlap_val = hpd_overlap(dist1_2d, dist2_2d, coverage=0.95)[0]      ax2.plot(         position_bins,         dist1,         alpha=0.6,         linewidth=2,         label=f\"Sep={sep}cm, overlap={overlap_val:.2f}\",     )  ax2.set_xlabel(\"Position (cm)\") ax2.set_ylabel(\"Probability Density\") ax2.set_title(\"Example Distributions at Different Separations\") ax2.legend(frameon=True, loc=\"upper right\")  plt.tight_layout() plt.show() In\u00a0[8]: Copied! <pre># Extremely narrow distribution\ndist_peaked = generate_1d_gaussian_distribution(position_bins, mean=50.0, std=0.5)\ndist_peaked_2d = dist_peaked[np.newaxis, :]\nhdr_peaked = highest_density_region(dist_peaked_2d, coverage=0.95)[0]\n\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(position_bins, dist_peaked, linewidth=2.5, color=\"#1f77b4\")\nax.fill_between(position_bins, 0, dist_peaked, where=hdr_peaked, alpha=0.4, color=\"green\")\nax.fill_between(position_bins, 0, dist_peaked, where=~hdr_peaked, alpha=0.2, color=\"gray\")\nax.set_xlabel(\"Position (cm)\")\nax.set_ylabel(\"Probability Density\")\nax.set_title(\"Very Peaked Distribution (std = 0.5 cm)\")\nplt.tight_layout()\nplt.show()\n\nprint(f\"HDR spans {hdr_peaked.sum()} bins (out of {len(position_bins)} total)\")\nprint(f\"HDR width: ~{hdr_peaked.sum() * (position_bins[1] - position_bins[0]):.1f} cm\")\n</pre> # Extremely narrow distribution dist_peaked = generate_1d_gaussian_distribution(position_bins, mean=50.0, std=0.5) dist_peaked_2d = dist_peaked[np.newaxis, :] hdr_peaked = highest_density_region(dist_peaked_2d, coverage=0.95)[0]  fig, ax = plt.subplots(figsize=(12, 5)) ax.plot(position_bins, dist_peaked, linewidth=2.5, color=\"#1f77b4\") ax.fill_between(position_bins, 0, dist_peaked, where=hdr_peaked, alpha=0.4, color=\"green\") ax.fill_between(position_bins, 0, dist_peaked, where=~hdr_peaked, alpha=0.2, color=\"gray\") ax.set_xlabel(\"Position (cm)\") ax.set_ylabel(\"Probability Density\") ax.set_title(\"Very Peaked Distribution (std = 0.5 cm)\") plt.tight_layout() plt.show()  print(f\"HDR spans {hdr_peaked.sum()} bins (out of {len(position_bins)} total)\") print(f\"HDR width: ~{hdr_peaked.sum() * (position_bins[1] - position_bins[0]):.1f} cm\") <pre>HDR spans 2 bins (out of 100 total)\nHDR width: ~2.0 cm\n</pre> <p>Result: HDR correctly identifies a very tight region. The algorithm handles highly peaked distributions well.</p> In\u00a0[9]: Copied! <pre># Nearly uniform distribution\ndist_flat = np.ones_like(position_bins) + np.random.normal(0, 0.01, len(position_bins))\ndist_flat = np.maximum(dist_flat, 0)  # Ensure non-negative\ndist_flat = dist_flat / dist_flat.sum()  # Normalize\n\ndist_flat_2d = dist_flat[np.newaxis, :]\nhdr_flat = highest_density_region(dist_flat_2d, coverage=0.95)[0]\n\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(position_bins, dist_flat, linewidth=2.5, color=\"#1f77b4\")\nax.fill_between(position_bins, 0, dist_flat, where=hdr_flat, alpha=0.4, color=\"green\")\nax.fill_between(position_bins, 0, dist_flat, where=~hdr_flat, alpha=0.2, color=\"gray\")\nax.set_xlabel(\"Position (cm)\")\nax.set_ylabel(\"Probability Density\")\nax.set_title(\"Nearly Uniform Distribution\")\nplt.tight_layout()\nplt.show()\n\nprint(f\"HDR spans {hdr_flat.sum()} bins (out of {len(position_bins)} total)\")\nprint(f\"HDR coverage: {hdr_flat.sum() / len(position_bins) * 100:.1f}% of bins\")\n</pre> # Nearly uniform distribution dist_flat = np.ones_like(position_bins) + np.random.normal(0, 0.01, len(position_bins)) dist_flat = np.maximum(dist_flat, 0)  # Ensure non-negative dist_flat = dist_flat / dist_flat.sum()  # Normalize  dist_flat_2d = dist_flat[np.newaxis, :] hdr_flat = highest_density_region(dist_flat_2d, coverage=0.95)[0]  fig, ax = plt.subplots(figsize=(12, 5)) ax.plot(position_bins, dist_flat, linewidth=2.5, color=\"#1f77b4\") ax.fill_between(position_bins, 0, dist_flat, where=hdr_flat, alpha=0.4, color=\"green\") ax.fill_between(position_bins, 0, dist_flat, where=~hdr_flat, alpha=0.2, color=\"gray\") ax.set_xlabel(\"Position (cm)\") ax.set_ylabel(\"Probability Density\") ax.set_title(\"Nearly Uniform Distribution\") plt.tight_layout() plt.show()  print(f\"HDR spans {hdr_flat.sum()} bins (out of {len(position_bins)} total)\") print(f\"HDR coverage: {hdr_flat.sum() / len(position_bins) * 100:.1f}% of bins\") <pre>HDR spans 95 bins (out of 100 total)\nHDR coverage: 95.0% of bins\n</pre> <p>Result: For nearly uniform distributions, HDR includes most of the space (as expected - there's no concentrated region). The algorithm handles flat distributions appropriately.</p> In\u00a0[10]: Copied! <pre># Create distribution with NaN values (simulating walls/barriers)\ndist_with_nan = generate_1d_gaussian_distribution(position_bins, mean=50.0, std=5.0)\n\n# Mark some regions as invalid (e.g., walls)\ndist_with_nan[20:25] = np.nan  # Barrier 1\ndist_with_nan[75:80] = np.nan  # Barrier 2\n\ndist_with_nan_2d = dist_with_nan[np.newaxis, :]\nhdr_with_nan = highest_density_region(dist_with_nan_2d, coverage=0.95)[0]\n\nfig, ax = plt.subplots(figsize=(12, 5))\n# Plot valid regions\nvalid_mask = ~np.isnan(dist_with_nan)\nax.plot(\n    position_bins[valid_mask],\n    dist_with_nan[valid_mask],\n    linewidth=2.5,\n    color=\"#1f77b4\",\n    label=\"Valid\",\n)\n\n# Mark NaN regions\nnan_mask = np.isnan(dist_with_nan)\nfor i in range(len(position_bins) - 1):\n    if nan_mask[i]:\n        ax.axvspan(position_bins[i], position_bins[i + 1], alpha=0.3, color=\"red\")\n\n# Plot HDR\nax.fill_between(\n    position_bins,\n    0,\n    np.nan_to_num(dist_with_nan, nan=0),\n    where=hdr_with_nan &amp; valid_mask,\n    alpha=0.4,\n    color=\"green\",\n    label=\"HDR\",\n)\n\nax.set_xlabel(\"Position (cm)\")\nax.set_ylabel(\"Probability Density\")\nax.set_title(\"Distribution with Invalid Regions (NaN values)\")\nax.legend(frameon=True, loc=\"upper right\")\nplt.tight_layout()\nplt.show()\n\nprint(\"NaN values are correctly excluded from HDR computation\")\nprint(\"HDR computed only over valid bins\")\n</pre> # Create distribution with NaN values (simulating walls/barriers) dist_with_nan = generate_1d_gaussian_distribution(position_bins, mean=50.0, std=5.0)  # Mark some regions as invalid (e.g., walls) dist_with_nan[20:25] = np.nan  # Barrier 1 dist_with_nan[75:80] = np.nan  # Barrier 2  dist_with_nan_2d = dist_with_nan[np.newaxis, :] hdr_with_nan = highest_density_region(dist_with_nan_2d, coverage=0.95)[0]  fig, ax = plt.subplots(figsize=(12, 5)) # Plot valid regions valid_mask = ~np.isnan(dist_with_nan) ax.plot(     position_bins[valid_mask],     dist_with_nan[valid_mask],     linewidth=2.5,     color=\"#1f77b4\",     label=\"Valid\", )  # Mark NaN regions nan_mask = np.isnan(dist_with_nan) for i in range(len(position_bins) - 1):     if nan_mask[i]:         ax.axvspan(position_bins[i], position_bins[i + 1], alpha=0.3, color=\"red\")  # Plot HDR ax.fill_between(     position_bins,     0,     np.nan_to_num(dist_with_nan, nan=0),     where=hdr_with_nan &amp; valid_mask,     alpha=0.4,     color=\"green\",     label=\"HDR\", )  ax.set_xlabel(\"Position (cm)\") ax.set_ylabel(\"Probability Density\") ax.set_title(\"Distribution with Invalid Regions (NaN values)\") ax.legend(frameon=True, loc=\"upper right\") plt.tight_layout() plt.show()  print(\"NaN values are correctly excluded from HDR computation\") print(\"HDR computed only over valid bins\") <pre>NaN values are correctly excluded from HDR computation\nHDR computed only over valid bins\n</pre> <p>Result: The algorithm correctly handles NaN values by treating them as invalid bins (zero mass) and excluding them from both normalization and HDR computation.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/02_highest_density_regions/#highest-density-regions-hdr","title":"Highest Density Regions (HDR)\u00b6","text":"<p>This notebook provides a deep dive into highest posterior density (HPD) regions, also called highest density regions (HDR). Understanding HDR is crucial for interpreting the <code>hpd_overlap()</code> metric and for visualizing probability distributions.</p> <p>Learning objectives:</p> <ul> <li>Understand what highest density regions represent</li> <li>Learn how coverage parameters work</li> <li>Handle unimodal vs multimodal distributions</li> <li>Visualize HDR for different distribution types</li> <li>Understand edge cases and limitations</li> </ul> <p>Previous: 01_introduction.ipynb | Next: 03_time_resolved_diagnostics.ipynb</p>"},{"location":"tutorials/02_highest_density_regions/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/02_highest_density_regions/#what-is-a-highest-density-region","title":"What is a Highest Density Region?\u00b6","text":"<p>The highest density region (HDR) with coverage $\\alpha$ is the smallest region that contains at least $\\alpha$ probability mass, where all points inside have higher density than points outside.</p> <p>Formal definition: $$R_\\alpha = \\{x : p(x) \\geq p^*\\}$$</p> <p>where $p^*$ is chosen such that $\\int_{R_\\alpha} p(x)dx \\geq \\alpha$</p> <p>Key properties:</p> <ul> <li>Contains at least $\\alpha$ of the probability mass</li> <li>Smallest region with that coverage (most compact)</li> <li>Can be non-contiguous for multimodal distributions</li> <li>Points inside have higher density than points outside</li> </ul> <p>Intuition: \"Where is the probability mass concentrated?\"</p>"},{"location":"tutorials/02_highest_density_regions/#example-1-unimodal-distribution","title":"Example 1: Unimodal Distribution\u00b6","text":"<p>Let's start with a simple Gaussian distribution and visualize its 95% HDR.</p>"},{"location":"tutorials/02_highest_density_regions/#interpretation","title":"Interpretation\u00b6","text":"<p>The green region shows where the distribution concentrates its probability mass:</p> <ul> <li>The HDR is a contiguous interval for unimodal distributions</li> <li>It captures the central high-density region around the peak</li> <li>The tails (gray) are excluded because they have lower density</li> <li>The region is symmetric for symmetric distributions</li> </ul> <p>Note: We're working with discrete distributions (histograms), so the actual coverage may slightly exceed 95% due to bin granularity.</p>"},{"location":"tutorials/02_highest_density_regions/#effect-of-coverage-parameter","title":"Effect of Coverage Parameter\u00b6","text":"<p>The <code>coverage</code> parameter controls how much probability mass to include. Let's visualize multiple coverage levels.</p>"},{"location":"tutorials/02_highest_density_regions/#key-observations","title":"Key Observations\u00b6","text":"<p>As coverage increases:</p> <ul> <li>HDR expands to include more probability mass</li> <li>50% HDR: Very tight, only the highest peak</li> <li>95% HDR: Standard choice, balances informativeness and coverage</li> <li>99% HDR: Very wide, includes almost everything</li> </ul> <p>Choosing coverage:</p> <ul> <li>95% is conventional (like confidence intervals)</li> <li>Higher coverage is more conservative but less informative</li> <li>Lower coverage is more selective but may miss important regions</li> </ul>"},{"location":"tutorials/02_highest_density_regions/#example-2-multimodal-distribution","title":"Example 2: Multimodal Distribution\u00b6","text":"<p>HDR becomes more interesting (and useful!) with multimodal distributions. Let's create a bimodal distribution.</p>"},{"location":"tutorials/02_highest_density_regions/#key-insight","title":"Key Insight\u00b6","text":"<p>HDR for multimodal distributions:</p> <ul> <li>Can be non-contiguous (multiple separated regions)</li> <li>Includes all regions with sufficiently high density</li> <li>Excludes the low-density valley between peaks</li> <li>This is a key advantage over simple credible intervals (which must be contiguous)</li> </ul> <p>Why this matters for neuroscience:</p> <ul> <li>Neural representations can be multimodal (e.g., head direction cells with symmetric responses)</li> <li>State uncertainty during decision-making (two possible choices)</li> <li>Place cells with multiple fields</li> </ul>"},{"location":"tutorials/02_highest_density_regions/#comparing-different-multimodal-structures","title":"Comparing Different Multimodal Structures\u00b6","text":"<p>Let's examine how HDR handles different types of multimodal distributions.</p>"},{"location":"tutorials/02_highest_density_regions/#key-observations","title":"Key Observations\u00b6","text":"<p>Left (equal, separated): HDR cleanly separates into two regions</p> <p>Middle (unequal, separated): HDR focuses more on the dominant peak. The smaller peak may be partially or fully excluded if coverage is low enough.</p> <p>Right (close peaks): When peaks are close, the HDR becomes nearly contiguous. The valley between peaks has high enough density to be included.</p> <p>Practical implication: HDR adapts to the distribution structure automatically.</p>"},{"location":"tutorials/02_highest_density_regions/#hdr-overlap-between-distributions","title":"HDR Overlap Between Distributions\u00b6","text":"<p>Now let's understand how <code>hpd_overlap()</code> works by visualizing the overlap between two distributions' HDRs.</p>"},{"location":"tutorials/02_highest_density_regions/#understanding-the-overlap-metric","title":"Understanding the Overlap Metric\u00b6","text":"<p>The overlap is computed as: $$\\text{overlap} = \\frac{|R_A \\cap R_B|}{\\min(|R_A|, |R_B|)}$$</p> <p>Why normalize by minimum size?</p> <ul> <li>Makes overlap symmetric and interpretable</li> <li>Value of 1.0 means one HDR is fully contained in the other</li> <li>Value of 0.0 means no spatial overlap</li> <li>Intermediate values show partial agreement</li> </ul> <p>Interpretation guide:</p> <ul> <li>&gt; 0.7: High spatial agreement</li> <li>0.3 - 0.7: Moderate agreement</li> <li>&lt; 0.3: Low agreement (distributions in different regions)</li> </ul>"},{"location":"tutorials/02_highest_density_regions/#varying-overlap-a-spectrum","title":"Varying Overlap: A Spectrum\u00b6","text":"<p>Let's explore how overlap changes as we move distributions apart.</p>"},{"location":"tutorials/02_highest_density_regions/#key-insights","title":"Key Insights\u00b6","text":"<p>Overlap decay:</p> <ul> <li>Drops rapidly as distributions separate</li> <li>Reaches zero when distributions don't overlap at all</li> <li>Smooth transition between perfect and no overlap</li> </ul> <p>Practical thresholds:</p> <ul> <li>&gt; 0.7: Distributions concentrate mass in similar locations</li> <li>0.3-0.7: Partial overlap, some spatial disagreement</li> <li>&lt; 0.3: Distributions are in substantially different regions</li> </ul>"},{"location":"tutorials/02_highest_density_regions/#edge-cases-and-robustness","title":"Edge Cases and Robustness\u00b6","text":"<p>Let's test HDR computation with challenging edge cases.</p>"},{"location":"tutorials/02_highest_density_regions/#case-1-very-peaked-distribution","title":"Case 1: Very Peaked Distribution\u00b6","text":""},{"location":"tutorials/02_highest_density_regions/#case-2-very-flat-distribution","title":"Case 2: Very Flat Distribution\u00b6","text":""},{"location":"tutorials/02_highest_density_regions/#case-3-distribution-with-nan-values","title":"Case 3: Distribution with NaN Values\u00b6","text":"<p>In neuroscience, we often have invalid spatial bins (e.g., walls in a maze, inaccessible regions). We use NaN to mark these.</p>"},{"location":"tutorials/02_highest_density_regions/#summary-what-we-learned","title":"Summary: What We Learned\u00b6","text":"<p>Core concepts:</p> <ol> <li>HDR identifies where probability mass is concentrated</li> <li>Coverage parameter controls how much mass to include (95% is standard)</li> <li>HDR can be non-contiguous for multimodal distributions</li> <li>HPD overlap measures spatial agreement between distributions</li> </ol> <p>Key insights:</p> <ul> <li>Unimodal distributions: HDR is a contiguous interval</li> <li>Multimodal distributions: HDR can have multiple separated regions</li> <li>Overlap metric: Normalized by smaller region for interpretability</li> <li>Edge cases: Algorithm handles peaked, flat, and NaN-containing distributions</li> </ul> <p>Practical guidelines:</p> <ul> <li>Use 95% coverage for consistency with standard practice</li> <li>Interpret overlap &gt; 0.7 as good spatial agreement</li> <li>Interpret overlap &lt; 0.3 as poor spatial agreement</li> <li>Visualize HDR to understand distribution structure</li> </ul> <p>Next steps:</p> <ul> <li>Next notebook: 03_time_resolved_diagnostics.ipynb - Apply these concepts to time series</li> <li>Jump ahead: 04_predictive_checks.ipynb - Advanced diagnostics</li> </ul>"},{"location":"tutorials/02_highest_density_regions/#exercises-optional","title":"Exercises (Optional)\u00b6","text":"<ol> <li>Create a trimodal distribution and visualize its 90% HDR</li> <li>For two distributions with 50% overlap, what's the spatial separation?</li> <li>How does HDR overlap change if you use 50% vs 95% coverage?</li> <li>Create a case where KL divergence is low but HDR overlap is also low (is this possible?)</li> </ol>"},{"location":"tutorials/03_time_resolved_diagnostics/","title":"Time-Resolved Diagnostics for State Space Models","text":"In\u00a0[1]: Copied! <pre>import sys\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nsys.path.insert(0, str(Path.cwd().parent))\n\nfrom utils import (\n    configure_notebook_plotting,\n    generate_misspecified_model_data,\n    generate_spatial_navigation_data,\n)\n\nfrom statespacecheck import (\n    aggregate_over_period,\n    combine_flags,\n    find_low_overlap_intervals,\n    flag_extreme_kl,\n    flag_low_overlap,\n    hpd_overlap,\n    kl_divergence,\n)\n\nconfigure_notebook_plotting()\n</pre> import sys from pathlib import Path  import matplotlib.pyplot as plt import numpy as np  sys.path.insert(0, str(Path.cwd().parent))  from utils import (     configure_notebook_plotting,     generate_misspecified_model_data,     generate_spatial_navigation_data, )  from statespacecheck import (     aggregate_over_period,     combine_flags,     find_low_overlap_intervals,     flag_extreme_kl,     flag_low_overlap,     hpd_overlap,     kl_divergence, )  configure_notebook_plotting() In\u00a0[2]: Copied! <pre># Generate data with a period of model misfit\ndata = generate_misspecified_model_data(\n    n_time=200, track_length=100.0, n_bins=50, misfit_start=80, misfit_end=120, seed=42\n)\n\nposition_bins = data[\"position_bins\"]\ntrue_position = data[\"true_position\"]\nstate_dist = data[\"state_dist\"]\nlikelihood = data[\"likelihood\"]\ntime = data[\"time\"]\nmisfit_start, misfit_end = data[\"misfit_period\"]\n\n# Print data shapes\nprint(\"Data shapes:\")\nprint(f\"  position_bins: {position_bins.shape}\")\nprint(f\"  state_dist: {state_dist.shape}\")\nprint(f\"  likelihood: {likelihood.shape}\")\nprint(f\"  time: {time.shape}\")\nprint(f\"\\nMisfit period: time {misfit_start}-{misfit_end}\")\n</pre> # Generate data with a period of model misfit data = generate_misspecified_model_data(     n_time=200, track_length=100.0, n_bins=50, misfit_start=80, misfit_end=120, seed=42 )  position_bins = data[\"position_bins\"] true_position = data[\"true_position\"] state_dist = data[\"state_dist\"] likelihood = data[\"likelihood\"] time = data[\"time\"] misfit_start, misfit_end = data[\"misfit_period\"]  # Print data shapes print(\"Data shapes:\") print(f\"  position_bins: {position_bins.shape}\") print(f\"  state_dist: {state_dist.shape}\") print(f\"  likelihood: {likelihood.shape}\") print(f\"  time: {time.shape}\") print(f\"\\nMisfit period: time {misfit_start}-{misfit_end}\") <pre>Data shapes:\n  position_bins: (50,)\n  state_dist: (200, 50)\n  likelihood: (200, 50)\n  time: (200,)\n\nMisfit period: time 80-120\n</pre> In\u00a0[3]: Copied! <pre>fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 10))\n\n# State distribution heatmap\nim1 = ax1.imshow(\n    state_dist.T,\n    aspect=\"auto\",\n    origin=\"lower\",\n    extent=[time[0], time[-1], position_bins[0], position_bins[-1]],\n    cmap=\"viridis\",\n    interpolation=\"nearest\",\n)\nax1.plot(time, true_position, \"r--\", linewidth=2, label=\"True position\", alpha=0.8)\nax1.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\")\nax1.set_ylabel(\"Position (cm)\")\nax1.set_title(\"State Distribution (Model Prediction)\")\nax1.legend(frameon=True, loc=\"upper right\")\nplt.colorbar(im1, ax=ax1, label=\"Probability Density\")\n\n# Likelihood heatmap\nim2 = ax2.imshow(\n    likelihood.T,\n    aspect=\"auto\",\n    origin=\"lower\",\n    extent=[time[0], time[-1], position_bins[0], position_bins[-1]],\n    cmap=\"viridis\",\n    interpolation=\"nearest\",\n)\nax2.plot(time, true_position, \"r--\", linewidth=2, label=\"True position\", alpha=0.8)\nax2.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\")\nax2.set_ylabel(\"Position (cm)\")\nax2.set_title(\"Likelihood (Data-Driven)\")\nax2.legend(frameon=True, loc=\"upper right\")\nplt.colorbar(im2, ax=ax2, label=\"Probability Density\")\n\n# Difference (absolute)\ndiff = np.abs(state_dist - likelihood)\nim3 = ax3.imshow(\n    diff.T,\n    aspect=\"auto\",\n    origin=\"lower\",\n    extent=[time[0], time[-1], position_bins[0], position_bins[-1]],\n    cmap=\"Reds\",\n    interpolation=\"nearest\",\n)\nax3.plot(time, true_position, \"k--\", linewidth=2, label=\"True position\", alpha=0.8)\nax3.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\")\nax3.set_xlabel(\"Time\")\nax3.set_ylabel(\"Position (cm)\")\nax3.set_title(\"Absolute Difference\")\nax3.legend(frameon=True, loc=\"upper right\")\nplt.colorbar(im3, ax=ax3, label=\"|State - Likelihood|\")\n\nplt.tight_layout()\nplt.show()\n</pre> fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 10))  # State distribution heatmap im1 = ax1.imshow(     state_dist.T,     aspect=\"auto\",     origin=\"lower\",     extent=[time[0], time[-1], position_bins[0], position_bins[-1]],     cmap=\"viridis\",     interpolation=\"nearest\", ) ax1.plot(time, true_position, \"r--\", linewidth=2, label=\"True position\", alpha=0.8) ax1.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\") ax1.set_ylabel(\"Position (cm)\") ax1.set_title(\"State Distribution (Model Prediction)\") ax1.legend(frameon=True, loc=\"upper right\") plt.colorbar(im1, ax=ax1, label=\"Probability Density\")  # Likelihood heatmap im2 = ax2.imshow(     likelihood.T,     aspect=\"auto\",     origin=\"lower\",     extent=[time[0], time[-1], position_bins[0], position_bins[-1]],     cmap=\"viridis\",     interpolation=\"nearest\", ) ax2.plot(time, true_position, \"r--\", linewidth=2, label=\"True position\", alpha=0.8) ax2.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\") ax2.set_ylabel(\"Position (cm)\") ax2.set_title(\"Likelihood (Data-Driven)\") ax2.legend(frameon=True, loc=\"upper right\") plt.colorbar(im2, ax=ax2, label=\"Probability Density\")  # Difference (absolute) diff = np.abs(state_dist - likelihood) im3 = ax3.imshow(     diff.T,     aspect=\"auto\",     origin=\"lower\",     extent=[time[0], time[-1], position_bins[0], position_bins[-1]],     cmap=\"Reds\",     interpolation=\"nearest\", ) ax3.plot(time, true_position, \"k--\", linewidth=2, label=\"True position\", alpha=0.8) ax3.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\") ax3.set_xlabel(\"Time\") ax3.set_ylabel(\"Position (cm)\") ax3.set_title(\"Absolute Difference\") ax3.legend(frameon=True, loc=\"upper right\") plt.colorbar(im3, ax=ax3, label=\"|State - Likelihood|\")  plt.tight_layout() plt.show() In\u00a0[4]: Copied! <pre># Compute KL divergence and HPD overlap over time\nkl_div = kl_divergence(state_dist, likelihood)\noverlap = hpd_overlap(state_dist, likelihood, coverage=0.95)\n\n# Visualize\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n\n# KL divergence\nax1.plot(time, kl_div, linewidth=2.5, color=\"#1f77b4\")\nax1.axhline(0.1, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\")\nax1.axhline(1.0, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\")\nax1.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"True misfit period\")\nax1.fill_between(time, 0, 0.1, alpha=0.2, color=\"green\", label=\"Good fit\")\nax1.fill_between(time, 1.0, ax1.get_ylim()[1], alpha=0.2, color=\"red\", label=\"Poor fit\")\nax1.set_ylabel(\"KL Divergence\")\nax1.set_title(\"Time-Resolved KL Divergence\")\nax1.legend(frameon=True, loc=\"upper left\")\nax1.grid(True, alpha=0.3)\n\n# HPD overlap\nax2.plot(time, overlap, linewidth=2.5, color=\"#ff7f0e\")\nax2.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\")\nax2.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\")\nax2.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"True misfit period\")\nax2.fill_between(time, 0.7, 1.0, alpha=0.2, color=\"green\", label=\"Good fit\")\nax2.fill_between(time, 0, 0.3, alpha=0.2, color=\"red\", label=\"Poor fit\")\nax2.set_xlabel(\"Time\")\nax2.set_ylabel(\"HPD Overlap\")\nax2.set_title(\"Time-Resolved HPD Overlap\")\nax2.legend(frameon=True, loc=\"lower left\")\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</pre> # Compute KL divergence and HPD overlap over time kl_div = kl_divergence(state_dist, likelihood) overlap = hpd_overlap(state_dist, likelihood, coverage=0.95)  # Visualize fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)  # KL divergence ax1.plot(time, kl_div, linewidth=2.5, color=\"#1f77b4\") ax1.axhline(0.1, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\") ax1.axhline(1.0, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\") ax1.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"True misfit period\") ax1.fill_between(time, 0, 0.1, alpha=0.2, color=\"green\", label=\"Good fit\") ax1.fill_between(time, 1.0, ax1.get_ylim()[1], alpha=0.2, color=\"red\", label=\"Poor fit\") ax1.set_ylabel(\"KL Divergence\") ax1.set_title(\"Time-Resolved KL Divergence\") ax1.legend(frameon=True, loc=\"upper left\") ax1.grid(True, alpha=0.3)  # HPD overlap ax2.plot(time, overlap, linewidth=2.5, color=\"#ff7f0e\") ax2.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\") ax2.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\") ax2.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"True misfit period\") ax2.fill_between(time, 0.7, 1.0, alpha=0.2, color=\"green\", label=\"Good fit\") ax2.fill_between(time, 0, 0.3, alpha=0.2, color=\"red\", label=\"Poor fit\") ax2.set_xlabel(\"Time\") ax2.set_ylabel(\"HPD Overlap\") ax2.set_title(\"Time-Resolved HPD Overlap\") ax2.legend(frameon=True, loc=\"lower left\") ax2.grid(True, alpha=0.3)  plt.tight_layout() plt.show() In\u00a0[5]: Copied! <pre># Flag time points with extreme values\nkl_flags = flag_extreme_kl(kl_div, z_thresh=3.0, min_len=5)\n\n# Flag times with low overlap (threshold=0.3 means flag when overlap &lt; 30%)\noverlap_flags = flag_low_overlap(overlap, threshold=0.3, min_len=5)\n\n# Combine flags (flag if either metric indicates problem)\ncombined_flags = combine_flags(kl_flags, overlap_flags, min_votes=1, min_len=5)\n\n# Print statistics\nprint(\"Flagged time points:\")\nprint(\n    f\"  High KL divergence (&gt; 1.0): {kl_flags.sum()} / {len(kl_flags)} ({kl_flags.sum() / len(kl_flags) * 100:.1f}%)\"\n)\nprint(\n    f\"  Low overlap (&lt; 0.3): {overlap_flags.sum()} / {len(overlap_flags)} ({overlap_flags.sum() / len(overlap_flags) * 100:.1f}%)\"\n)\nprint(\n    f\"  Combined (either flag): {combined_flags.sum()} / {len(combined_flags)} ({combined_flags.sum() / len(combined_flags) * 100:.1f}%)\"\n)\nprint(\n    f\"\\nActual misfit period: {misfit_end - misfit_start} time points ({(misfit_end - misfit_start) / len(time) * 100:.1f}%)\"\n)\n</pre> # Flag time points with extreme values kl_flags = flag_extreme_kl(kl_div, z_thresh=3.0, min_len=5)  # Flag times with low overlap (threshold=0.3 means flag when overlap &lt; 30%) overlap_flags = flag_low_overlap(overlap, threshold=0.3, min_len=5)  # Combine flags (flag if either metric indicates problem) combined_flags = combine_flags(kl_flags, overlap_flags, min_votes=1, min_len=5)  # Print statistics print(\"Flagged time points:\") print(     f\"  High KL divergence (&gt; 1.0): {kl_flags.sum()} / {len(kl_flags)} ({kl_flags.sum() / len(kl_flags) * 100:.1f}%)\" ) print(     f\"  Low overlap (&lt; 0.3): {overlap_flags.sum()} / {len(overlap_flags)} ({overlap_flags.sum() / len(overlap_flags) * 100:.1f}%)\" ) print(     f\"  Combined (either flag): {combined_flags.sum()} / {len(combined_flags)} ({combined_flags.sum() / len(combined_flags) * 100:.1f}%)\" ) print(     f\"\\nActual misfit period: {misfit_end - misfit_start} time points ({(misfit_end - misfit_start) / len(time) * 100:.1f}%)\" ) <pre>Flagged time points:\n  High KL divergence (&gt; 1.0): 40 / 200 (20.0%)\n  Low overlap (&lt; 0.3): 40 / 200 (20.0%)\n  Combined (either flag): 40 / 200 (20.0%)\n\nActual misfit period: 40 time points (20.0%)\n</pre> <p>The flagging functions successfully identify approximately the right proportion of problematic time points!</p> In\u00a0[6]: Copied! <pre># Find intervals of low overlap (threshold=0.3 means flag periods where overlap &lt; 30%)\nintervals = find_low_overlap_intervals(overlap, threshold=0.3, min_len=5)\n\nprint(\"Detected problem intervals:\")\nfor i, (start, end) in enumerate(intervals, 1):\n    duration = end - start\n    print(f\"  Interval {i}: time {start}-{end} (duration = {duration})\")\n\n# Visualize intervals\nfig, ax = plt.subplots(figsize=(14, 5))\nax.plot(time, overlap, linewidth=2.5, color=\"#ff7f0e\", label=\"HPD Overlap\")\nax.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Detection threshold\")\n\n# Highlight detected intervals\nfor start, end in intervals:\n    ax.axvspan(\n        start,\n        end,\n        alpha=0.3,\n        color=\"red\",\n        label=\"Detected interval\" if start == intervals[0][0] else \"\",\n    )\n\n# Show true misfit period for comparison\nax.axvspan(\n    misfit_start,\n    misfit_end,\n    alpha=0.2,\n    color=\"blue\",\n    edgecolor=\"blue\",\n    linewidth=2,\n    linestyle=\"--\",\n    label=\"True misfit period\",\n)\n\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"HPD Overlap\")\nax.set_title(\"Automatic Detection of Problem Intervals\")\nax.legend(frameon=True, loc=\"lower left\")\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</pre> # Find intervals of low overlap (threshold=0.3 means flag periods where overlap &lt; 30%) intervals = find_low_overlap_intervals(overlap, threshold=0.3, min_len=5)  print(\"Detected problem intervals:\") for i, (start, end) in enumerate(intervals, 1):     duration = end - start     print(f\"  Interval {i}: time {start}-{end} (duration = {duration})\")  # Visualize intervals fig, ax = plt.subplots(figsize=(14, 5)) ax.plot(time, overlap, linewidth=2.5, color=\"#ff7f0e\", label=\"HPD Overlap\") ax.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Detection threshold\")  # Highlight detected intervals for start, end in intervals:     ax.axvspan(         start,         end,         alpha=0.3,         color=\"red\",         label=\"Detected interval\" if start == intervals[0][0] else \"\",     )  # Show true misfit period for comparison ax.axvspan(     misfit_start,     misfit_end,     alpha=0.2,     color=\"blue\",     edgecolor=\"blue\",     linewidth=2,     linestyle=\"--\",     label=\"True misfit period\", )  ax.set_xlabel(\"Time\") ax.set_ylabel(\"HPD Overlap\") ax.set_title(\"Automatic Detection of Problem Intervals\") ax.legend(frameon=True, loc=\"lower left\") ax.grid(True, alpha=0.3) plt.tight_layout() plt.show() <pre>Detected problem intervals:\n  Interval 1: time 80-120 (duration = 40)\n</pre> <pre>/var/folders/86/m147b4k17lddvs_xsw0mj2zw0000gn/T/ipykernel_49121/686125127.py:25: UserWarning: Setting the 'color' property will override the edgecolor or facecolor properties.\n  ax.axvspan(\n</pre> <p>The algorithm successfully identified a contiguous interval that closely matches the true misfit period!</p> In\u00a0[7]: Copied! <pre># Define periods of interest (e.g., early, middle, late session)\nperiod_names = [\"Early\", \"Middle\", \"Late\"]\nperiod_masks = [\n    time &lt; 70,  # Early\n    (time &gt;= 70) &amp; (time &lt; 130),  # Middle (includes misfit)\n    time &gt;= 130,  # Late\n]\n\n# Aggregate metrics over periods\nkl_by_period = [aggregate_over_period(kl_div, mask, reduction=\"mean\") for mask in period_masks]\noverlap_by_period = [\n    aggregate_over_period(overlap, mask, reduction=\"mean\") for mask in period_masks\n]\n\n# Print results\nprint(\"Aggregated diagnostics by period:\")\nprint(f\"\\n{'Period':&lt;10} {'Mean KL':&gt;10} {'Mean Overlap':&gt;12}\")\nprint(\"-\" * 35)\nfor i, name in enumerate(period_names):\n    print(f\"{name:&lt;10} {kl_by_period[i]:&gt;10.3f} {overlap_by_period[i]:&gt;12.3f}\")\n\n# Visualize\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nx = np.arange(len(period_names))\nwidth = 0.6\n\n# KL divergence by period\nbars1 = ax1.bar(x, kl_by_period, width, color=[\"green\", \"red\", \"green\"], alpha=0.7)\nax1.axhline(1.0, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\")\nax1.set_ylabel(\"Mean KL Divergence\")\nax1.set_title(\"KL Divergence by Period\")\nax1.set_xticks(x)\nax1.set_xticklabels(period_names)\nax1.legend(frameon=True)\nax1.grid(True, alpha=0.3, axis=\"y\")\n\n# Overlap by period\nbars2 = ax2.bar(x, overlap_by_period, width, color=[\"green\", \"red\", \"green\"], alpha=0.7)\nax2.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\")\nax2.set_ylabel(\"Mean HPD Overlap\")\nax2.set_title(\"HPD Overlap by Period\")\nax2.set_xticks(x)\nax2.set_xticklabels(period_names)\nax2.legend(frameon=True)\nax2.grid(True, alpha=0.3, axis=\"y\")\n\nplt.tight_layout()\nplt.show()\n</pre> # Define periods of interest (e.g., early, middle, late session) period_names = [\"Early\", \"Middle\", \"Late\"] period_masks = [     time &lt; 70,  # Early     (time &gt;= 70) &amp; (time &lt; 130),  # Middle (includes misfit)     time &gt;= 130,  # Late ]  # Aggregate metrics over periods kl_by_period = [aggregate_over_period(kl_div, mask, reduction=\"mean\") for mask in period_masks] overlap_by_period = [     aggregate_over_period(overlap, mask, reduction=\"mean\") for mask in period_masks ]  # Print results print(\"Aggregated diagnostics by period:\") print(f\"\\n{'Period':&lt;10} {'Mean KL':&gt;10} {'Mean Overlap':&gt;12}\") print(\"-\" * 35) for i, name in enumerate(period_names):     print(f\"{name:&lt;10} {kl_by_period[i]:&gt;10.3f} {overlap_by_period[i]:&gt;12.3f}\")  # Visualize fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  x = np.arange(len(period_names)) width = 0.6  # KL divergence by period bars1 = ax1.bar(x, kl_by_period, width, color=[\"green\", \"red\", \"green\"], alpha=0.7) ax1.axhline(1.0, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\") ax1.set_ylabel(\"Mean KL Divergence\") ax1.set_title(\"KL Divergence by Period\") ax1.set_xticks(x) ax1.set_xticklabels(period_names) ax1.legend(frameon=True) ax1.grid(True, alpha=0.3, axis=\"y\")  # Overlap by period bars2 = ax2.bar(x, overlap_by_period, width, color=[\"green\", \"red\", \"green\"], alpha=0.7) ax2.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\") ax2.set_ylabel(\"Mean HPD Overlap\") ax2.set_title(\"HPD Overlap by Period\") ax2.set_xticks(x) ax2.set_xticklabels(period_names) ax2.legend(frameon=True) ax2.grid(True, alpha=0.3, axis=\"y\")  plt.tight_layout() plt.show() <pre>Aggregated diagnostics by period:\n\nPeriod        Mean KL Mean Overlap\n-----------------------------------\nEarly           0.128        1.000\nMiddle          5.651        0.333\nLate            0.128        1.000\n</pre> In\u00a0[8]: Copied! <pre># Generate data with good model fit\ngood_data = generate_spatial_navigation_data(\n    n_time=200,\n    track_length=100.0,\n    n_bins=50,\n    velocity=10.0,\n    state_uncertainty=2.5,\n    likelihood_uncertainty=3.0,\n    drift=0.0,  # No systematic bias\n    seed=42,\n)\n\n# Compute diagnostics\nkl_good = kl_divergence(good_data[\"state_dist\"], good_data[\"likelihood\"])\noverlap_good = hpd_overlap(good_data[\"state_dist\"], good_data[\"likelihood\"], coverage=0.95)\n\n# Visualize\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n\n# KL divergence\nax1.plot(good_data[\"time\"], kl_good, linewidth=2.5, color=\"#1f77b4\", alpha=0.7)\nax1.axhline(0.1, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\")\nax1.axhline(1.0, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\")\nax1.fill_between(good_data[\"time\"], 0, 0.1, alpha=0.2, color=\"green\", label=\"Good fit\")\nax1.set_ylabel(\"KL Divergence\")\nax1.set_title(\"Well-Specified Model: KL Divergence\")\nax1.legend(frameon=True, loc=\"upper right\")\nax1.grid(True, alpha=0.3)\n\n# HPD overlap\nax2.plot(good_data[\"time\"], overlap_good, linewidth=2.5, color=\"#ff7f0e\", alpha=0.7)\nax2.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\")\nax2.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\")\nax2.fill_between(good_data[\"time\"], 0.7, 1.0, alpha=0.2, color=\"green\", label=\"Good fit\")\nax2.set_xlabel(\"Time\")\nax2.set_ylabel(\"HPD Overlap\")\nax2.set_title(\"Well-Specified Model: HPD Overlap\")\nax2.legend(frameon=True, loc=\"lower right\")\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print summary statistics\nprint(\"Well-specified model summary:\")\nprint(f\"  Mean KL divergence: {np.mean(kl_good):.4f}\")\nprint(f\"  Mean HPD overlap: {np.mean(overlap_good):.4f}\")\nprint(f\"  % time with high KL (&gt; 1.0): {(kl_good &gt; 1.0).sum() / len(kl_good) * 100:.1f}%\")\nprint(\n    f\"  % time with low overlap (&lt; 0.3): {(overlap_good &lt; 0.3).sum() / len(overlap_good) * 100:.1f}%\"\n)\n</pre> # Generate data with good model fit good_data = generate_spatial_navigation_data(     n_time=200,     track_length=100.0,     n_bins=50,     velocity=10.0,     state_uncertainty=2.5,     likelihood_uncertainty=3.0,     drift=0.0,  # No systematic bias     seed=42, )  # Compute diagnostics kl_good = kl_divergence(good_data[\"state_dist\"], good_data[\"likelihood\"]) overlap_good = hpd_overlap(good_data[\"state_dist\"], good_data[\"likelihood\"], coverage=0.95)  # Visualize fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)  # KL divergence ax1.plot(good_data[\"time\"], kl_good, linewidth=2.5, color=\"#1f77b4\", alpha=0.7) ax1.axhline(0.1, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\") ax1.axhline(1.0, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\") ax1.fill_between(good_data[\"time\"], 0, 0.1, alpha=0.2, color=\"green\", label=\"Good fit\") ax1.set_ylabel(\"KL Divergence\") ax1.set_title(\"Well-Specified Model: KL Divergence\") ax1.legend(frameon=True, loc=\"upper right\") ax1.grid(True, alpha=0.3)  # HPD overlap ax2.plot(good_data[\"time\"], overlap_good, linewidth=2.5, color=\"#ff7f0e\", alpha=0.7) ax2.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\") ax2.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\") ax2.fill_between(good_data[\"time\"], 0.7, 1.0, alpha=0.2, color=\"green\", label=\"Good fit\") ax2.set_xlabel(\"Time\") ax2.set_ylabel(\"HPD Overlap\") ax2.set_title(\"Well-Specified Model: HPD Overlap\") ax2.legend(frameon=True, loc=\"lower right\") ax2.grid(True, alpha=0.3)  plt.tight_layout() plt.show()  # Print summary statistics print(\"Well-specified model summary:\") print(f\"  Mean KL divergence: {np.mean(kl_good):.4f}\") print(f\"  Mean HPD overlap: {np.mean(overlap_good):.4f}\") print(f\"  % time with high KL (&gt; 1.0): {(kl_good &gt; 1.0).sum() / len(kl_good) * 100:.1f}%\") print(     f\"  % time with low overlap (&lt; 0.3): {(overlap_good &lt; 0.3).sum() / len(overlap_good) * 100:.1f}%\" ) <pre>Well-specified model summary:\n  Mean KL divergence: 0.0295\n  Mean HPD overlap: 1.0000\n  % time with high KL (&gt; 1.0): 0.0%\n  % time with low overlap (&lt; 0.3): 0.0%\n</pre> In\u00a0[9]: Copied! <pre># Generate data with small systematic drift\ndrift_data = generate_spatial_navigation_data(\n    n_time=200,\n    track_length=100.0,\n    n_bins=50,\n    drift=5.0,  # Small 5 cm bias\n    seed=42,\n)\n\n# Compute diagnostics\nkl_drift = kl_divergence(drift_data[\"state_dist\"], drift_data[\"likelihood\"])\noverlap_drift = hpd_overlap(drift_data[\"state_dist\"], drift_data[\"likelihood\"], coverage=0.95)\n\n# Compare to good model\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n\n# Good model - KL\nax1.plot(good_data[\"time\"], kl_good, linewidth=2.5, color=\"#1f77b4\", alpha=0.7)\nax1.axhline(0.1, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2)\nax1.set_ylabel(\"KL Divergence\")\nax1.set_title(\"Good Model: KL Divergence\")\nax1.grid(True, alpha=0.3)\nax1.set_ylim([0, ax1.get_ylim()[1]])\n\n# Drift model - KL\nax2.plot(drift_data[\"time\"], kl_drift, linewidth=2.5, color=\"#d62728\", alpha=0.7)\nax2.axhline(0.1, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2)\nax2.set_ylabel(\"KL Divergence\")\nax2.set_title(\"Biased Model: KL Divergence (5 cm drift)\")\nax2.grid(True, alpha=0.3)\nax2.set_ylim([0, ax2.get_ylim()[1]])\n\n# Good model - Overlap\nax3.plot(good_data[\"time\"], overlap_good, linewidth=2.5, color=\"#1f77b4\", alpha=0.7)\nax3.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2)\nax3.set_xlabel(\"Time\")\nax3.set_ylabel(\"HPD Overlap\")\nax3.set_title(\"Good Model: HPD Overlap\")\nax3.grid(True, alpha=0.3)\n\n# Drift model - Overlap\nax4.plot(drift_data[\"time\"], overlap_drift, linewidth=2.5, color=\"#d62728\", alpha=0.7)\nax4.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2)\nax4.set_xlabel(\"Time\")\nax4.set_ylabel(\"HPD Overlap\")\nax4.set_title(\"Biased Model: HPD Overlap\")\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print comparison\nprint(\"Model comparison:\")\nprint(f\"\\n{'Metric':&lt;25} {'Good Model':&gt;12} {'Biased Model':&gt;12} {'Difference':&gt;12}\")\nprint(\"-\" * 65)\nprint(\n    f\"{'Mean KL divergence':&lt;25} {np.mean(kl_good):&gt;12.4f} {np.mean(kl_drift):&gt;12.4f} {np.mean(kl_drift) - np.mean(kl_good):&gt;12.4f}\"\n)\nprint(\n    f\"{'Mean HPD overlap':&lt;25} {np.mean(overlap_good):&gt;12.4f} {np.mean(overlap_drift):&gt;12.4f} {np.mean(overlap_drift) - np.mean(overlap_good):&gt;12.4f}\"\n)\n</pre> # Generate data with small systematic drift drift_data = generate_spatial_navigation_data(     n_time=200,     track_length=100.0,     n_bins=50,     drift=5.0,  # Small 5 cm bias     seed=42, )  # Compute diagnostics kl_drift = kl_divergence(drift_data[\"state_dist\"], drift_data[\"likelihood\"]) overlap_drift = hpd_overlap(drift_data[\"state_dist\"], drift_data[\"likelihood\"], coverage=0.95)  # Compare to good model fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))  # Good model - KL ax1.plot(good_data[\"time\"], kl_good, linewidth=2.5, color=\"#1f77b4\", alpha=0.7) ax1.axhline(0.1, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2) ax1.set_ylabel(\"KL Divergence\") ax1.set_title(\"Good Model: KL Divergence\") ax1.grid(True, alpha=0.3) ax1.set_ylim([0, ax1.get_ylim()[1]])  # Drift model - KL ax2.plot(drift_data[\"time\"], kl_drift, linewidth=2.5, color=\"#d62728\", alpha=0.7) ax2.axhline(0.1, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2) ax2.set_ylabel(\"KL Divergence\") ax2.set_title(\"Biased Model: KL Divergence (5 cm drift)\") ax2.grid(True, alpha=0.3) ax2.set_ylim([0, ax2.get_ylim()[1]])  # Good model - Overlap ax3.plot(good_data[\"time\"], overlap_good, linewidth=2.5, color=\"#1f77b4\", alpha=0.7) ax3.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2) ax3.set_xlabel(\"Time\") ax3.set_ylabel(\"HPD Overlap\") ax3.set_title(\"Good Model: HPD Overlap\") ax3.grid(True, alpha=0.3)  # Drift model - Overlap ax4.plot(drift_data[\"time\"], overlap_drift, linewidth=2.5, color=\"#d62728\", alpha=0.7) ax4.axhline(0.7, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2) ax4.set_xlabel(\"Time\") ax4.set_ylabel(\"HPD Overlap\") ax4.set_title(\"Biased Model: HPD Overlap\") ax4.grid(True, alpha=0.3)  plt.tight_layout() plt.show()  # Print comparison print(\"Model comparison:\") print(f\"\\n{'Metric':&lt;25} {'Good Model':&gt;12} {'Biased Model':&gt;12} {'Difference':&gt;12}\") print(\"-\" * 65) print(     f\"{'Mean KL divergence':&lt;25} {np.mean(kl_good):&gt;12.4f} {np.mean(kl_drift):&gt;12.4f} {np.mean(kl_drift) - np.mean(kl_good):&gt;12.4f}\" ) print(     f\"{'Mean HPD overlap':&lt;25} {np.mean(overlap_good):&gt;12.4f} {np.mean(overlap_drift):&gt;12.4f} {np.mean(overlap_drift) - np.mean(overlap_good):&gt;12.4f}\" ) <pre>Model comparison:\n\nMetric                      Good Model Biased Model   Difference\n-----------------------------------------------------------------\nMean KL divergence              0.0295       1.5166       1.4870\nMean HPD overlap                1.0000       0.6292      -0.3708\n</pre>"},{"location":"tutorials/03_time_resolved_diagnostics/#time-resolved-diagnostics-for-state-space-models","title":"Time-Resolved Diagnostics for State Space Models\u00b6","text":"<p>This notebook demonstrates how to apply goodness-of-fit diagnostics to time series data, identifying when and where models fail. This is the core application of the <code>statespacecheck</code> package.</p> <p>Learning objectives:</p> <ul> <li>Apply diagnostics to realistic time series data</li> <li>Identify periods of poor model fit</li> <li>Use flagging functions to detect problematic time points</li> <li>Aggregate metrics over time periods</li> <li>Interpret diagnostic patterns in neuroscience context</li> </ul> <p>Previous: 02_highest_density_regions.ipynb | Next: 04_predictive_checks.ipynb</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/03_time_resolved_diagnostics/#example-scenario-spatial-navigation","title":"Example Scenario: Spatial Navigation\u00b6","text":"<p>Imagine we have a state space model tracking an animal's position on a linear track. The model uses:</p> <ul> <li>State model: Predicts position based on velocity and past positions</li> <li>Observation model: Infers position from neural activity (e.g., place cell firing)</li> </ul> <p>We want to know: Does the model accurately capture the animal's behavior throughout the session?</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#generate-synthetic-data","title":"Generate Synthetic Data\u00b6","text":"<p>Let's create a realistic example where the model works well most of the time but fails during a specific period.</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#visualize-distributions-over-time","title":"Visualize Distributions Over Time\u00b6","text":"<p>Let's first look at the distributions as a heatmap to get a sense of the temporal evolution.</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#observations","title":"Observations\u00b6","text":"<ul> <li>Top panel: State distribution tracks the true position smoothly</li> <li>Middle panel: Likelihood also tracks position, but with a large offset during the misfit period (80-120)</li> <li>Bottom panel: The difference highlights the misfit period in red</li> </ul> <p>Now let's quantify this disagreement using our diagnostic metrics.</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#compute-time-resolved-diagnostics","title":"Compute Time-Resolved Diagnostics\u00b6","text":"<p>The power of <code>statespacecheck</code> is computing diagnostics at every time point to identify when models fail.</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#interpretation","title":"Interpretation\u00b6","text":"<p>KL Divergence (top):</p> <ul> <li>Low and stable during good fit periods</li> <li>Spikes dramatically during misfit period (80-120)</li> <li>Correctly identifies the problem period!</li> </ul> <p>HPD Overlap (bottom):</p> <ul> <li>High and stable during good fit periods</li> <li>Drops to near zero during misfit period</li> <li>Also correctly identifies the problem!</li> </ul> <p>Key insight: Both metrics successfully detect the misfit period without any knowledge of ground truth. This is the power of comparing distributions that should be consistent.</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#automatic-problem-detection","title":"Automatic Problem Detection\u00b6","text":"<p>Instead of manually inspecting plots, we can use flagging functions to automatically identify problematic time points.</p> <p>Key parameters:</p> <ul> <li><code>z_thresh</code>: Z-score threshold for KL divergence (higher = more conservative)</li> <li><code>threshold</code>: Overlap threshold (lower values = stricter, flag more time points)</li> <li><code>min_len</code>: Minimum duration to filter out brief artifacts</li> </ul>"},{"location":"tutorials/03_time_resolved_diagnostics/#finding-contiguous-problem-periods","title":"Finding Contiguous Problem Periods\u00b6","text":"<p>Often we want to identify continuous intervals of poor fit, not just individual time points.</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#aggregating-over-periods","title":"Aggregating Over Periods\u00b6","text":"<p>Sometimes we want to summarize diagnostics over specific time periods (e.g., task epochs, behavioral states).</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#interpretation","title":"Interpretation\u00b6","text":"<p>The middle period (which contains our misfit period) shows:</p> <ul> <li>High mean KL divergence (above threshold)</li> <li>Low mean overlap (below threshold)</li> </ul> <p>This confirms poor model fit during that epoch, while the early and late periods show good fit.</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#realistic-example-good-model-fit","title":"Realistic Example: Good Model Fit\u00b6","text":"<p>Let's contrast this with a well-specified model where state and likelihood are consistent throughout.</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#interpretation","title":"Interpretation\u00b6","text":"<p>For a well-specified model:</p> <ul> <li>KL divergence: Consistently low, occasional small fluctuations</li> <li>HPD overlap: Consistently high, stays well above threshold</li> <li>No systematic problems: Metrics don't show prolonged periods of poor fit</li> </ul> <p>Compare to misspecified model:</p> <ul> <li>Well-specified: Mean KL ~ 0.01, Mean overlap ~ 0.85</li> <li>Misspecified: Mean KL ~ 0.5, Mean overlap ~ 0.6</li> <li>Clear quantitative difference!</li> </ul>"},{"location":"tutorials/03_time_resolved_diagnostics/#detecting-subtle-model-issues","title":"Detecting Subtle Model Issues\u00b6","text":"<p>Let's explore a more subtle case: small systematic bias between state and likelihood.</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#interpretation","title":"Interpretation\u00b6","text":"<p>Even a small 5 cm systematic bias is detectable:</p> <ul> <li>KL divergence: Noticeably higher in biased model</li> <li>HPD overlap: Noticeably lower in biased model</li> <li>Diagnostics are sensitive to subtle model issues</li> </ul> <p>This demonstrates the power of these metrics for detecting problems that might be missed by eye or by global accuracy measures.</p>"},{"location":"tutorials/03_time_resolved_diagnostics/#summary-what-we-learned","title":"Summary: What We Learned\u00b6","text":"<p>Core workflow:</p> <ol> <li>Compute time-resolved diagnostics (<code>kl_divergence</code>, <code>hpd_overlap</code>)</li> <li>Visualize metrics over time to identify patterns</li> <li>Use flagging functions to automatically detect problems</li> <li>Find contiguous problem intervals</li> <li>Aggregate over task periods for summary statistics</li> </ol> <p>Key functions:</p> <ul> <li><code>kl_divergence()</code>, <code>hpd_overlap()</code>: Compute metrics at each time point</li> <li><code>flag_extreme_kl()</code>, <code>flag_low_overlap()</code>: Detect problematic time points</li> <li><code>combine_flags()</code>: Combine multiple diagnostic criteria</li> <li><code>find_low_overlap_intervals()</code>: Find contiguous problem periods</li> <li><code>aggregate_over_period()</code>: Summarize metrics over epochs</li> </ul> <p>Practical insights:</p> <ul> <li>Metrics successfully detect misfit periods without ground truth</li> <li>Both local (time-resolved) and global (aggregated) views are informative</li> <li>Multiple complementary diagnostics provide robustness</li> <li>Sensitive enough to detect subtle systematic biases</li> </ul> <p>Neuroscience applications:</p> <ul> <li>Identify when state space models fail during behavior</li> <li>Compare model performance across task epochs</li> <li>Detect systematic biases in neural decoding</li> <li>Validate model assumptions across different behavioral states</li> </ul> <p>Next steps:</p> <ul> <li>Next notebook: 04_predictive_checks.ipynb - Advanced posterior predictive checks</li> <li>Apply to your data: Use these tools on real state space model outputs!</li> </ul>"},{"location":"tutorials/03_time_resolved_diagnostics/#exercises-optional","title":"Exercises (Optional)\u00b6","text":"<ol> <li>Try different threshold values for flagging. How does this affect sensitivity vs specificity?</li> <li>Generate data with multiple short misfit periods. Can the interval detection find them?</li> <li>What's the smallest drift you can reliably detect with these metrics?</li> <li>Create a case where KL divergence detects a problem but overlap doesn't (or vice versa). Why does this happen?</li> </ol>"},{"location":"tutorials/04_predictive_checks/","title":"Posterior Predictive Checks for State Space Models","text":"In\u00a0[1]: Copied! <pre>import sys\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\nsys.path.insert(0, str(Path.cwd().parent))\n\nfrom utils import configure_notebook_plotting\n\nfrom statespacecheck import (\n    log_predictive_density,\n    predictive_density,\n    predictive_pvalue,\n)\n\nconfigure_notebook_plotting()\n</pre> import sys from pathlib import Path  import matplotlib.pyplot as plt import numpy as np from scipy.stats import norm  sys.path.insert(0, str(Path.cwd().parent))  from utils import configure_notebook_plotting  from statespacecheck import (     log_predictive_density,     predictive_density,     predictive_pvalue, )  configure_notebook_plotting() In\u00a0[2]: Copied! <pre># Generate simple data\nposition_bins = np.linspace(0, 100, 50)\n\n# State distribution: Model expects position around 50 cm\nstate_dist = norm.pdf(position_bins, loc=50.0, scale=5.0)\nstate_dist = state_dist / state_dist.sum()  # Normalize\n\n# Likelihood: Observation suggests position around 52 cm (close to prediction)\nlikelihood = norm.pdf(position_bins, loc=52.0, scale=3.0)\n# NOTE: likelihood is NOT normalized - it's p(y|x) evaluated at each x\n\n# Add time dimension\nstate_dist_2d = state_dist[np.newaxis, :]\nlikelihood_2d = likelihood[np.newaxis, :]\n\n# Compute predictive density\npred_dens = predictive_density(state_dist_2d, likelihood_2d)[0]\n\nprint(f\"Predictive density: {pred_dens:.6f}\")\nprint(f\"\\nInterpretation: The observed data has predictive density {pred_dens:.6f}\")\nprint(\"This value depends on the bin size and normalization.\")\n\n# Visualize\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(position_bins, state_dist, linewidth=2.5, color=\"#1f77b4\", label=\"State distribution p(x)\")\nax.plot(\n    position_bins,\n    likelihood,\n    linewidth=2.5,\n    color=\"#ff7f0e\",\n    linestyle=\"--\",\n    label=\"Likelihood p(y|x)\",\n)\nax.fill_between(position_bins, state_dist, alpha=0.3, color=\"#1f77b4\")\nax.fill_between(position_bins, likelihood, alpha=0.3, color=\"#ff7f0e\")\nax.axvline(50, color=\"#1f77b4\", linestyle=\":\", alpha=0.5, label=\"State mean\")\nax.axvline(52, color=\"#ff7f0e\", linestyle=\":\", alpha=0.5, label=\"Likelihood mean\")\nax.set_xlabel(\"Position (cm)\")\nax.set_ylabel(\"Probability Density\")\nax.set_title(f\"Predictive Density Computation (p(y) = {pred_dens:.6f})\")\nax.legend(frameon=True, loc=\"upper right\")\nplt.tight_layout()\nplt.show()\n</pre> # Generate simple data position_bins = np.linspace(0, 100, 50)  # State distribution: Model expects position around 50 cm state_dist = norm.pdf(position_bins, loc=50.0, scale=5.0) state_dist = state_dist / state_dist.sum()  # Normalize  # Likelihood: Observation suggests position around 52 cm (close to prediction) likelihood = norm.pdf(position_bins, loc=52.0, scale=3.0) # NOTE: likelihood is NOT normalized - it's p(y|x) evaluated at each x  # Add time dimension state_dist_2d = state_dist[np.newaxis, :] likelihood_2d = likelihood[np.newaxis, :]  # Compute predictive density pred_dens = predictive_density(state_dist_2d, likelihood_2d)[0]  print(f\"Predictive density: {pred_dens:.6f}\") print(f\"\\nInterpretation: The observed data has predictive density {pred_dens:.6f}\") print(\"This value depends on the bin size and normalization.\")  # Visualize fig, ax = plt.subplots(figsize=(12, 5)) ax.plot(position_bins, state_dist, linewidth=2.5, color=\"#1f77b4\", label=\"State distribution p(x)\") ax.plot(     position_bins,     likelihood,     linewidth=2.5,     color=\"#ff7f0e\",     linestyle=\"--\",     label=\"Likelihood p(y|x)\", ) ax.fill_between(position_bins, state_dist, alpha=0.3, color=\"#1f77b4\") ax.fill_between(position_bins, likelihood, alpha=0.3, color=\"#ff7f0e\") ax.axvline(50, color=\"#1f77b4\", linestyle=\":\", alpha=0.5, label=\"State mean\") ax.axvline(52, color=\"#ff7f0e\", linestyle=\":\", alpha=0.5, label=\"Likelihood mean\") ax.set_xlabel(\"Position (cm)\") ax.set_ylabel(\"Probability Density\") ax.set_title(f\"Predictive Density Computation (p(y) = {pred_dens:.6f})\") ax.legend(frameon=True, loc=\"upper right\") plt.tight_layout() plt.show() <pre>Predictive density: 0.064510\n\nInterpretation: The observed data has predictive density 0.064510\nThis value depends on the bin size and normalization.\n</pre> In\u00a0[3]: Copied! <pre># Case 1: Good fit - likelihood agrees with state\nlikelihood_good = norm.pdf(position_bins, loc=50.0, scale=3.0)  # Same location as state\nlikelihood_good_2d = likelihood_good[np.newaxis, :]\npred_dens_good = predictive_density(state_dist_2d, likelihood_good_2d)[0]\n\n# Case 2: Poor fit - likelihood disagrees with state\nlikelihood_poor = norm.pdf(position_bins, loc=80.0, scale=3.0)  # Far from state\nlikelihood_poor_2d = likelihood_poor[np.newaxis, :]\npred_dens_poor = predictive_density(state_dist_2d, likelihood_poor_2d)[0]\n\n# Visualize\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Good fit\nax1.plot(position_bins, state_dist, linewidth=2.5, color=\"#1f77b4\", label=\"State dist\")\nax1.plot(\n    position_bins,\n    likelihood_good,\n    linewidth=2.5,\n    color=\"#2ca02c\",\n    linestyle=\"--\",\n    label=\"Likelihood\",\n)\nax1.fill_between(position_bins, state_dist, alpha=0.3, color=\"#1f77b4\")\nax1.fill_between(position_bins, likelihood_good, alpha=0.3, color=\"#2ca02c\")\nax1.set_xlabel(\"Position (cm)\")\nax1.set_ylabel(\"Density\")\nax1.set_title(f\"Good Fit: p(y) = {pred_dens_good:.6f}\")\nax1.legend(frameon=True, loc=\"upper right\")\n\n# Poor fit\nax2.plot(position_bins, state_dist, linewidth=2.5, color=\"#1f77b4\", label=\"State dist\")\nax2.plot(\n    position_bins,\n    likelihood_poor,\n    linewidth=2.5,\n    color=\"#d62728\",\n    linestyle=\"--\",\n    label=\"Likelihood\",\n)\nax2.fill_between(position_bins, state_dist, alpha=0.3, color=\"#1f77b4\")\nax2.fill_between(position_bins, likelihood_poor, alpha=0.3, color=\"#d62728\")\nax2.set_xlabel(\"Position (cm)\")\nax2.set_ylabel(\"Density\")\nax2.set_title(f\"Poor Fit: p(y) = {pred_dens_poor:.6f}\")\nax2.legend(frameon=True, loc=\"upper right\")\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Predictive density comparison:\")\nprint(f\"  Good fit: {pred_dens_good:.6f}\")\nprint(f\"  Poor fit: {pred_dens_poor:.6f}\")\nprint(f\"  Ratio: {pred_dens_good / pred_dens_poor:.1f}x higher for good fit\")\n</pre> # Case 1: Good fit - likelihood agrees with state likelihood_good = norm.pdf(position_bins, loc=50.0, scale=3.0)  # Same location as state likelihood_good_2d = likelihood_good[np.newaxis, :] pred_dens_good = predictive_density(state_dist_2d, likelihood_good_2d)[0]  # Case 2: Poor fit - likelihood disagrees with state likelihood_poor = norm.pdf(position_bins, loc=80.0, scale=3.0)  # Far from state likelihood_poor_2d = likelihood_poor[np.newaxis, :] pred_dens_poor = predictive_density(state_dist_2d, likelihood_poor_2d)[0]  # Visualize fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))  # Good fit ax1.plot(position_bins, state_dist, linewidth=2.5, color=\"#1f77b4\", label=\"State dist\") ax1.plot(     position_bins,     likelihood_good,     linewidth=2.5,     color=\"#2ca02c\",     linestyle=\"--\",     label=\"Likelihood\", ) ax1.fill_between(position_bins, state_dist, alpha=0.3, color=\"#1f77b4\") ax1.fill_between(position_bins, likelihood_good, alpha=0.3, color=\"#2ca02c\") ax1.set_xlabel(\"Position (cm)\") ax1.set_ylabel(\"Density\") ax1.set_title(f\"Good Fit: p(y) = {pred_dens_good:.6f}\") ax1.legend(frameon=True, loc=\"upper right\")  # Poor fit ax2.plot(position_bins, state_dist, linewidth=2.5, color=\"#1f77b4\", label=\"State dist\") ax2.plot(     position_bins,     likelihood_poor,     linewidth=2.5,     color=\"#d62728\",     linestyle=\"--\",     label=\"Likelihood\", ) ax2.fill_between(position_bins, state_dist, alpha=0.3, color=\"#1f77b4\") ax2.fill_between(position_bins, likelihood_poor, alpha=0.3, color=\"#d62728\") ax2.set_xlabel(\"Position (cm)\") ax2.set_ylabel(\"Density\") ax2.set_title(f\"Poor Fit: p(y) = {pred_dens_poor:.6f}\") ax2.legend(frameon=True, loc=\"upper right\")  plt.tight_layout() plt.show()  print(\"Predictive density comparison:\") print(f\"  Good fit: {pred_dens_good:.6f}\") print(f\"  Poor fit: {pred_dens_poor:.6f}\") print(f\"  Ratio: {pred_dens_good / pred_dens_poor:.1f}x higher for good fit\") <pre>Predictive density comparison:\n  Good fit: 0.068418\n  Poor fit: 0.000000\n  Ratio: 559777.2x higher for good fit\n</pre> <p>The predictive density is much higher when state and likelihood agree! This quantifies how \"surprising\" the data is under the model.</p> In\u00a0[4]: Copied! <pre># Demonstrate numerical issues with many bins\nposition_bins_fine = np.linspace(0, 100, 500)  # Many bins!\n\nstate_fine = norm.pdf(position_bins_fine, loc=50.0, scale=5.0)\nstate_fine = state_fine / state_fine.sum()\nlikelihood_fine = norm.pdf(position_bins_fine, loc=50.0, scale=3.0)\n\nstate_fine_2d = state_fine[np.newaxis, :]\nlikelihood_fine_2d = likelihood_fine[np.newaxis, :]\n\n# Linear space computation\npred_dens_linear = predictive_density(state_fine_2d, likelihood_fine_2d)[0]\n\n# Log space computation\nlog_pred_dens = log_predictive_density(state_fine_2d, likelihood=likelihood_fine_2d)[0]\n\nprint(f\"Linear space: p(y) = {pred_dens_linear:.10f}\")\nprint(f\"Log space: log p(y) = {log_pred_dens:.6f}\")\nprint(f\"Verification: exp(log p(y)) = {np.exp(log_pred_dens):.10f}\")\nprint(\"\\nLog space is more numerically stable for small probabilities!\")\n</pre> # Demonstrate numerical issues with many bins position_bins_fine = np.linspace(0, 100, 500)  # Many bins!  state_fine = norm.pdf(position_bins_fine, loc=50.0, scale=5.0) state_fine = state_fine / state_fine.sum() likelihood_fine = norm.pdf(position_bins_fine, loc=50.0, scale=3.0)  state_fine_2d = state_fine[np.newaxis, :] likelihood_fine_2d = likelihood_fine[np.newaxis, :]  # Linear space computation pred_dens_linear = predictive_density(state_fine_2d, likelihood_fine_2d)[0]  # Log space computation log_pred_dens = log_predictive_density(state_fine_2d, likelihood=likelihood_fine_2d)[0]  print(f\"Linear space: p(y) = {pred_dens_linear:.10f}\") print(f\"Log space: log p(y) = {log_pred_dens:.6f}\") print(f\"Verification: exp(log p(y)) = {np.exp(log_pred_dens):.10f}\") print(\"\\nLog space is more numerically stable for small probabilities!\") <pre>Linear space: p(y) = 0.0684180366\nLog space: log p(y) = -2.682119\nVerification: exp(log p(y)) = 0.0684180366\n\nLog space is more numerically stable for small probabilities!\n</pre> <p>Why use log-space?</p> <ul> <li>Prevents underflow for very small probabilities</li> <li>More numerically stable for optimization</li> <li>Standard practice in Bayesian inference</li> <li>Can directly use log-likelihoods without exp/log round-trip</li> </ul> In\u00a0[5]: Copied! <pre># Generate time series data\nnp.random.seed(42)\nn_time = 100\n\n# True model: position follows Gaussian with some dynamics\ntrue_means = 50 + 10 * np.sin(2 * np.pi * np.arange(n_time) / 50)\nstate_dist_seq = np.array([norm.pdf(position_bins, loc=m, scale=5.0) for m in true_means])\nstate_dist_seq = state_dist_seq / state_dist_seq.sum(axis=1, keepdims=True)\n\n# Observed likelihood: mostly consistent with model\nobserved_likelihood = np.array(\n    [norm.pdf(position_bins, loc=m + np.random.normal(0, 2), scale=3.0) for m in true_means]\n)\n\n# Compute observed log predictive density\nobserved_log_pred = log_predictive_density(state_dist_seq, likelihood=observed_likelihood)\n\nprint(\"Observed log predictive densities (first 5 time points):\")\nprint(observed_log_pred[:5])\n</pre> # Generate time series data np.random.seed(42) n_time = 100  # True model: position follows Gaussian with some dynamics true_means = 50 + 10 * np.sin(2 * np.pi * np.arange(n_time) / 50) state_dist_seq = np.array([norm.pdf(position_bins, loc=m, scale=5.0) for m in true_means]) state_dist_seq = state_dist_seq / state_dist_seq.sum(axis=1, keepdims=True)  # Observed likelihood: mostly consistent with model observed_likelihood = np.array(     [norm.pdf(position_bins, loc=m + np.random.normal(0, 2), scale=3.0) for m in true_means] )  # Compute observed log predictive density observed_log_pred = log_predictive_density(state_dist_seq, likelihood=observed_likelihood)  print(\"Observed log predictive densities (first 5 time points):\") print(observed_log_pred[:5]) <pre>Observed log predictive densities (first 5 time points):\n[-2.69663203 -2.68324333 -2.70679529 -2.81856703 -2.68534396]\n</pre> <p>Now let's create a sampler that generates datasets from the model and compute their predictive densities:</p> In\u00a0[6]: Copied! <pre>def create_sampler(state_dist_seq, true_means, position_bins):\n    \"\"\"Create a function that samples predictive densities from the model.\"\"\"\n    rng = np.random.default_rng(42)  # Fixed seed for reproducibility\n\n    def sampler(n_samples):\n        \"\"\"Sample n predictive density realizations from the model.\"\"\"\n        log_pred_samples = np.zeros((n_samples, len(true_means)))\n\n        for i in range(n_samples):\n            # Simulate likelihood from the model\n            sim_likelihood = np.array(\n                [norm.pdf(position_bins, loc=m + rng.normal(0, 2), scale=3.0) for m in true_means]\n            )\n\n            # Compute predictive density for this simulation\n            log_pred_samples[i] = log_predictive_density(state_dist_seq, likelihood=sim_likelihood)\n\n        return log_pred_samples\n\n    return sampler\n\n\n# Create sampler\nsampler = create_sampler(state_dist_seq, true_means, position_bins)\n\n# Compute p-values\np_values = predictive_pvalue(observed_log_pred, sampler, n_samples=1000)\n\nprint(\"\\nP-values (first 10 time points):\")\nprint(p_values[:10])\nprint(\"\\nP-value statistics:\")\nprint(f\"  Mean: {np.mean(p_values):.3f}\")\nprint(f\"  Std: {np.std(p_values):.3f}\")\nprint(f\"  Min: {np.min(p_values):.3f}\")\nprint(f\"  Max: {np.max(p_values):.3f}\")\n</pre> def create_sampler(state_dist_seq, true_means, position_bins):     \"\"\"Create a function that samples predictive densities from the model.\"\"\"     rng = np.random.default_rng(42)  # Fixed seed for reproducibility      def sampler(n_samples):         \"\"\"Sample n predictive density realizations from the model.\"\"\"         log_pred_samples = np.zeros((n_samples, len(true_means)))          for i in range(n_samples):             # Simulate likelihood from the model             sim_likelihood = np.array(                 [norm.pdf(position_bins, loc=m + rng.normal(0, 2), scale=3.0) for m in true_means]             )              # Compute predictive density for this simulation             log_pred_samples[i] = log_predictive_density(state_dist_seq, likelihood=sim_likelihood)          return log_pred_samples      return sampler   # Create sampler sampler = create_sampler(state_dist_seq, true_means, position_bins)  # Compute p-values p_values = predictive_pvalue(observed_log_pred, sampler, n_samples=1000)  print(\"\\nP-values (first 10 time points):\") print(p_values[:10]) print(\"\\nP-value statistics:\") print(f\"  Mean: {np.mean(p_values):.3f}\") print(f\"  Std: {np.std(p_values):.3f}\") print(f\"  Min: {np.min(p_values):.3f}\") print(f\"  Max: {np.max(p_values):.3f}\") <pre>\nP-values (first 10 time points):\n[0.392 0.096 0.49  0.864 0.185 0.192 0.892 0.53  0.356 0.44 ]\n\nP-value statistics:\n  Mean: 0.467\n  Std: 0.278\n  Min: 0.005\n  Max: 0.995\n</pre> <p>For a well-specified model, p-values should be approximately uniformly distributed between 0 and 1. Let's visualize:</p> In\u00a0[7]: Copied! <pre>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Time series of p-values\nax1.plot(\n    np.arange(n_time), p_values, linewidth=2, marker=\"o\", markersize=4, alpha=0.7, color=\"#1f77b4\"\n)\nax1.axhline(0.5, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Expected mean\")\nax1.fill_between(np.arange(n_time), 0.05, 0.95, alpha=0.2, color=\"green\", label=\"90% range\")\nax1.set_xlabel(\"Time\")\nax1.set_ylabel(\"P-value\")\nax1.set_title(\"Posterior Predictive P-values Over Time\")\nax1.legend(frameon=True, loc=\"upper right\")\nax1.grid(True, alpha=0.3)\n\n# Histogram of p-values\nax2.hist(p_values, bins=20, edgecolor=\"black\", alpha=0.7, color=\"#1f77b4\", density=True)\nax2.axhline(1.0, color=\"red\", linestyle=\"--\", linewidth=2, alpha=0.5, label=\"Uniform distribution\")\nax2.set_xlabel(\"P-value\")\nax2.set_ylabel(\"Density\")\nax2.set_title(\"Distribution of P-values\")\nax2.legend(frameon=True, loc=\"upper right\")\nax2.grid(True, alpha=0.3, axis=\"y\")\n\nplt.tight_layout()\nplt.show()\n</pre> fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))  # Time series of p-values ax1.plot(     np.arange(n_time), p_values, linewidth=2, marker=\"o\", markersize=4, alpha=0.7, color=\"#1f77b4\" ) ax1.axhline(0.5, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Expected mean\") ax1.fill_between(np.arange(n_time), 0.05, 0.95, alpha=0.2, color=\"green\", label=\"90% range\") ax1.set_xlabel(\"Time\") ax1.set_ylabel(\"P-value\") ax1.set_title(\"Posterior Predictive P-values Over Time\") ax1.legend(frameon=True, loc=\"upper right\") ax1.grid(True, alpha=0.3)  # Histogram of p-values ax2.hist(p_values, bins=20, edgecolor=\"black\", alpha=0.7, color=\"#1f77b4\", density=True) ax2.axhline(1.0, color=\"red\", linestyle=\"--\", linewidth=2, alpha=0.5, label=\"Uniform distribution\") ax2.set_xlabel(\"P-value\") ax2.set_ylabel(\"Density\") ax2.set_title(\"Distribution of P-values\") ax2.legend(frameon=True, loc=\"upper right\") ax2.grid(True, alpha=0.3, axis=\"y\")  plt.tight_layout() plt.show() In\u00a0[8]: Copied! <pre># Create misspecified scenario: model predicts wrong location during middle period\nn_time = 100\nmisfit_start = 40\nmisfit_end = 60\n\n# State distribution: model's prediction\nmodel_means = 50 + 10 * np.sin(2 * np.pi * np.arange(n_time) / 50)\nstate_dist_misspec = np.array([norm.pdf(position_bins, loc=m, scale=5.0) for m in model_means])\nstate_dist_misspec = state_dist_misspec / state_dist_misspec.sum(axis=1, keepdims=True)\n\n# Observed data: matches model except during misfit period\nrng = np.random.default_rng(42)\nobserved_means = model_means.copy()\nobserved_means[misfit_start:misfit_end] += 25  # Large offset during misfit period!\n\nobserved_like_misspec = np.array(\n    [norm.pdf(position_bins, loc=m + rng.normal(0, 2), scale=3.0) for m in observed_means]\n)\n\n# Compute observed log predictive density\nobserved_log_pred_misspec = log_predictive_density(\n    state_dist_misspec, likelihood=observed_like_misspec\n)\n\n# Create sampler (using correct model means, not observed)\nsampler_misspec = create_sampler(state_dist_misspec, model_means, position_bins)\n\n# Compute p-values\np_values_misspec = predictive_pvalue(observed_log_pred_misspec, sampler_misspec, n_samples=1000)\n\n# Visualize\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# P-values over time\nax1.plot(\n    np.arange(n_time),\n    p_values_misspec,\n    linewidth=2,\n    marker=\"o\",\n    markersize=4,\n    alpha=0.7,\n    color=\"#1f77b4\",\n)\nax1.axhline(0.5, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Expected mean\")\nax1.axhline(0.05, color=\"red\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Extreme thresholds\")\nax1.axhline(0.95, color=\"red\", linestyle=\"--\", alpha=0.5, linewidth=2)\nax1.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"True misfit period\")\nax1.set_ylabel(\"P-value\")\nax1.set_title(\"P-values with Model Misspecification\")\nax1.legend(frameon=True, loc=\"upper right\")\nax1.grid(True, alpha=0.3)\n\n# Log predictive density over time\nax2.plot(np.arange(n_time), observed_log_pred_misspec, linewidth=2, color=\"#ff7f0e\")\nax2.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\")\nax2.set_xlabel(\"Time\")\nax2.set_ylabel(\"Log Predictive Density\")\nax2.set_title(\"Log Predictive Density Over Time\")\nax2.legend(frameon=True, loc=\"upper right\")\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print statistics\nprint(\"P-value statistics by period:\")\nprint(f\"\\n{'Period':&lt;15} {'Mean P-value':&gt;12} {'% Extreme (&lt;0.05 or &gt;0.95)':&gt;25}\")\nprint(\"-\" * 55)\ngood_pvals = p_values_misspec[\n    np.concatenate([np.arange(misfit_start), np.arange(misfit_end, n_time)])\n]\nbad_pvals = p_values_misspec[misfit_start:misfit_end]\nprint(\n    f\"{'Good fit':&lt;15} {np.mean(good_pvals):&gt;12.3f} {((good_pvals &lt; 0.05) | (good_pvals &gt; 0.95)).sum() / len(good_pvals) * 100:&gt;24.1f}%\"\n)\nprint(\n    f\"{'Misfit':&lt;15} {np.mean(bad_pvals):&gt;12.3f} {((bad_pvals &lt; 0.05) | (bad_pvals &gt; 0.95)).sum() / len(bad_pvals) * 100:&gt;24.1f}%\"\n)\n</pre> # Create misspecified scenario: model predicts wrong location during middle period n_time = 100 misfit_start = 40 misfit_end = 60  # State distribution: model's prediction model_means = 50 + 10 * np.sin(2 * np.pi * np.arange(n_time) / 50) state_dist_misspec = np.array([norm.pdf(position_bins, loc=m, scale=5.0) for m in model_means]) state_dist_misspec = state_dist_misspec / state_dist_misspec.sum(axis=1, keepdims=True)  # Observed data: matches model except during misfit period rng = np.random.default_rng(42) observed_means = model_means.copy() observed_means[misfit_start:misfit_end] += 25  # Large offset during misfit period!  observed_like_misspec = np.array(     [norm.pdf(position_bins, loc=m + rng.normal(0, 2), scale=3.0) for m in observed_means] )  # Compute observed log predictive density observed_log_pred_misspec = log_predictive_density(     state_dist_misspec, likelihood=observed_like_misspec )  # Create sampler (using correct model means, not observed) sampler_misspec = create_sampler(state_dist_misspec, model_means, position_bins)  # Compute p-values p_values_misspec = predictive_pvalue(observed_log_pred_misspec, sampler_misspec, n_samples=1000)  # Visualize fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)  # P-values over time ax1.plot(     np.arange(n_time),     p_values_misspec,     linewidth=2,     marker=\"o\",     markersize=4,     alpha=0.7,     color=\"#1f77b4\", ) ax1.axhline(0.5, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Expected mean\") ax1.axhline(0.05, color=\"red\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Extreme thresholds\") ax1.axhline(0.95, color=\"red\", linestyle=\"--\", alpha=0.5, linewidth=2) ax1.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"True misfit period\") ax1.set_ylabel(\"P-value\") ax1.set_title(\"P-values with Model Misspecification\") ax1.legend(frameon=True, loc=\"upper right\") ax1.grid(True, alpha=0.3)  # Log predictive density over time ax2.plot(np.arange(n_time), observed_log_pred_misspec, linewidth=2, color=\"#ff7f0e\") ax2.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\") ax2.set_xlabel(\"Time\") ax2.set_ylabel(\"Log Predictive Density\") ax2.set_title(\"Log Predictive Density Over Time\") ax2.legend(frameon=True, loc=\"upper right\") ax2.grid(True, alpha=0.3)  plt.tight_layout() plt.show()  # Print statistics print(\"P-value statistics by period:\") print(f\"\\n{'Period':&lt;15} {'Mean P-value':&gt;12} {'% Extreme (&lt;0.05 or &gt;0.95)':&gt;25}\") print(\"-\" * 55) good_pvals = p_values_misspec[     np.concatenate([np.arange(misfit_start), np.arange(misfit_end, n_time)]) ] bad_pvals = p_values_misspec[misfit_start:misfit_end] print(     f\"{'Good fit':&lt;15} {np.mean(good_pvals):&gt;12.3f} {((good_pvals &lt; 0.05) | (good_pvals &gt; 0.95)).sum() / len(good_pvals) * 100:&gt;24.1f}%\" ) print(     f\"{'Misfit':&lt;15} {np.mean(bad_pvals):&gt;12.3f} {((bad_pvals &lt; 0.05) | (bad_pvals &gt; 0.95)).sum() / len(bad_pvals) * 100:&gt;24.1f}%\" ) <pre>P-value statistics by period:\n\nPeriod          Mean P-value % Extreme (&lt;0.05 or &gt;0.95)\n-------------------------------------------------------\nGood fit               0.446                      6.2%\nMisfit                 1.000                    100.0%\n</pre> In\u00a0[9]: Copied! <pre>from statespacecheck import hpd_overlap, kl_divergence\n\n# Compute all diagnostics for the misspecified case\nkl_div_misspec = kl_divergence(state_dist_misspec, observed_like_misspec)\noverlap_misspec = hpd_overlap(state_dist_misspec, observed_like_misspec, coverage=0.95)\n\n# Visualize all three together\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n\n# KL divergence\nax1.plot(np.arange(n_time), kl_div_misspec, linewidth=2.5, color=\"#1f77b4\")\nax1.axhline(1.0, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\")\nax1.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\")\nax1.set_ylabel(\"KL Divergence\")\nax1.set_title(\"Multiple Diagnostic Views of Model Fit\")\nax1.legend(frameon=True, loc=\"upper left\")\nax1.grid(True, alpha=0.3)\n\n# HPD overlap\nax2.plot(np.arange(n_time), overlap_misspec, linewidth=2.5, color=\"#ff7f0e\")\nax2.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\")\nax2.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\")\nax2.set_ylabel(\"HPD Overlap\")\nax2.legend(frameon=True, loc=\"lower left\")\nax2.grid(True, alpha=0.3)\n\n# P-values\nax3.plot(np.arange(n_time), p_values_misspec, linewidth=2.5, color=\"#2ca02c\")\nax3.axhline(0.05, color=\"red\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Extreme thresholds\")\nax3.axhline(0.95, color=\"red\", linestyle=\"--\", alpha=0.5, linewidth=2)\nax3.axhline(0.5, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Expected value\")\nax3.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\")\nax3.set_xlabel(\"Time\")\nax3.set_ylabel(\"P-value\")\nax3.legend(frameon=True, loc=\"upper right\")\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</pre> from statespacecheck import hpd_overlap, kl_divergence  # Compute all diagnostics for the misspecified case kl_div_misspec = kl_divergence(state_dist_misspec, observed_like_misspec) overlap_misspec = hpd_overlap(state_dist_misspec, observed_like_misspec, coverage=0.95)  # Visualize all three together fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 12), sharex=True)  # KL divergence ax1.plot(np.arange(n_time), kl_div_misspec, linewidth=2.5, color=\"#1f77b4\") ax1.axhline(1.0, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"High threshold\") ax1.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\") ax1.set_ylabel(\"KL Divergence\") ax1.set_title(\"Multiple Diagnostic Views of Model Fit\") ax1.legend(frameon=True, loc=\"upper left\") ax1.grid(True, alpha=0.3)  # HPD overlap ax2.plot(np.arange(n_time), overlap_misspec, linewidth=2.5, color=\"#ff7f0e\") ax2.axhline(0.3, color=\"orange\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Low threshold\") ax2.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\") ax2.set_ylabel(\"HPD Overlap\") ax2.legend(frameon=True, loc=\"lower left\") ax2.grid(True, alpha=0.3)  # P-values ax3.plot(np.arange(n_time), p_values_misspec, linewidth=2.5, color=\"#2ca02c\") ax3.axhline(0.05, color=\"red\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Extreme thresholds\") ax3.axhline(0.95, color=\"red\", linestyle=\"--\", alpha=0.5, linewidth=2) ax3.axhline(0.5, color=\"green\", linestyle=\"--\", alpha=0.5, linewidth=2, label=\"Expected value\") ax3.axvspan(misfit_start, misfit_end, alpha=0.2, color=\"red\", label=\"Misfit period\") ax3.set_xlabel(\"Time\") ax3.set_ylabel(\"P-value\") ax3.legend(frameon=True, loc=\"upper right\") ax3.grid(True, alpha=0.3)  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/04_predictive_checks/#posterior-predictive-checks-for-state-space-models","title":"Posterior Predictive Checks for State Space Models\u00b6","text":"<p>This notebook introduces posterior predictive checks, an advanced diagnostic that asks: \"If the model were true, how likely is the observed data?\" This provides a complementary view to the state-likelihood consistency checks.</p> <p>Learning objectives:</p> <ul> <li>Understand predictive density and its computation</li> <li>Learn when to use log-space for numerical stability</li> <li>Compute posterior predictive p-values</li> <li>Interpret p-value patterns for model diagnostics</li> <li>Combine multiple diagnostic approaches</li> </ul> <p>Previous: 03_time_resolved_diagnostics.ipynb</p>"},{"location":"tutorials/04_predictive_checks/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/04_predictive_checks/#what-is-predictive-density","title":"What is Predictive Density?\u00b6","text":"<p>Predictive density measures how likely the observed data is under the model's prediction:</p> <p>$$p(y_t) = \\int p(y_t | x_t) p(x_t | y_{1:t-1}) dx_t = \\sum_x p(y_t | x_t) p(x_t)$$</p> <p>Components:</p> <ul> <li>$p(y_t | x_t)$: Observation likelihood (how likely is this observation at each state?)</li> <li>$p(x_t)$: State distribution (what states are probable?)</li> <li>Integration: Average likelihood over all probable states</li> </ul> <p>Interpretation:</p> <ul> <li>High predictive density: Data is consistent with model predictions</li> <li>Low predictive density: Data is surprising given the model</li> <li>Averaged over time, gives overall model fit</li> </ul> <p>Key difference from previous metrics:</p> <ul> <li>KL/overlap: Compare distributions (state vs likelihood)</li> <li>Predictive density: Evaluate data plausibility under the model</li> </ul>"},{"location":"tutorials/04_predictive_checks/#computing-predictive-density","title":"Computing Predictive Density\u00b6","text":"<p>Let's start with a simple example.</p>"},{"location":"tutorials/04_predictive_checks/#understanding-the-computation","title":"Understanding the Computation\u00b6","text":"<p>The predictive density integrates (sums) the product of:</p> <ol> <li>State distribution: Probability of each position</li> <li>Likelihood: How well the data fits at each position</li> </ol> <p>When state and likelihood agree (both peak near the same location), predictive density is high. When they disagree, predictive density is low.</p>"},{"location":"tutorials/04_predictive_checks/#comparing-good-vs-poor-fit","title":"Comparing Good vs Poor Fit\u00b6","text":"<p>Let's compare predictive density for well-fitting vs poorly-fitting data.</p>"},{"location":"tutorials/04_predictive_checks/#log-predictive-density-for-numerical-stability","title":"Log Predictive Density for Numerical Stability\u00b6","text":"<p>For real applications with many bins or peaked distributions, predictive densities can become very small, leading to numerical underflow. We use log-space computation for stability.</p>"},{"location":"tutorials/04_predictive_checks/#posterior-predictive-p-values","title":"Posterior Predictive P-Values\u00b6","text":"<p>A single predictive density value is hard to interpret (\"Is 0.0023 good or bad?\"). Posterior predictive p-values provide context by comparing to simulated data:</p> <p>Process:</p> <ol> <li>Compute predictive density for observed data</li> <li>Generate many datasets from the model</li> <li>Compute predictive density for each simulated dataset</li> <li>P-value = proportion of simulated densities \u2265 observed density</li> </ol> <p>Interpretation:</p> <ul> <li>p \u2248 0.5: Observed data is typical for this model</li> <li>p \u2248 0 or 1: Observed data is extreme (model may be misspecified)</li> <li>Systematic patterns: Indicate model problems</li> </ul>"},{"location":"tutorials/04_predictive_checks/#example-computing-p-values","title":"Example: Computing P-Values\u00b6","text":"<p>Let's create a simple scenario and compute p-values.</p>"},{"location":"tutorials/04_predictive_checks/#interpretation","title":"Interpretation\u00b6","text":"<p>Left panel: P-values fluctuate around 0.5, as expected for well-specified model</p> <p>Right panel: P-values are roughly uniform (approximately equal density across range)</p> <p>What this tells us:</p> <ul> <li>Observed data is consistent with model predictions</li> <li>No systematic deviations (no clustering near 0 or 1)</li> <li>Model appears well-calibrated</li> </ul>"},{"location":"tutorials/04_predictive_checks/#detecting-model-misspecification-with-p-values","title":"Detecting Model Misspecification with P-Values\u00b6","text":"<p>Now let's see what happens when the model is misspecified during a period.</p>"},{"location":"tutorials/04_predictive_checks/#interpretation","title":"Interpretation\u00b6","text":"<p>Key observations:</p> <ul> <li>P-values drop dramatically during misfit period (many near 0)</li> <li>Log predictive density decreases during misfit (data is surprising)</li> <li>Good periods have p-values scattered around 0.5</li> <li>Misfit periods have extreme p-values (data very unlikely under model)</li> </ul> <p>Why p-values near 0?</p> <ul> <li>Observed log predictive density is much lower than typical simulations</li> <li>Most simulated datasets are more consistent with model than observed data</li> <li>This indicates model fails to capture the data during that period</li> </ul>"},{"location":"tutorials/04_predictive_checks/#combining-multiple-diagnostics","title":"Combining Multiple Diagnostics\u00b6","text":"<p>Let's bring everything together: KL divergence, HPD overlap, and predictive p-values provide complementary views of model fit.</p>"},{"location":"tutorials/04_predictive_checks/#complementary-information","title":"Complementary Information\u00b6","text":"<p>Each diagnostic provides a different perspective:</p> <p>KL Divergence:</p> <ul> <li>Measures distribution disagreement</li> <li>Spikes when state and likelihood concentrate mass differently</li> <li>Sensitive to shape differences</li> </ul> <p>HPD Overlap:</p> <ul> <li>Measures spatial consistency</li> <li>Drops when high-density regions don't align</li> <li>Intuitive spatial interpretation</li> </ul> <p>P-values:</p> <ul> <li>Compares observed to expected under model</li> <li>Detects when data is surprising/extreme</li> <li>Requires simulation but provides calibrated interpretation</li> </ul> <p>All three agree: The misfit period is clearly problematic!</p>"},{"location":"tutorials/04_predictive_checks/#practical-workflow-for-your-data","title":"Practical Workflow for Your Data\u00b6","text":"<p>Here's a recommended workflow for applying these diagnostics to real state space models:</p>"},{"location":"tutorials/04_predictive_checks/#1-compute-basic-diagnostics","title":"1. Compute Basic Diagnostics\u00b6","text":"<pre>kl_div = kl_divergence(state_dist, likelihood)\noverlap = hpd_overlap(state_dist, likelihood, coverage=0.95)\n</pre>"},{"location":"tutorials/04_predictive_checks/#2-visualize-over-time","title":"2. Visualize Over Time\u00b6","text":"<pre>plt.plot(time, kl_div)\nplt.plot(time, overlap)\n</pre>"},{"location":"tutorials/04_predictive_checks/#3-flag-problematic-periods","title":"3. Flag Problematic Periods\u00b6","text":"<pre>from statespacecheck import flag_extreme_kl, flag_low_overlap\nkl_flags = flag_extreme_kl(kl_div, threshold=1.0)\noverlap_flags = flag_low_overlap(overlap, threshold=0.3)\n</pre>"},{"location":"tutorials/04_predictive_checks/#4-if-issues-detected-compute-p-values","title":"4. If Issues Detected, Compute P-values\u00b6","text":"<pre>log_pred = log_predictive_density(state_dist, likelihood=likelihood)\n# Create sampler based on your model\np_vals = predictive_pvalue(log_pred, sampler, n_samples=1000)\n</pre>"},{"location":"tutorials/04_predictive_checks/#5-interpret-results","title":"5. Interpret Results\u00b6","text":"<ul> <li>Multiple metrics agree \u2192 Strong evidence of problem</li> <li>Only one metric flags \u2192 Investigate further</li> <li>P-values systematically extreme \u2192 Model misspecification</li> <li>All metrics good \u2192 Model fits well!</li> </ul>"},{"location":"tutorials/04_predictive_checks/#summary-what-we-learned","title":"Summary: What We Learned\u00b6","text":"<p>Core concepts:</p> <ul> <li>Predictive density: How likely is observed data under the model?</li> <li>Log-space: Use <code>log_predictive_density()</code> for numerical stability</li> <li>P-values: Compare observed to simulated data for calibrated interpretation</li> </ul> <p>Key functions:</p> <ul> <li><code>predictive_density()</code>: Compute p(y) = \u222b p(y|x) p(x) dx</li> <li><code>log_predictive_density()</code>: Compute in log-space for stability</li> <li><code>predictive_pvalue()</code>: Monte Carlo p-values via simulation</li> </ul> <p>Practical insights:</p> <ul> <li>Predictive checks complement distribution comparison metrics</li> <li>P-values near 0 or 1 indicate model problems</li> <li>Log-space is essential for realistic applications</li> <li>Multiple diagnostics provide robustness</li> </ul> <p>Complete diagnostic toolkit:</p> <ol> <li>KL divergence: Distribution disagreement</li> <li>HPD overlap: Spatial consistency</li> <li>Predictive p-values: Data plausibility</li> <li>Time-resolved analysis: Identify when/where models fail</li> </ol> <p>Neuroscience applications:</p> <ul> <li>Validate neural decoding models</li> <li>Compare different state space architectures</li> <li>Identify behavioral epochs where models break down</li> <li>Calibrate uncertainty estimates</li> </ul> <p>You're now equipped to thoroughly diagnose state space models!</p>"},{"location":"tutorials/04_predictive_checks/#exercises-optional","title":"Exercises (Optional)\u00b6","text":"<ol> <li>What happens to p-values if you use too few simulation samples (e.g., n_samples=10)?</li> <li>Create a case where KL divergence is high but p-values are moderate. What does this tell you?</li> <li>How would you use predictive checks to compare two different model architectures?</li> <li>Can you detect the difference between systematic bias and random noise using these diagnostics?</li> </ol>"}]}