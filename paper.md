Introduction
Many brain states, such as memory, emotion, and attention, are latent dynamic states. They are not directly observable, they are not solely determined by the external world and they can change over time, making them challenging to understand. However, these brain states are some of the most important to understanding complex behavior, because they go beyond simple stimulus and response relationships. A major challenge in neuroscience is to relate neural activity (spikes, spike waveforms, calcium traces, etc.) to latent dynamic brain states.

State space models are a powerful way to understand how neural activity relates to latent dynamic brain states (refs). They have been used to understand blah blah blah (examples). The core assumption of these state space models is that complex, high dimensional neural activity can be related to low dimensional, latent states. This consists of two main components:
a state transition model, which says how the latent states can evolve over time
an observation model, which relates the observed neural activity at a single time to the current latent state.

Through these two components, state space models aggregate information over time about the latent states, weighing both the information from the data at the current time (normalized likelihood distribution) with the accumulated information of the latent state (prediction distribution). The normalized likelihood represents the likelihood of the data given only the current neural activity and state (assuming a uniform prior), as defined by the observation model. The prediction distribution projects forward what the state is expected to be based on past information via the state transition model. This powerful combination enables robust estimation of the latent state, effectively balancing new information with the accumulated history of the system and prior knowledge. However, the relationship between the predictive and likelihood distributions themselves carries rich diagnostic information that is often overlooked.

When these two distributions agree, the model’s prior expectations and data-driven evidence are consistent. When they diverge, the mismatch can reveal where and when the model fails to capture the structure of the data. Such divergences can arise from inaccuracies in either the state transition dynamics (e.g., overly rigid temporal evolution) or the observation model (e.g., mis-specified tuning or noise assumptions). Examining these discrepancies therefore provides a direct, interpretable measure of model–data agreement—analogous to a local, time-resolved goodness-of-fit test.

State space models are necessarily simplifications of complex neural states, making assumptions about the dimensionality of the latent state and how it evolves, and how the data relates to those states. An important part of the scientific process is iterative model building, where models are fit to the data, evaluated and refined to better answer the scientific question (ref). However, before refining or interpreting a model, neuroscientists need to assess how well the model fits to the data and to their prior knowledge of the data (Gelman ref). This is known as the goodness of fit of the model. These analyses are distinct from cross-validation, which measures predictive generalization, or permutation tests, which assess how much structure is lost when relationships in the data are disrupted (e.g., shuffling spike times or receptive fields).

Despite their importance, goodness-of-fit analyses are underused in neuroscience, particularly for latent variable models. Evaluating model–data agreement is difficult because the latent state cannot be observed directly (Newman & Thomas, 2014). Without ground truth, model validity can only be inferred indirectly—from patterns captured, reconstructions achieved, or performance on downstream tasks. Moreover, neural data are inherently dynamic, so model fit may vary substantially across time. Traditional global goodness-of-fit metrics, such as residual or innovation-based tests, often fail to detect local misfits or diagnose whether errors arise from the state or observation model (Auger-Méthé et al., 2021). Consequently, these tools can reject models that are intentionally simplified yet scientifically meaningful, while offering little guidance for targeted model improvement. Finally, accessible software for such diagnostic analyses remains scarce.

To address these gaps, we develop and validate a suite of three complementary model checking methods designed to provide actionable feedback to neuroscientists. Specifically, our methods are designed to:
Identify time periods of poor model–data agreement, enabling local evaluation of fit
Understand errors arising from the state versus observation components, supporting targeted model refinement.
Provide intuitive visualizations that make these diagnostics accessible to a broad scientific audience.
We demonstrate the utility of these techniques on real hippocampal data. Together, these tools make it possible to move beyond binary model rejection toward an iterative, diagnostic approach to model building—advancing the use of state space models as interpretable, testable theories of neural dynamics.
